{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanghyun-ai/ktcloud_genai/blob/main/203_LLM_%ED%85%8D%EC%8A%A4%ED%8A%B8%EB%B6%84%EB%A5%98.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **텍스트 분류(Text Classification)**"
      ],
      "metadata": {
        "id": "yp-AQkTjxp64"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "- 💡 **NOTE**\n",
        "    - 이 노트북의 코드를 실행하려면 GPU를 사용하는 것이 좋습니다. 구글 코랩에서는 **런타임 > 런타임 유형 변경 > 하드웨어 가속기 > T4 GPU**를 선택하세요.\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "81_Ybs4LI7IX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "yvWiFESCy0SN"
      },
      "outputs": [],
      "source": [
        "# %%capture : 코드 셀의 출력을 숨겨\n",
        "%%capture\n",
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "05VEeiVN2Aqk"
      },
      "outputs": [],
      "source": [
        "# 깃허브에서 위젯 상태 오류를 피하기 위해 진행 표시줄을 나타내지 않도록 설정합니다.\n",
        "from transformers.utils import logging\n",
        "\n",
        "logging.disable_progress_bar()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "VzvPBd4-cVcj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **텍스트 분류(Text Classification) 이해**"
      ],
      "metadata": {
        "id": "k5PDPAe5cWOX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **분류** :  가장 일반적인 자연어 처리 작업\n",
        "- **분류 작업의 목표**:\n",
        "    - 모델을 훈련하여 입력 테스트에 레이블(label) or 클래스(class)를 할당하는 것\n",
        "- **텍스트 분류**:\n",
        "    - **감성 분석**(sentiment analysis)\n",
        "    - **의도 감지**(intent detection)\n",
        "    - **엔티티 추출**(entity extraction)\n",
        "    - **언어 감지** 등 광범위한 애플리케이션에서 사용됨\n",
        "- **표현 언어 모델과 생성 언어 모델이 분류 작업에 많은 영향을 끼치고 있음**\n",
        "    - 표현(Representation) 언어 모델\n",
        "        - 세상의 모든 글을 읽고 깊이 이해하는 독해 전문가\n",
        "    - 생성(Generative) 언어 모델\n",
        "        - 하나의 주제를 바탕으로 유창하게 새로운 글을 쓰는 작문 전문가"
      ],
      "metadata": {
        "id": "dqXVM70Cce-2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| 구분\t| 표현 언어 모델 (Representation Model)\t| 생성 언어 모델 (Generative Model)\t| 인코더-디코더 모델 (Seq2Seq Model) |\n",
        "|--- | ---|---|---|\n",
        "| 목표\t|  문장의 의미 이해 및 표현 (Understanding)\t|  다음 단어 예측을 통한 새로운 문장 생성 (Generation)\t|  입력 문장 이해 후, 새로운 형태의 문장 생성|\n",
        "| 핵심 아이디어\t|  양방향(Bidirectional) 문맥 파악\t|  단방향(Unidirectional) 문맥 기반 예측\t|  **이해**(인코더)와 **생성**(디코더)의 결합\t|\n",
        "| 대표 모델\t|  **BERT**, RoBERTa, ALBERT, ELECTRA\t|  **GPT** 계열, LLaMA, Claude, PaLM\t|  **T5**, BART, (초기) Transformer\t|\n",
        "| 주요 구조\t|  트랜스포머 인코더 (Encoder)\t|  트랜스포머 디코더 (Decoder)\t|  트랜스포머 인코더 + 디코더\t|\n",
        "| 학습 방식\t|  MLM (Masked Language Model): 문장의 일부 단어를 가리고 맞추는 방식| \tCLM (Causal Language Model): 이전 단어들로 다음 단어를 예측하는 방식\t|  두 가지 방식을 혼합하여 사용\t|\n",
        "| 비유\t|  🧑‍🏫 독해 전문가, 맥락 탐정\t|  ✍️ 창의적인 작가, 이야기꾼\t|  🌐 능숙한 통역가, 요약 전문가\t|\n",
        "| 잘하는 작업\t\"|  - 분류: 감성 분석, 뉴스 카테고리 분류<br>- 개체명 인식(NER): 인물, 장소 찾기<br>- 질의응답: 본문에서 정답 찾기<br>- **작업에 특화된 모델과 임베딩 생성 모델로 사용**| - 창작: 챗봇, 소설/기사 작성, 이메일 초안<br>- 요약: 긴 글을 짧게 요약<br>- 코드 생성: 기능에 맞는 코드 작성\"\t\"| - 번역: 한국어 → 영어<br>- 요약: (특히) 입력과 출력의 형태가 다른 요약\"\t|\n"
      ],
      "metadata": {
        "id": "f2W5S0xyfK2R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **(텍스트 분류 작업을 위한) 허깅페이스에서의 모델 사용 방법(task명)**\n",
        "    - **1.텍스트 분류 모델 사용**(text-classification) :\n",
        "        - https://huggingface.co/models?pipeline_tag=text-classification\n",
        "    - **2.임베딩 생성 모델 사용**(feature-extraction) :\n",
        "        - https://huggingface.co/models?pipeline_tag=feature-extraction"
      ],
      "metadata": {
        "id": "2XIZ5hchzVoY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "C5KuXbU3iUSw"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBeVnXxQWy7-"
      },
      "source": [
        "### **분류에 사용할 데이터셋**(영화 리뷰 데이터셋)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 허깅페이스 데이터셋 : https://huggingface.co/datasets\n",
        "- 로튼 토마토 (Rotten Tomatoes) 데이터셋:\n",
        "    - https://www.rottentomatoes.com/\n",
        "    - 영화와 TV 프로그램에 대한 리뷰를 모아놓은(review-aggregation) 미국의 웹사이트\n",
        "    - 자연어 처리(NLP) 연구에 매우 중요하게 사용\n",
        "    - 데이터 구성\n",
        "        - Review (리뷰 텍스트)\n",
        "        - Label (감성 라벨): 긍정(1,positive), 부정(0,negative)"
      ],
      "metadata": {
        "id": "oPz8uRTPimXE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "5phRS_z2U_3T",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363,
          "referenced_widgets": [
            "681949a153cc4c3e826aff6459bc7997",
            "78d1e50360ca4a4d82c416762e93cf31",
            "d4b809c7ec274adc9f73579d6e324f2b",
            "196e9fd2b8d745bb80d66792341d4467",
            "21883d250d164e01beda2ee94523a75e",
            "7c33bd0f38624df8b8b55c340903f67d",
            "be081cd2128347ac82e87804b3e63b7b",
            "c440febc76554cd2a5e76daa86509cb6",
            "9ea838a9079c46499303a9db4ce4381e",
            "2532eddeed7147bcb908fa2b6cd4258f",
            "1cd5f9b349aa4ed188c9f7ffc4fa74e8",
            "530a891c6d43499783e0858f94deb934",
            "39faf577742646f8b5e544287512fe98",
            "cf2c3b9ea1fa44baa88aa092190742c7",
            "7bed481258cd45dd99c7477ee9f201b3",
            "954f80acde6b4bfc9243815a0ea961f6",
            "0811aff0b764444eb85e5ea132518e92",
            "916a6887790a4276839833ccdf1ee5f5",
            "dc4b9280da334a4593ea8154ee355958",
            "1b85a9e1e5e2428297b3c63c90221336",
            "ca8de1d521ac42fba4f0d36f9aebb4c1",
            "869d6bcb99ef43218cb5050f89ff832c",
            "a5a12e9a9330453a97f4ad7942a2e535",
            "072d23b6a2ee43d9ae687ce33212df0e",
            "fdd0ccb937944571bef96d0d2a9bc84e",
            "1d80e501e05a4104908db3c853d45e98",
            "1d105c8f7d354f97b52b1e0b042e2a1a",
            "5387c5bbf1304cf0a0ec889665f0b0f4",
            "80c29c3bbbed44a39f956aef7ea8f6b7",
            "21756bab6ca547c5a61c450b926d3fa9",
            "2f3952789c80482fac76daa9f3377945",
            "51b22df98b0d4e30aa29d5e95e00211e",
            "6def8b7c763c44f1943a07b54244cd82"
          ]
        },
        "outputId": "077bd46b-fce7-47d5-9ff3-03704b525fa7"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split:   0%|          | 0/8530 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "681949a153cc4c3e826aff6459bc7997"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating validation split:   0%|          | 0/1066 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "530a891c6d43499783e0858f94deb934"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating test split:   0%|          | 0/1066 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a5a12e9a9330453a97f4ad7942a2e535"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['text', 'label'],\n",
              "        num_rows: 8530\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['text', 'label'],\n",
              "        num_rows: 1066\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['text', 'label'],\n",
              "        num_rows: 1066\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# 데이터를 로드합니다.\n",
        "data = load_dataset(\"rotten_tomatoes\")\n",
        "data\n",
        "# 훈련, 검증, 테스트 데이터셋으로 구분됨\n",
        "# - 검증 데이터셋 : 모델을 훈련할 때 파라미터 튜닝할 경우 사용할 수 있음"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "xJJmaJzHDLZv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5363c2c-bd14-478b-f009-86da35a082a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(8530, 2)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'text': ['the rock is destined to be the 21st century\\'s new \" conan \" and that he\\'s going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .',\n",
              "  'things really get weird , though not particularly scary : the movie is all portent and no content .'],\n",
              " 'label': [1, 0]}"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "print(data[\"train\"].shape)\n",
        "data[\"train\"][0, -1]  # 첫 행의 마지막 열 값"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xya5dfmVoR1R"
      },
      "source": [
        "### **표현 모델로 텍스트 분류하기(BERT 계열)**\n",
        "\n",
        "- BERT 계열 모델은 **작업(task)에 특화된 모델과 임베딩 생성 모델로 사용됨**\n",
        "- BERT 변종 베이스 모델 (자세한 내용은 각 논문 찾아보기)\n",
        "    - **BERT 베이스 모델**(uncased) : https://huggingface.co/google-bert/bert-base-uncased\n",
        "    - **RoBERTa 베이스 모델** : https://huggingface.co/FacebookAI/roberta-base\n",
        "    - **DistilBERT 베이스 모델** : https://huggingface.co/distilbert/distilbert-base-uncased\n",
        "    - **DeBERTa 베이스 모델** : https://huggingface.co/microsoft/deberta-base\n",
        "    - **bert-tiny** : https://huggingface.co/prajjwal1/bert-tiny\n",
        "    - **ALBERT 베이스 V2** : https://huggingface.co/albert/albert-base-v2\n",
        "- 파인튜닝 모델 예:\n",
        "    - **Twitter-RoBERTa-base 모델**:  감성 분석을 위해 트윗 데이터에서 미세 튜닝한 RoBERTa 모델\n",
        "        - https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment-latest\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **예제: 나만의 영화 평론 분석기**\n",
        "- [참고] 103-LLM-언어 AI 맛보기.ipynb"
      ],
      "metadata": {
        "id": "rFPKvp8i1NvA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hugging Face의 transformers 라이브러리 활용\n",
        "from transformers import pipeline\n",
        "\n",
        "# 감성 분석 파이프라인 로드 (sentiment-analysis는 \"작업 유형\"을 지정하는 것)\n",
        "sentiment_analyzer = pipeline(\"sentiment-analysis\")  # Task : sentiment-analysis\n",
        "\n",
        "# review = \"이 영화는 정말 시간 가는 줄 모르고 봤어요. 배우들 연기가 최고!\"\n",
        "review = \"어떻게 감독이 된 건지 의문인 영화\"\n",
        "result = sentiment_analyzer(review)\n",
        "\n",
        "print('\\n✅ 감성 분석(Sentiment Analysis) 결과')\n",
        "print(f\"리뷰: {review}\")\n",
        "print(f\"분석 결과: {result}\") # [{'label': 'POSITIVE', 'score': 0.99...}]\n",
        "\n",
        "# 핵심 구문 추출은 별도의 모델이나 기법이 필요\n",
        "# 예시: \"배우들 연기\", \"시간 가는 줄\" 등을 추출"
      ],
      "metadata": {
        "id": "4_NfiMJ3wPca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "927559be-fb9d-46e0-ae50-8e278d9bd1ff"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ 감성 분석(Sentiment Analysis) 결과\n",
            "리뷰: 어떻게 감독이 된 건지 의문인 영화\n",
            "분석 결과: [{'label': 'POSITIVE', 'score': 0.7819299697875977}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "import pandas as pd\n",
        "\n",
        "# 감정 분석 파이프라인 초기화\n",
        "#   model=<업로더이름>/<모델이름>\n",
        "#       nlptown → 모델을 공개한 Hugging Face 사용자(혹은 조직) 계정명\n",
        "#       bert-base-multilingual-uncased-sentiment → 모델의 이름\n",
        "#           BERT 기반\n",
        "#           다국어 지원\n",
        "#           대소문자 구분하지 않음 (uncased)\n",
        "#           감성 분석용으로 학습된 모델\n",
        "sentiment_analyzer = pipeline(\"sentiment-analysis\",\n",
        "                             model=\"nlptown/bert-base-multilingual-uncased-sentiment\" )\n",
        "\n",
        "\n",
        "# 샘플 리뷰 데이터\n",
        "reviews = [\n",
        "    \"이 제품 정말 좋아요! 추천합니다.\",\n",
        "    \"배송이 너무 늦었어요. 실망스럽네요.\",\n",
        "    \"가격 대비 괜찮은 것 같습니다.\"\n",
        "]\n",
        "\n",
        "# 감정 분석 수행\n",
        "results = []\n",
        "for review in reviews:\n",
        "    result = sentiment_analyzer(review)\n",
        "    results.append({\n",
        "        'text': review,\n",
        "        'sentiment': result[0]['label'],\n",
        "        'confidence': result[0]['score']\n",
        "    })\n",
        "\n",
        "# 결과 출력\n",
        "df = pd.DataFrame(results)\n",
        "\n",
        "print('\\n✅ 감성 분석(Sentiment Analysis) 결과')\n",
        "print(df)"
      ],
      "metadata": {
        "id": "uprHR5Xrwp52",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2dfccee-982b-4995-8028-841d8ff0a3ad"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ 감성 분석(Sentiment Analysis) 결과\n",
            "                   text sentiment  confidence\n",
            "0   이 제품 정말 좋아요! 추천합니다.   5 stars    0.718316\n",
            "1  배송이 너무 늦었어요. 실망스럽네요.    1 star    0.425818\n",
            "2     가격 대비 괜찮은 것 같습니다.   3 stars    0.556177\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "qlLmVts8wNwP"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "co68g-Eloknf"
      },
      "source": [
        "## **1. 작업에 특화된 모델 사용하기**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **예제: 감성분석을 위한 사전학습모델을 사용**\n",
        "- 💡 아래 예제는 감성분석을 위한 사전학습모델을 사용합니다."
      ],
      "metadata": {
        "id": "aH0Rc-Y6B39Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **데이터셋 준비하기**\n",
        "    - 앞에서 실행한 내용"
      ],
      "metadata": {
        "id": "fObD6Y9q6ogs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 허깅페이스 데이터셋 사용\n",
        "from datasets import load_dataset\n",
        "\n",
        "# rotten_tomatoes에서 데이터를 로드합니다.\n",
        "data = load_dataset(\"rotten_tomatoes\")\n",
        "data\n",
        "# 훈련, 검증, 테스트 데이터셋으로 구분됨\n",
        "# - 검증 데이터셋 : 모델을 훈련할 때 파라미터 튜닝할 경우 사용할 수 있음"
      ],
      "metadata": {
        "id": "IptzuBz16ooQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0516f38b-86f4-48a2-c171-46d7a8b578f3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['text', 'label'],\n",
              "        num_rows: 8530\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['text', 'label'],\n",
              "        num_rows: 1066\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['text', 'label'],\n",
              "        num_rows: 1066\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. 모델 로드하기**\n",
        "    - 훈련 데이터 없는 단어를 만나더라도 토큰을 결합하여 표현을 생성할 수 있다."
      ],
      "metadata": {
        "id": "PbmgIjrn5pLX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ph-3T3XJopdN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 474
        },
        "outputId": "6defce94-f110-4f83-adfb-df07c525cc26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1895366723.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# 파이프라인으로 모델을 로드합니다.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m pipe = pipeline(\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/pipelines/__init__.py\u001b[0m in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, processor, framework, revision, use_fast, token, device, device_map, dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m   1227\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"processor\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocessor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1229\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mpipeline_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframework\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mframework\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/pipelines/text_classification.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         self.check_model_type(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, tokenizer, feature_extractor, image_processor, processor, modelcard, framework, task, device, binary_output, **kwargs)\u001b[0m\n\u001b[1;32m   1042\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mhf_device_map\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1043\u001b[0m         ):\n\u001b[0;32m-> 1044\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m         \u001b[0;31m# If it's a generation pipeline and the model can generate:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   4345\u001b[0m                     \u001b[0;34m\" `dtype` by passing the correct `dtype` argument.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4346\u001b[0m                 )\n\u001b[0;32m-> 4347\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4348\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4349\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhalf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1367\u001b[0m                     \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1369\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1371\u001b[0m     def register_full_backward_pre_hook(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    926\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    927\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 928\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    926\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    927\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 928\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    926\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    927\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 928\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    953\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 955\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    956\u001b[0m             \u001b[0mp_should_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    957\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1353\u001b[0m                         \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_to_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m                     )\n\u001b[0;32m-> 1355\u001b[0;31m                 return t.to(\n\u001b[0m\u001b[1;32m   1356\u001b[0m                     \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1357\u001b[0m                     \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    410\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"CUDA_MODULE_LOADING\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"CUDA_MODULE_LOADING\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"LAZY\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 412\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    413\u001b[0m         \u001b[0;31m# Some of the queued calls may reentrantly call _lazy_init();\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0;31m# we need to just return without initializing in that case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# 허깅 페이스 모델 경로\n",
        "model_path = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
        "\n",
        "# 파이프라인으로 모델을 로드합니다.\n",
        "pipe = pipeline(\n",
        "    model=model_path,\n",
        "    tokenizer=model_path,\n",
        "    return_all_scores=True,\n",
        "    device=\"cuda:0\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. 테스트 세트로 모델 실행**"
      ],
      "metadata": {
        "id": "m3h6sKJd6FOq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B2gbnL5Q69Y5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from transformers.pipelines.pt_utils import KeyDataset\n",
        "\n",
        "# 추론을 실행합니다.\n",
        "y_pred = []   # 예측 결과\n",
        "for output in tqdm(pipe(KeyDataset(data[\"test\"], \"text\")), total=len(data[\"test\"])):\n",
        "    negative_score = output[0][\"score\"]\n",
        "    positive_score = output[2][\"score\"]\n",
        "    assignment = np.argmax([negative_score, positive_score])\n",
        "    y_pred.append(assignment)\n",
        "# output: [{'label': 'negative', 'score': 0.00516123790293932}, {'label': 'neutral', 'score': 0.040233541280031204}, {'label': 'positive', 'score': 0.9546052813529968}]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. 모델 평가하기**"
      ],
      "metadata": {
        "id": "tTFaLgmD64A_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X0KyKHtqyjn3"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "def evaluate_performance(y_true, y_pred):\n",
        "    \"\"\"분류 리포트를 만들어 출력합니다.\"\"\"\n",
        "    performance = classification_report(\n",
        "        y_true, y_pred,\n",
        "        target_names=[\"Negative Review\", \"Positive Review\"]\n",
        "    )\n",
        "    print(performance)\n",
        "\n",
        "# 모델 평가하기\n",
        "evaluate_performance(data[\"test\"][\"label\"], y_pred)\n",
        "\n",
        "# 영화 리뷰에서 훈련하지 않은 모델이지만(사전훈련모델) F1-score가 0.8이면 괜찮은 성능"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **혼동행렬(Confusion Matrix)**"
      ],
      "metadata": {
        "id": "vJXt4EnD8W8H"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fum3MTSyymlW"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# 1. 혼동 행렬 계산\n",
        "cm = confusion_matrix(data[\"test\"][\"label\"], y_pred)\n",
        "\n",
        "# 2. 클래스 이름 정의\n",
        "class_names = [\"Negative Review\", \"Positive Review\"]\n",
        "\n",
        "# 3. 혼동 행렬을 히트맵으로 시각화\n",
        "plt.figure(figsize=(6, 4)) # 그래프 크기 설정\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=class_names, yticklabels=class_names)\n",
        "plt.xlabel('Predicted Label') # x축 라벨\n",
        "plt.ylabel('True Label')      # y축 라벨\n",
        "plt.title('Confusion Matrix') # 그래프 제목\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **[실습] 다른 모델 사용해서 Accuracy 성능 높여보기**\n",
        "- **사용 모델** : distilbert/distilbert-base-uncased-finetuned-sst-2-english"
      ],
      "metadata": {
        "id": "dkiPotVj-xK9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#------------------\n",
        "# 1.모델 로드하기\n",
        "#------------------\n",
        "from transformers import pipeline\n",
        "\n",
        "# 허깅 페이스 모델 경로\n",
        "model_path = \"distilbert/distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "\n",
        "# 파이프라인으로 모델을 로드합니다.\n",
        "pipe = pipeline(\n",
        "    model=model_path,\n",
        "    tokenizer=model_path,\n",
        "    return_all_scores=True,\n",
        "    device=\"cuda:0\"\n",
        ")\n",
        "\n",
        "#------------------\n",
        "# 2.테스트 세트로 모델 실행\n",
        "#------------------\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from transformers.pipelines.pt_utils import KeyDataset\n",
        "\n",
        "# 추론을 실행합니다.\n",
        "y_pred = []   # 예측 결과\n",
        "for output in tqdm(pipe(KeyDataset(data[\"test\"], \"text\")), total=len(data[\"test\"])):\n",
        "    negative_score = output[0][\"score\"]\n",
        "    positive_score = output[1][\"score\"]\n",
        "    assignment = np.argmax([negative_score, positive_score])\n",
        "    y_pred.append(assignment)\n",
        "# output: [{'label': 'NEGATIVE', 'score': 0.00018543035548646003}, {'label': 'POSITIVE', 'score': 0.9998145699501038}]\n",
        "\n",
        "#------------------\n",
        "# 3.모델 평가하기\n",
        "#------------------\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "def evaluate_performance(y_true, y_pred):\n",
        "    \"\"\"분류 리포트를 만들어 출력합니다.\"\"\"\n",
        "    performance = classification_report(\n",
        "        y_true, y_pred,\n",
        "        target_names=[\"Negative Review\", \"Positive Review\"]\n",
        "    )\n",
        "    print(performance)\n",
        "\n",
        "# 모델 평가하기\n",
        "evaluate_performance(data[\"test\"][\"label\"], y_pred)\n"
      ],
      "metadata": {
        "id": "_qihhqi2--6G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "Gyn4s2lrCFsZ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wr3WT4jzoNZE"
      },
      "source": [
        "## **2. 임베딩을 활용하여 분류 작업 수행하기**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **임베딩을 생성하는 모델 선택하는 방법**\n",
        "\n",
        "- **(일반적 성능 확인)MTEB 리더보드를 우선 찾아본다**.\n",
        "    - https://huggingface.co/spaces/mteb/leaderboard\n",
        "    - MTEB (Massive Text Embedding Benchmark)\n",
        "        - 허깅페이스 연구팀에서 제공하는 다양한 자연어 처리(NLP) 과제에서 **임베딩 모델(embedding model)의 품질**을 객관적으로 평가하기 위한 벤치마크\n",
        "        - 58개 데이셋과 112개의 언어를 포괄하는 8개의 임베딩 평가 작업으로 구성됨\n",
        "        - **2024년 이후 최신 임베딩 모델의 비교 평가 지표로 MTEB 사용 확산**\n",
        "        - 영어 중심이지만, Korean 포함 다국어(Multilingual MTEB)도 지원\n",
        "        - 성능 뿐만 아니라 추론 속도 등 다양한 항목 고려해야됨\n",
        "\n",
        "- **(자신의 목적에 맞게: *텍스트 분류*) 임베딩 모델(BERT 계열) 사용**\n",
        "    - sentence-transformers/all-mpnet-base-v2** 사용\n",
        "    - https://huggingface.co/sentence-transformers/all-mpnet-base-v2\n",
        "    - 작지만 성능 좋다.\n",
        "\n"
      ],
      "metadata": {
        "id": "YAO8F2MO4jqD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **MTEB로 평가되는 대표 과제(Task) 유형**\n",
        "\n",
        "|카테고리|설명|예시|\n",
        "|---|---|---|\n",
        "|Classification|텍스트 분류|뉴스 기사 주제 분류|\n",
        "|Clustering|문장 또는 문서 군집화|유사 뉴스 기사 묶기|\n",
        "|Pair Classification|문장 쌍 관계 분류|문장1과 문장2가 같은 의미인가?|\n",
        "|STS (Semantic Textual Similarity)|문장 의미 유사도 측정|“나는 학교에 간다” ↔ “나는 학교로 갔다”|\n",
        "|Retrieval|검색 및 정보 검색|질문에 맞는 문서 찾기|\n",
        "|Reranking|검색 결과 재정렬|검색 결과 중 더 관련성 높은 순서로 정렬|\n",
        "|Summarization / Bitext Mining 등|\"요약, 번역 등 다양\"|\"문서요약, 병렬문장 탐색 등\"|\n",
        "\n",
        "- **MTEB 사용 예**"
      ],
      "metadata": {
        "id": "hADuRUD0KU3t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mteb"
      ],
      "metadata": {
        "collapsed": true,
        "id": "LglQeHlmOCF5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from mteb import MTEB\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# 1. 평가할 모델 로드\n",
        "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "# 2. 벤치마크 선택 (예: 영어 STS)\n",
        "evaluation = MTEB(tasks=[\"STSBenchmark\"])\n",
        "\n",
        "# 3. 평가 실행\n",
        "evaluation.run(model, output_folder=\"results\")\n",
        "\n",
        "# 결과는 ./results 폴더에 JSON 형식으로 저장됨\n"
      ],
      "metadata": {
        "id": "YtxIYfeBKLOI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "t9RDKGf9L0N3"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8yuSP3heMzT"
      },
      "source": [
        "### **예제: 지도 학습 분류**\n",
        "- **작업 방법**\n",
        "    1. **임베딩 모델을 사용해 특성을 생성**(임베딩 생성)\n",
        "    2. **특성을 분류기에 주입**(ex: 로지스틱 회귀로 분류)\n",
        "- **이점** :\n",
        "    - 비용이 많이 들 수 있는 임베딩 모델을 미세 튜닝할 필요 없다.\n",
        "    - CPU에서 로지스틱 회귀와 같은 분류기를 훈련하면 된다.\n",
        "- **사용 임베딩 모델**\n",
        "    - sentence-transformers/all-mpnet-base-v2"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. 임베딩 모델을 사용해 특성을 생성**(임베딩 생성)"
      ],
      "metadata": {
        "id": "PSxbr6p-EfsX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jGV9VS4bhq7f"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# 모델을 로드합니다.\n",
        "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
        "\n",
        "# 텍스트를 임베딩으로 변환합니다.\n",
        "train_embeddings = model.encode(list(data[\"train\"][\"text\"]), show_progress_bar=True)\n",
        "test_embeddings = model.encode(list(data[\"test\"][\"text\"]), show_progress_bar=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B5L5CLcOxIeA"
      },
      "outputs": [],
      "source": [
        "train_embeddings.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. 특성을 분류기에 주입**(ex: 로지스틱 회귀로 분류)"
      ],
      "metadata": {
        "id": "bC4qrX6rEsw9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8A7oTxoph6bn"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# 훈련 세트의 임베딩으로 로지스틱 회귀 모델을 훈련합니다.\n",
        "clf = LogisticRegression(random_state=42)\n",
        "clf.fit(train_embeddings, data[\"train\"][\"label\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. 모델 평가**"
      ],
      "metadata": {
        "id": "842xzHr1E8ZD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tFvO9KhMokF7"
      },
      "outputs": [],
      "source": [
        "# 테스트 세트 임베딩에 대해 예측을 수행합니다.\n",
        "y_pred = clf.predict(test_embeddings)\n",
        "evaluate_performance(data[\"test\"][\"label\"], y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dwGIHxXpJgrC"
      },
      "source": [
        "- 💡**팁!**  \n",
        "\n",
        "분류기를 사용하지 않는다면 어떻게 할 수 있을까요? 분류기 대신 클래스별 임베딩을 평균하고 코사인 유사도를 적용하여 문서와 가장 잘 맞는 클래스를 예측할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3f_DnG1uJ7Sk"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# 타깃 레이블에 대한 문서의 임베딩을 모두 평균하여 타깃 임베딩을 만듭니다.\n",
        "df = pd.DataFrame(np.hstack([train_embeddings, np.array(data[\"train\"][\"label\"]).reshape(-1, 1)]))\n",
        "averaged_target_embeddings = df.groupby(768).mean().values\n",
        "\n",
        "# 테스트 임베딩과 가장 가까운 타깃 임베딩을 찾습니다.\n",
        "sim_matrix = cosine_similarity(test_embeddings, averaged_target_embeddings)\n",
        "y_pred = np.argmax(sim_matrix, axis=1)\n",
        "\n",
        "# 모델을 평가합니다.\n",
        "evaluate_performance(data[\"test\"][\"label\"], y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCWdzjMIjzx0"
      },
      "source": [
        "### **예제: 데이터에 레이블이 없는 경우**\n",
        "\n",
        "- **제로샷 분류** (ZeroShotClassification) 적용\n",
        "- **원리**: 라벨을 “설명 문장(hypothesis)”으로 바꿔서, 자연어 추론(NLI, Natural Language Inference) 문제로 변환 후 모델이 참/거짓을 판단하도록 함.\n",
        "- **예**:\n",
        "    - 문장: \"이 영화는 정말 재밌었다\"\n",
        "    - 라벨 후보: [\"긍정\", \"부정\"]\n",
        "    - **변환** → \"이 문장은 긍정을 표현한다.\" (참/거짓 예측)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YSj6CdAetsNp"
      },
      "outputs": [],
      "source": [
        "# 레이블의 임베딩을 만듭니다.\n",
        "label_embeddings = model.encode([\"A negative review\",  \"A positive review\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZEIN7XnbtsQJ"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# 각 문서와 가장 잘 맞는 레이블을 찾습니다.\n",
        "sim_matrix = cosine_similarity(test_embeddings, label_embeddings)\n",
        "y_pred = np.argmax(sim_matrix, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u6LyeuEUxIbW"
      },
      "outputs": [],
      "source": [
        "evaluate_performance(data[\"test\"][\"label\"], y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ox27Rg71zclg"
      },
      "source": [
        "### **[실습] 제로샷 레이블을 변경하여 성능 향상시키기**\n",
        "\n",
        "- 레이블 설명을 다르게 설정하여 모델 평가하기\n",
        "    - A negative review --> **A very negative movie review**\n",
        "    - A positive review --> **A very positive movie review** 로 변경"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1OwdMqDnzuJ3"
      },
      "outputs": [],
      "source": [
        "label_embeddings = model.encode([\"A very negative movie review\",\n",
        "                                 \"A very positive movie review\"])\n",
        "\n",
        "sim_matrix = cosine_similarity(test_embeddings, label_embeddings)\n",
        "y_pred = np.argmax(sim_matrix, axis=1)\n",
        "\n",
        "evaluate_performance(data[\"test\"][\"label\"], y_pred)\n",
        "\n",
        "# 설명에 있는 레이블 내용(단어)를 통해 임베딩은 영화 리뷰임을 감지하고 좀 더 극단적인 레이블에 초점을 맞춘다."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "yIK_YERPL_KI"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CC9iEGcuUit"
      },
      "source": [
        "## **3. 생성 모델로 텍스트 분류하기**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **GPT 모델**과 같은 **생성 언어 모델**은 아무 맥락 없이 리뷰를 제공하면 모델이 무엇을 해야할지 알지 못합니다.\n",
        "- **모델이 맥락을 이해하도록 돕기 위해** 모델에 제공되는 명령(or **프롬프트**)을 통해 이루어집니다.\n",
        "- 원하는 출력을 얻도록 **프롬프트를 반복적으로 개선**하는 것을 **프롬프트 엔지니어링**(prompt engineering)이라고 부룹니다."
      ],
      "metadata": {
        "id": "23w2mmTDM1i5"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFPPzUHoEESB"
      },
      "source": [
        "### **예제: T5 모델 사용하기**\n",
        "- **T5: Text-To-Text Transfer Transformer**\n",
        "- 시퀀스-투-시퀀스 모델(Seq2Seq)\n",
        "- 원본 트랜스포머와 비슷하게 12개의 디코더+12개의 인코더를 쌓아서 구성됨\n",
        "- 마스크드 언어 모델링을 사용해 사전 훈렴됨\n",
        "- **각각의 작업을 시퀀스-투-시퀀스 작업으로 변환하고 동시에 훈련한다**.--> **다양한 작업에 대해 모델을 훈련할 수 있다**.\n",
        "- 이러한 미세 튜닝방법으로 GPT 모델에 가까운 수준으로 명령을 따른다. https://arxiv.org/abs/2210.11416\n",
        "- 이 작업으로 만들어진 모델이 **Flan-T5**다.\n",
        "- **다양한 Flan-T5 모델**\n",
        "    - google/flan-t5-base : https://huggingface.co/google/flan-t5-base\n",
        "    - google/flan-t5-small : https://huggingface.co/google/flan-t5-small\n",
        "    - google/flan-t5-large : https://huggingface.co/google/flan-t5-large\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Flan-T5 모델 로드하기**"
      ],
      "metadata": {
        "id": "knDcHnwJP2ae"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nVbTUMktEfJ3"
      },
      "outputs": [],
      "source": [
        "# 모델을 로드합니다.\n",
        "pipe = pipeline(\n",
        "    \"text2text-generation\",\n",
        "    model=\"google/flan-t5-small\",\n",
        "    device=\"cuda:0\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. 프롬프트 추가하기**"
      ],
      "metadata": {
        "id": "jH1LUxjdQwkh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o5nWQORcFlNn"
      },
      "outputs": [],
      "source": [
        "# 프롬프트를 추가합니다.\n",
        "prompt = \"Is the following sentence positive or negative? \"\n",
        "data = data.map(lambda example: {\"t5\": prompt + example['text']})\n",
        "data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. 테스트 데이터 실행**"
      ],
      "metadata": {
        "id": "PMiXjdsGRcCN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nas574KFFSvR"
      },
      "outputs": [],
      "source": [
        "# 추론을 실행합니다.\n",
        "y_pred = []\n",
        "for output in tqdm(pipe(KeyDataset(data[\"test\"], \"t5\")), total=len(data[\"test\"])):\n",
        "    text = output[0][\"generated_text\"]\n",
        "    y_pred.append(0 if text == \"negative\" else 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Wk2i856GnCv"
      },
      "outputs": [],
      "source": [
        "evaluate_performance(data[\"test\"][\"label\"], y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4V9iq_EELWx"
      },
      "source": [
        "### **예제: ChatGPT로 분류하기**\n",
        "- **클로즈드 모델**(closed model)\n",
        "- **훈련과정 소개** : https://openai.com/ko-KR/index/chatgpt/\n",
        "- **모델 생성 과정**\n",
        "    1. 입력 프롬프트에 대해 기대하는 출력을 수동으로 만든다.\n",
        "        - **모범 답안 학습하기**(지시 데이터:instruction data)\n",
        "        - --> 모델의 첫번째 버전 생성\n",
        "    2. 만들어진 모델을 사용해 여러 출력을 생성하고 가장 나쁜 에서 가장 좋은 것까지 수동으로 순위를 매김\n",
        "        - **선호도 데이터 사용**\n",
        "        - --> 최종 모델 만든다.\n",
        "    - 지시 데이터 대신 **선호도 데이터를 사용**하는 이점: **데이터가 표현하는 뉘앙스로 부터 모델이 사람의 기호에 맞는 출력을 생성하는 방법을 학습할 수 있음**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fJU5_X573XIT"
      },
      "outputs": [],
      "source": [
        "# httpx 패키지의 proxies 매개변수 오류를 피하기 위해\n",
        "# https://community.openai.com/t/error-with-openai-1-56-0-client-init-got-an-unexpected-keyword-argument-proxies/\n",
        "# !pip install openai==1.55.3 httpx==0.27.2 --force-reinstall --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 아래 명령어 실행시 gradio와 pydantic 버전 오류 무시하기\n",
        "!pip install openai httpx --upgrade"
      ],
      "metadata": {
        "id": "C1miCxyLbkE0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **클로즈드 모델 사용하는 방법** : **API사용**\n",
        "    1. **계정 생성** : https://openai.com/ko-KR/\n",
        "    2. **API key 생성** :  https://platform.openai.com/api-keys\n",
        "    3. **API key를 사용해 클라이언트(client) 만든다**.\n",
        "    - **사용량(Usage) 확인** : https://platform.openai.com/usage\n",
        "    - **가이드 확인**: https://platform.openai.com/docs/guides/rate-limits/retrying-with-exponential-backoff\n",
        "    "
      ],
      "metadata": {
        "id": "6rnYj3qIV8e6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tES6HFOwNjF6"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "from tqdm import tqdm\n",
        "\n",
        "# 클라이언트를 만듭니다.\n",
        "client = openai.OpenAI(api_key=\"YOUR_KEY_HERE\") # YOUR_KEY_HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **chatgpt_generation 함수 만들기**\n",
        "    - 프롬프트, 문서, 모델을 입력받아 텍스트 생성한다."
      ],
      "metadata": {
        "id": "oCFcYPyQdbOY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dGiovm3wyCOz"
      },
      "outputs": [],
      "source": [
        "def chatgpt_generation(prompt, document, model=\"gpt-4o-mini\"):\n",
        "    \"\"\"프롬프트와 문서를 입력받아 출력을 생성합니다.\"\"\"\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"You are a helpful assistant.\"\n",
        "            },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\":   prompt.replace(\"[DOCUMENT]\", document)\n",
        "            }\n",
        "    ]\n",
        "    chat_completion = client.chat.completions.create(\n",
        "      messages=messages,\n",
        "      model=model,\n",
        "      temperature=0\n",
        "    )\n",
        "    return chat_completion.choices[0].message.content"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **모델에게 요청하기 위한 템플릿 만들기**"
      ],
      "metadata": {
        "id": "hY_dZClHdwvd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qL_kMwQEvMcd"
      },
      "outputs": [],
      "source": [
        "# 프롬프트 템플릿을 정의합니다.\n",
        "prompt = \"\"\"Predict whether the following document is a positive or negative movie review:\n",
        "\n",
        "[DOCUMENT]\n",
        "\n",
        "If it is positive return 1 and if it is negative return 0. Do not give any other answers.\n",
        "\"\"\"\n",
        "\n",
        "# Predict the target using GPT\n",
        "document = \"unpretentious , charming , quirky , original\"\n",
        "chatgpt_generation(prompt, document)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ea-8XYpY3jp6"
      },
      "source": [
        "- 💡[**주의**] 다음 단계는 전체 테스트 데이터로 오픈AI 모델 중 하나를 실행하는 것입니다. 하지만 전체 테스트 데이터는 1,066개이므로 충분한 크레딧이 있을 때만 실행하세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gEyGElIv25Aq"
      },
      "outputs": [],
      "source": [
        "# 무료 크레딧을 아끼고 싶다면 이 코드를 건너 뛰세요.\n",
        "predictions = [chatgpt_generation(prompt, doc) for doc in tqdm(data[\"test\"][\"text\"])]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B07O8wtZ05x1"
      },
      "outputs": [],
      "source": [
        "# 예측을 저장합니다.\n",
        "y_pred = [int(pred) for pred in predictions]\n",
        "\n",
        "# 성능을 평가합니다.\n",
        "evaluate_performance(data[\"test\"][\"label\"], y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "WHQwAXriXRLp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **[참고] ChatGTP 훈련 방법**\n",
        "\n",
        "- 참고 : https://openai.com/ko-KR/index/chatgpt/\n",
        "- **인간의 피드백을 활용한 강화학습**(Reinforcement Learning from Human Feedback, RLHF)의 3가지 핵심 단계\n",
        "- ChatGPT와 같은 최신 대화형 AI 모델을 만드는 데 결정적인 역할을 한 기술\n",
        "- **ChatGPT 훈련단계 Diagram**\n",
        "\n",
        "![ChatGPT Diagram](https://images.ctfassets.net/kftzwdyauwt9/40in10B8KtAGrQvwRv5cop/8241bb17c283dced48ea034a41d7464a/chatgpt_diagram_light.png?w=1920&q=80&fm=webp)\n"
      ],
      "metadata": {
        "id": "L3HNXmDLXSWE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1단계: 지도 미세조정 (SFT) - 모범 답안 학습하기**\n",
        "이 단계는 AI에게 **좋은 답변이란 이런 것이다**라는 모범 사례를 직접 가르치는 과정입니다.\n",
        "\n",
        "- **① 질문 (Prompt)**: \"6살 아이에게 강화학습을 설명해줘\"와 같은 지시문(프롬프트)을 준비합니다.\n",
        "\n",
        "- **② 인간의 답변 작성** (Human Demonstration): 전문적인 데이터 라벨러가 이 질문에 대해 가장 이상적인, 고품질의 답변(\"간식과 벌을 주면서 가르치는 거야\")을 직접 작성합니다.\n",
        "\n",
        "- **③ 지도 미세조정** (Supervised Fine-Tuning, SFT): 이 '질문-모범 답변' 쌍을 거대한 언어 모델(LLM)에게 학습시킵니다. 모델은 이 과정을 통해 인간의 지시를 따르는 방법과 좋은 답변의 스타일 및 형식을 배우게 됩니다. 마치 학생에게 모범 답안지를 주고 그대로 따라 써보며 공부하게 하는 것과 같습니다.\n",
        "\n",
        "결론적으로 1단계는, 기본적인 언어 능력만 있던 AI가 인간의 지시를 잘 따르는 '기초 소양'을 갖추게 하는 과정입니다.\n"
      ],
      "metadata": {
        "id": "B89j6mpSYOFj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### **2단계: 보상 모델 학습 (RM) - 좋은 답변을 알아보는 심사위원 만들기**\n",
        "1단계만으로는 AI가 생성하는 다양한 답변 중 어떤 것이 더 나은지 판단할 수 없습니다. 그래서 이 단계에서는 **좋은 답변을 구별하는 눈을 가진 심사위원 AI**를 따로 만듭니다.\n",
        "\n",
        "- **① 여러 답변 생성**: 1단계에서 SFT를 거친 모델에게 동일한 질문(\"6살 아이에게 강화학습을 설명해줘\")을 주고, 여러 가지 다른 답변(A, B, C, D)을 생성하게 합니다.\n",
        "\n",
        "- **② 인간의 순위 평가** (Human Ranking): 인간 라벨러가 AI가 생성한 여러 답변을 읽어보고, 가장 좋은 답변부터 가장 나쁜 답변까지 순위를 매깁니다. 이미지에서는 D > C > A > B 순서로 평가했습니다.\n",
        "\n",
        "- **③ 보상 모델 학습** (Reward Model, RM): 이 '순위 데이터'를 가지고 별도의 AI 모델을 학습시킵니다. 이 모델이 바로 **보상 모델**(RM)입니다. RM은 답변을 직접 생성하지 않고, 대신 어떤 답변이 들어왔을 때 인간이 매긴 순위와 유사하게 '보상 점수'를 예측하도록 훈련됩니다. 즉, D 답변에는 높은 점수를, B 답변에는 낮은 점수를 주도록 학습하는 것입니다.\n",
        "\n",
        "결론적으로 2단계는, 어떤 답변이 인간의 선호도에 더 부합하는지를 점수로 평가할 수 있는 AI 심사위원을 만드는 과정입니다.\n",
        "\n"
      ],
      "metadata": {
        "id": "GQY_Wo73Yuyw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3단계: 강화학습을 통한 미세조정 (PPO) - 심사위원에게 칭찬받으며 실력 키우기**\n",
        "마지막 단계에서는 1단계에서 만든 '학생 AI'를 2단계에서 만든 '심사위원 AI'를 이용해 더욱 똑똑하게 만드는 본격적인 강화학습 과정입니다.\n",
        "\n",
        "- **① 새로운 질문과 답변 생성**: 1단계의 SFT 모델(이제부터는 '정책(Policy)'이라고 부릅니다)에게 새로운 질문(\"수달에 대한 이야기를 써줘\")을 던집니다. 모델은 학습한 대로 답변(\"옛날 옛적에...\")을 생성합니다.\n",
        "\n",
        "- **② 보상 모델의 평가**: 생성된 답변을 2단계에서 만든 '심사위원'인 보상 모델(RM)에게 보여줍니다. RM은 이 답변이 얼마나 좋은지 평가하여 **보상 점수**(rk)를 매깁니다.\n",
        "\n",
        "- **③ 강화학습 알고리즘(PPO) 적용**: 이 보상 점수를 이용해 '학생 AI'를 업데이트합니다.\n",
        "\n",
        "    - **높은 점수를 받으면** (칭찬): \"아, 이런 식으로 답변하니까 좋아하는구나!\"라고 배우며, 앞으로 그런 스타일의 답변을 더 잘 생성하도록 가중치를 조정합니다.\n",
        "\n",
        "    - **낮은 점수를 받으면** (지적): \"이런 답변은 별로인가 보네.\"라고 배우며, 해당 유형의 답변은 덜 생성하도록 가중치를 조정합니다.\n",
        "\n",
        "- **④ 반복**: 이 과정을 수없이 반복하며, '학생 AI'는 어떻게 하면 '심사위원 AI'로부터 더 높은 보상 점수를 받을 수 있는지 스스로 터득하며 성능을 점차 향상시킵니다.\n",
        "\n",
        "결론적으로 3단계는, AI가 스스로 답변을 생성하고, 인간의 선호도를 학습한 보상 모델로부터 피드백을 받으며 답변의 품질을 스스로 개선해나가는 핵심 과정입니다. 이 과정을 통해 AI는 단순히 지식을 나열하는 것을 넘어, 훨씬 더 자연스럽고 유용하며 안전한 답변을 생성할 수 있게 됩니다."
      ],
      "metadata": {
        "id": "LPQCpjUqYzV9"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "681949a153cc4c3e826aff6459bc7997": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_78d1e50360ca4a4d82c416762e93cf31",
              "IPY_MODEL_d4b809c7ec274adc9f73579d6e324f2b",
              "IPY_MODEL_196e9fd2b8d745bb80d66792341d4467"
            ],
            "layout": "IPY_MODEL_21883d250d164e01beda2ee94523a75e"
          }
        },
        "78d1e50360ca4a4d82c416762e93cf31": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7c33bd0f38624df8b8b55c340903f67d",
            "placeholder": "​",
            "style": "IPY_MODEL_be081cd2128347ac82e87804b3e63b7b",
            "value": "Generating train split: 100%"
          }
        },
        "d4b809c7ec274adc9f73579d6e324f2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c440febc76554cd2a5e76daa86509cb6",
            "max": 8530,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9ea838a9079c46499303a9db4ce4381e",
            "value": 8530
          }
        },
        "196e9fd2b8d745bb80d66792341d4467": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2532eddeed7147bcb908fa2b6cd4258f",
            "placeholder": "​",
            "style": "IPY_MODEL_1cd5f9b349aa4ed188c9f7ffc4fa74e8",
            "value": " 8530/8530 [00:00&lt;00:00, 66708.21 examples/s]"
          }
        },
        "21883d250d164e01beda2ee94523a75e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7c33bd0f38624df8b8b55c340903f67d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "be081cd2128347ac82e87804b3e63b7b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c440febc76554cd2a5e76daa86509cb6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9ea838a9079c46499303a9db4ce4381e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2532eddeed7147bcb908fa2b6cd4258f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1cd5f9b349aa4ed188c9f7ffc4fa74e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "530a891c6d43499783e0858f94deb934": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_39faf577742646f8b5e544287512fe98",
              "IPY_MODEL_cf2c3b9ea1fa44baa88aa092190742c7",
              "IPY_MODEL_7bed481258cd45dd99c7477ee9f201b3"
            ],
            "layout": "IPY_MODEL_954f80acde6b4bfc9243815a0ea961f6"
          }
        },
        "39faf577742646f8b5e544287512fe98": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0811aff0b764444eb85e5ea132518e92",
            "placeholder": "​",
            "style": "IPY_MODEL_916a6887790a4276839833ccdf1ee5f5",
            "value": "Generating validation split: 100%"
          }
        },
        "cf2c3b9ea1fa44baa88aa092190742c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dc4b9280da334a4593ea8154ee355958",
            "max": 1066,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1b85a9e1e5e2428297b3c63c90221336",
            "value": 1066
          }
        },
        "7bed481258cd45dd99c7477ee9f201b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ca8de1d521ac42fba4f0d36f9aebb4c1",
            "placeholder": "​",
            "style": "IPY_MODEL_869d6bcb99ef43218cb5050f89ff832c",
            "value": " 1066/1066 [00:00&lt;00:00, 11158.24 examples/s]"
          }
        },
        "954f80acde6b4bfc9243815a0ea961f6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0811aff0b764444eb85e5ea132518e92": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "916a6887790a4276839833ccdf1ee5f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dc4b9280da334a4593ea8154ee355958": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1b85a9e1e5e2428297b3c63c90221336": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ca8de1d521ac42fba4f0d36f9aebb4c1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "869d6bcb99ef43218cb5050f89ff832c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a5a12e9a9330453a97f4ad7942a2e535": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_072d23b6a2ee43d9ae687ce33212df0e",
              "IPY_MODEL_fdd0ccb937944571bef96d0d2a9bc84e",
              "IPY_MODEL_1d80e501e05a4104908db3c853d45e98"
            ],
            "layout": "IPY_MODEL_1d105c8f7d354f97b52b1e0b042e2a1a"
          }
        },
        "072d23b6a2ee43d9ae687ce33212df0e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5387c5bbf1304cf0a0ec889665f0b0f4",
            "placeholder": "​",
            "style": "IPY_MODEL_80c29c3bbbed44a39f956aef7ea8f6b7",
            "value": "Generating test split: 100%"
          }
        },
        "fdd0ccb937944571bef96d0d2a9bc84e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_21756bab6ca547c5a61c450b926d3fa9",
            "max": 1066,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2f3952789c80482fac76daa9f3377945",
            "value": 1066
          }
        },
        "1d80e501e05a4104908db3c853d45e98": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_51b22df98b0d4e30aa29d5e95e00211e",
            "placeholder": "​",
            "style": "IPY_MODEL_6def8b7c763c44f1943a07b54244cd82",
            "value": " 1066/1066 [00:00&lt;00:00, 8172.40 examples/s]"
          }
        },
        "1d105c8f7d354f97b52b1e0b042e2a1a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5387c5bbf1304cf0a0ec889665f0b0f4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "80c29c3bbbed44a39f956aef7ea8f6b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "21756bab6ca547c5a61c450b926d3fa9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2f3952789c80482fac76daa9f3377945": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "51b22df98b0d4e30aa29d5e95e00211e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6def8b7c763c44f1943a07b54244cd82": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}