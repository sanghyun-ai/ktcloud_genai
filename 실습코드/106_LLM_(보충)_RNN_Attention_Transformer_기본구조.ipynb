{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanghyun-ai/ktcloud_genai/blob/main/%EC%8B%A4%EC%8A%B5%EC%BD%94%EB%93%9C/106_LLM_(%EB%B3%B4%EC%B6%A9)_RNN_Attention_Transformer_%EA%B8%B0%EB%B3%B8%EA%B5%AC%EC%A1%B0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **RNN vs Seq2Seq vs Attention vs Transformer**"
      ],
      "metadata": {
        "id": "Kt3UZ2gRcqqm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "p6ajhPyPdN3b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **장거리 의존성 문제(long-term dependency problem)**와\n",
        "- **병렬 처리의 한계를 해결하는 방향으로 발전**함"
      ],
      "metadata": {
        "id": "veeRqs3glTKF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. RNN(Recurrent Neural Network)**\n"
      ],
      "metadata": {
        "id": "2a6-GJV4Ykeq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class SimpleRNN(nn.Module):\n",
        "    \"\"\"기본 RNN을 이용한 번역 모델 (교육용 간소화 버전)\"\"\"\n",
        "\n",
        "    def __init__(self, input_vocab_size, output_vocab_size, hidden_size):\n",
        "        super(SimpleRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # 임베딩 레이어\n",
        "        self.embedding = nn.Embedding(input_vocab_size, hidden_size)\n",
        "\n",
        "        # RNN 레이어 (순차적 처리)\n",
        "        self.rnn = nn.RNN(hidden_size, hidden_size, batch_first=True)\n",
        "\n",
        "        # 출력 레이어\n",
        "        self.fc = nn.Linear(hidden_size, output_vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch_size, sequence_length)\n",
        "\n",
        "        # 임베딩: (batch_size, seq_len, hidden_size)\n",
        "        embedded = self.embedding(x)\n",
        "\n",
        "        # RNN 처리 (순차적으로 한 단어씩 처리)\n",
        "        # output: (batch_size, seq_len, hidden_size)\n",
        "        # hidden: (1, batch_size, hidden_size) - 마지막 hidden state\n",
        "        output, hidden = self.rnn(embedded)\n",
        "\n",
        "        # 각 시점의 출력을 단어 확률로 변환\n",
        "        predictions = self.fc(output)\n",
        "\n",
        "        return predictions, hidden\n",
        "\n",
        "# 사용 예시\n",
        "input_vocab_size = 1000   # 한국어 단어 수\n",
        "output_vocab_size = 1000  # 영어 단어 수\n",
        "hidden_size = 256\n",
        "\n",
        "model = SimpleRNN(input_vocab_size, output_vocab_size, hidden_size)\n",
        "\n",
        "# 예시 입력: \"나는 점심 식사로 파스타를 먹을 예정입니다\" (토큰화된 인덱스)\n",
        "sample_input = torch.tensor([[10, 25, 43, 87, 102, 156]])  # shape: (1, 6)\n",
        "\n",
        "output, hidden = model(sample_input)\n",
        "print(f\"RNN 출력 shape: {output.shape}\")  # (1, 6, 1000)\n",
        "print(f\"최종 hidden state shape: {hidden.shape}\")  # (1, 1, 256)\n",
        "print(\"문제점: 긴 문장에서 '나는'의 정보가 마지막까지 전달되기 어려움!\")"
      ],
      "metadata": {
        "id": "QcXuZ49lYdq8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6bcac77-2a20-43d3-a763-22486caa82be"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RNN 출력 shape: torch.Size([1, 6, 1000])\n",
            "최종 hidden state shape: torch.Size([1, 1, 256])\n",
            "문제점: 긴 문장에서 '나는'의 정보가 마지막까지 전달되기 어려움!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 예제 1: Hidden State의 실제 값 확인하기"
      ],
      "metadata": {
        "id": "vHOgNvo3oBBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# 간단한 RNN 모델\n",
        "class SimpleRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(SimpleRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch_size, sequence_length, input_size)\n",
        "        # output: 모든 시점의 hidden state\n",
        "        # hidden: 마지막 hidden state\n",
        "        output, hidden = self.rnn(x)\n",
        "        return output, hidden\n",
        "\n",
        "# 모델 생성\n",
        "input_size = 10    # 단어 임베딩 차원\n",
        "hidden_size = 20   # hidden state 차원\n",
        "model = SimpleRNN(input_size, hidden_size)\n",
        "\n",
        "# 예시 입력: 6개 단어 (\"나는\", \"점심\", \"식사로\", \"파스타를\", \"먹을\", \"예정입니다\")\n",
        "# 실제로는 단어를 벡터로 변환한 값\n",
        "sequence_length = 6\n",
        "batch_size = 1\n",
        "x = torch.randn(batch_size, sequence_length, input_size)\n",
        "\n",
        "# Forward pass\n",
        "all_hidden_states, final_hidden = model(x)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"각 시점의 Hidden State 확인\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for t in range(sequence_length):\n",
        "    words = [\"나는\", \"점심\", \"식사로\", \"파스타를\", \"먹을\", \"예정입니다\"]\n",
        "    print(f\"\\nh{t+1} ('{words[t]}' 처리 후):\")\n",
        "    print(f\"  Shape: {all_hidden_states[0, t].shape}\")  # (hidden_size,)\n",
        "    print(f\"  값 샘플 (처음 5개): {all_hidden_states[0, t, :5].detach().numpy()}\")\n",
        "    print(f\"  평균: {all_hidden_states[0, t].mean().item():.4f}\")\n",
        "    print(f\"  표준편차: {all_hidden_states[0, t].std().item():.4f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"최종 Hidden State (h6):\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Shape: {final_hidden.shape}\")  # (1, batch_size, hidden_size)\n",
        "print(f\"이것이 전체 문장의 의미를 압축한 벡터입니다!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MAStQ1N9n_9I",
        "outputId": "2764a28c-ee59-44e8-e4ff-708295faac1f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "각 시점의 Hidden State 확인\n",
            "============================================================\n",
            "\n",
            "h1 ('나는' 처리 후):\n",
            "  Shape: torch.Size([20])\n",
            "  값 샘플 (처음 5개): [ 0.29985452 -0.2139289  -0.29840955 -0.04969395 -0.67423046]\n",
            "  평균: 0.0983\n",
            "  표준편차: 0.4239\n",
            "\n",
            "h2 ('점심' 처리 후):\n",
            "  Shape: torch.Size([20])\n",
            "  값 샘플 (처음 5개): [ 0.09721795 -0.07027858 -0.3024618   0.06140145  0.04500112]\n",
            "  평균: -0.0653\n",
            "  표준편차: 0.3114\n",
            "\n",
            "h3 ('식사로' 처리 후):\n",
            "  Shape: torch.Size([20])\n",
            "  값 샘플 (처음 5개): [-0.29584005  0.23863187  0.31218764 -0.49818477  0.56312275]\n",
            "  평균: -0.0041\n",
            "  표준편차: 0.4587\n",
            "\n",
            "h4 ('파스타를' 처리 후):\n",
            "  Shape: torch.Size([20])\n",
            "  값 샘플 (처음 5개): [-0.319365    0.59877855 -0.7083339  -0.49061483  0.03293834]\n",
            "  평균: 0.0104\n",
            "  표준편차: 0.4240\n",
            "\n",
            "h5 ('먹을' 처리 후):\n",
            "  Shape: torch.Size([20])\n",
            "  값 샘플 (처음 5개): [ 0.45612144 -0.30095637 -0.48437113 -0.0419737  -0.05000209]\n",
            "  평균: 0.0824\n",
            "  표준편차: 0.3501\n",
            "\n",
            "h6 ('예정입니다' 처리 후):\n",
            "  Shape: torch.Size([20])\n",
            "  값 샘플 (처음 5개): [ 0.19061457  0.06515525 -0.1807474  -0.34183508 -0.0019587 ]\n",
            "  평균: -0.0002\n",
            "  표준편차: 0.4072\n",
            "\n",
            "============================================================\n",
            "최종 Hidden State (h6):\n",
            "============================================================\n",
            "Shape: torch.Size([1, 1, 20])\n",
            "이것이 전체 문장의 의미를 압축한 벡터입니다!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 예제 2: Hidden State가 어떻게 변화하는지 시각화"
      ],
      "metadata": {
        "id": "-d6aHaseol6G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "# 더 상세한 RNN 클래스\n",
        "class RNNWithHiddenTracking(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_size):\n",
        "        super(RNNWithHiddenTracking, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.rnn = nn.RNN(embedding_dim, hidden_size, batch_first=True)\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "\n",
        "        # 각 시점의 hidden state를 수동으로 계산하여 추적\n",
        "        batch_size = x.shape[0]\n",
        "        seq_len = x.shape[1]\n",
        "\n",
        "        # 초기 hidden state (h0) - 보통 0으로 초기화\n",
        "        h = torch.zeros(1, batch_size, self.hidden_size)\n",
        "\n",
        "        hidden_states = []\n",
        "\n",
        "        for t in range(seq_len):\n",
        "            # 한 단어씩 처리\n",
        "            x_t = embedded[:, t:t+1, :]  # (batch_size, 1, embedding_dim)\n",
        "            out, h = self.rnn(x_t, h)\n",
        "            hidden_states.append(h.squeeze(0))  # (batch_size, hidden_size)\n",
        "\n",
        "        return torch.stack(hidden_states, dim=1), h\n",
        "\n",
        "# 사용 예시\n",
        "vocab_size = 1000\n",
        "embedding_dim = 50\n",
        "hidden_size = 128\n",
        "\n",
        "model = RNNWithHiddenTracking(vocab_size, embedding_dim, hidden_size)\n",
        "\n",
        "# 예시 문장: \"나는 점심 식사로 파스타를 먹을 예정입니다\"\n",
        "# (실제로는 토큰 ID로 변환된 값)\n",
        "sentence = torch.tensor([[10, 25, 43, 87, 102, 156]])  # shape: (1, 6)\n",
        "words = [\"나는\", \"점심\", \"식사로\", \"파스타를\", \"먹을\", \"예정입니다\"]\n",
        "\n",
        "# Forward pass\n",
        "all_hiddens, final_hidden = model(sentence)\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"Hidden State의 변화 추적\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# 각 hidden state의 통계 비교\n",
        "for t in range(len(words)):\n",
        "    h_t = all_hiddens[0, t]  # t번째 hidden state\n",
        "\n",
        "    print(f\"\\n[시점 {t+1}] '{words[t]}' 처리 후 - h{t+1}\")\n",
        "    print(f\"  크기: {h_t.shape}\")\n",
        "    print(f\"  평균값: {h_t.mean().item():.6f}\")\n",
        "    print(f\"  최댓값: {h_t.max().item():.6f}\")\n",
        "    print(f\"  최솟값: {h_t.min().item():.6f}\")\n",
        "\n",
        "    # 이전 hidden state와의 변화량 계산\n",
        "    if t > 0:\n",
        "        h_prev = all_hiddens[0, t-1]\n",
        "        change = torch.abs(h_t - h_prev).mean().item()\n",
        "        print(f\"  이전 상태와의 변화량: {change:.6f}\")\n",
        "        print(f\"    → 새로운 단어 '{words[t]}'의 정보가 추가됨!\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"핵심 포인트:\")\n",
        "print(\"=\" * 70)\n",
        "print(\"✓ h1은 '나는'만 보고 만들어진 벡터\")\n",
        "print(\"✓ h2는 h1 + '점심' 정보를 결합한 벡터\")\n",
        "print(\"✓ h3은 h2 + '식사로' 정보를 결합한 벡터\")\n",
        "print(\"✓ ...\")\n",
        "print(\"✓ h6은 전체 문장의 의미를 담은 최종 벡터\")\n",
        "print(\"\\n하지만 RNN의 문제: h6에서 '나는'의 정보가 많이 희미해짐! (기울기 소실)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Qw-LdD3omH0",
        "outputId": "bad4ae40-d672-4b59-d6da-85e7e97652df"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "Hidden State의 변화 추적\n",
            "======================================================================\n",
            "\n",
            "[시점 1] '나는' 처리 후 - h1\n",
            "  크기: torch.Size([128])\n",
            "  평균값: -0.008030\n",
            "  최댓값: 0.622678\n",
            "  최솟값: -0.781650\n",
            "\n",
            "[시점 2] '점심' 처리 후 - h2\n",
            "  크기: torch.Size([128])\n",
            "  평균값: -0.021931\n",
            "  최댓값: 0.746920\n",
            "  최솟값: -0.701562\n",
            "  이전 상태와의 변화량: 0.320961\n",
            "    → 새로운 단어 '점심'의 정보가 추가됨!\n",
            "\n",
            "[시점 3] '식사로' 처리 후 - h3\n",
            "  크기: torch.Size([128])\n",
            "  평균값: -0.008400\n",
            "  최댓값: 0.688216\n",
            "  최솟값: -0.808804\n",
            "  이전 상태와의 변화량: 0.306977\n",
            "    → 새로운 단어 '식사로'의 정보가 추가됨!\n",
            "\n",
            "[시점 4] '파스타를' 처리 후 - h4\n",
            "  크기: torch.Size([128])\n",
            "  평균값: -0.015400\n",
            "  최댓값: 0.731057\n",
            "  최솟값: -0.824842\n",
            "  이전 상태와의 변화량: 0.324786\n",
            "    → 새로운 단어 '파스타를'의 정보가 추가됨!\n",
            "\n",
            "[시점 5] '먹을' 처리 후 - h5\n",
            "  크기: torch.Size([128])\n",
            "  평균값: -0.024338\n",
            "  최댓값: 0.724042\n",
            "  최솟값: -0.778063\n",
            "  이전 상태와의 변화량: 0.379232\n",
            "    → 새로운 단어 '먹을'의 정보가 추가됨!\n",
            "\n",
            "[시점 6] '예정입니다' 처리 후 - h6\n",
            "  크기: torch.Size([128])\n",
            "  평균값: 0.009061\n",
            "  최댓값: 0.846417\n",
            "  최솟값: -0.811501\n",
            "  이전 상태와의 변화량: 0.442882\n",
            "    → 새로운 단어 '예정입니다'의 정보가 추가됨!\n",
            "\n",
            "======================================================================\n",
            "핵심 포인트:\n",
            "======================================================================\n",
            "✓ h1은 '나는'만 보고 만들어진 벡터\n",
            "✓ h2는 h1 + '점심' 정보를 결합한 벡터\n",
            "✓ h3은 h2 + '식사로' 정보를 결합한 벡터\n",
            "✓ ...\n",
            "✓ h6은 전체 문장의 의미를 담은 최종 벡터\n",
            "\n",
            "하지만 RNN의 문제: h6에서 '나는'의 정보가 많이 희미해짐! (기울기 소실)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 예제 3: Hidden State 간 유사도 비교"
      ],
      "metadata": {
        "id": "zHl87oteo4UF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def cosine_similarity_matrix(hidden_states):\n",
        "    \"\"\"각 hidden state 간의 코사인 유사도 계산\"\"\"\n",
        "    # hidden_states: (seq_len, hidden_size)\n",
        "\n",
        "    # 정규화\n",
        "    normalized = F.normalize(hidden_states, p=2, dim=1)\n",
        "\n",
        "    # 유사도 행렬 계산\n",
        "    similarity = torch.mm(normalized, normalized.t())\n",
        "\n",
        "    return similarity\n",
        "\n",
        "# 모델 생성 및 실행\n",
        "model = RNNWithHiddenTracking(vocab_size=1000, embedding_dim=50, hidden_size=128)\n",
        "sentence = torch.tensor([[10, 25, 43, 87, 102, 156]])\n",
        "all_hiddens, _ = model(sentence)\n",
        "\n",
        "# Hidden states 추출 (batch 차원 제거)\n",
        "hidden_states = all_hiddens.squeeze(0)  # (6, 128)\n",
        "\n",
        "# 유사도 행렬 계산\n",
        "similarity_matrix = cosine_similarity_matrix(hidden_states)\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"Hidden State 간 유사도 분석\")\n",
        "print(\"=\" * 70)\n",
        "print(\"\\n코사인 유사도 행렬:\")\n",
        "print(\"(1.0 = 완전히 같음, 0.0 = 완전히 다름)\\n\")\n",
        "\n",
        "words = [\"나는\", \"점심\", \"식사로\", \"파스타를\", \"먹을\", \"예정입니다\"]\n",
        "\n",
        "# 헤더 출력\n",
        "print(\"        \", end=\"\")\n",
        "for w in words:\n",
        "    print(f\"{w:>8}\", end=\"\")\n",
        "print()\n",
        "\n",
        "# 행렬 출력\n",
        "for i, word in enumerate(words):\n",
        "    print(f\"{word:>6}  \", end=\"\")\n",
        "    for j in range(len(words)):\n",
        "        print(f\"{similarity_matrix[i, j].item():>8.4f}\", end=\"\")\n",
        "    print()\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"분석:\")\n",
        "print(\"=\" * 70)\n",
        "print(\"✓ 대각선 값들은 1.0 (자기 자신과의 유사도)\")\n",
        "print(\"✓ h1과 h6의 유사도가 낮음 → '나는' 정보가 h6에 적게 남음\")\n",
        "print(\"✓ 인접한 hidden state들의 유사도가 높음 → 점진적 변화\")\n",
        "print(\"✓ 이것이 RNN의 장기 의존성(long-term dependency) 문제!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "anxUTxqpo4dd",
        "outputId": "b9fa2aed-0a37-41cb-aad0-025271afb5ce"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "Hidden State 간 유사도 분석\n",
            "======================================================================\n",
            "\n",
            "코사인 유사도 행렬:\n",
            "(1.0 = 완전히 같음, 0.0 = 완전히 다름)\n",
            "\n",
            "              나는      점심     식사로    파스타를      먹을   예정입니다\n",
            "    나는    1.0000 -0.1717 -0.1588 -0.0879  0.1138  0.1544\n",
            "    점심   -0.1717  1.0000  0.1461  0.0233 -0.1443 -0.1860\n",
            "   식사로   -0.1588  0.1461  1.0000  0.1548 -0.1451 -0.1732\n",
            "  파스타를   -0.0879  0.0233  0.1548  1.0000  0.1298 -0.1235\n",
            "    먹을    0.1138 -0.1443 -0.1451  0.1298  1.0000  0.2857\n",
            " 예정입니다    0.1544 -0.1860 -0.1732 -0.1235  0.2857  1.0000\n",
            "\n",
            "======================================================================\n",
            "분석:\n",
            "======================================================================\n",
            "✓ 대각선 값들은 1.0 (자기 자신과의 유사도)\n",
            "✓ h1과 h6의 유사도가 낮음 → '나는' 정보가 h6에 적게 남음\n",
            "✓ 인접한 hidden state들의 유사도가 높음 → 점진적 변화\n",
            "✓ 이것이 RNN의 장기 의존성(long-term dependency) 문제!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "9oL_NRbAokGQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. Seq2Seq with Attention(간소화 버전)**\n",
        "- Seq2Seq + Attention**"
      ],
      "metadata": {
        "id": "Tgs2edDclmQu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    \"\"\"Seq2Seq의 Encoder 부분\"\"\"\n",
        "\n",
        "    def __init__(self, input_vocab_size, embedding_dim, hidden_size):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(input_vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_size, batch_first=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch_size, seq_len)\n",
        "        embedded = self.embedding(x)  # (batch_size, seq_len, embedding_dim)\n",
        "\n",
        "        # 모든 시점의 hidden state 반환 (Attention을 위해 필요)\n",
        "        outputs, (hidden, cell) = self.lstm(embedded)\n",
        "        # outputs: (batch_size, seq_len, hidden_size) - 모든 시점의 hidden\n",
        "        # hidden: (1, batch_size, hidden_size) - 마지막 hidden\n",
        "\n",
        "        return outputs, hidden, cell\n",
        "\n",
        "\n",
        "class AttentionDecoder(nn.Module):\n",
        "    \"\"\"Attention 메커니즘을 사용하는 Decoder\"\"\"\n",
        "\n",
        "    def __init__(self, output_vocab_size, embedding_dim, hidden_size):\n",
        "        super(AttentionDecoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(output_vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim + hidden_size, hidden_size, batch_first=True)\n",
        "\n",
        "        # Attention 계산을 위한 레이어\n",
        "        self.attention = nn.Linear(hidden_size * 2, 1)\n",
        "        self.fc = nn.Linear(hidden_size, output_vocab_size)\n",
        "\n",
        "    def forward(self, x, encoder_outputs, hidden, cell):\n",
        "        # x: (batch_size, 1) - 현재 생성할 단어\n",
        "        # encoder_outputs: (batch_size, src_len, hidden_size)\n",
        "\n",
        "        embedded = self.embedding(x)  # (batch_size, 1, embedding_dim)\n",
        "\n",
        "        # Attention 가중치 계산\n",
        "        batch_size = encoder_outputs.shape[0]\n",
        "        src_len = encoder_outputs.shape[1]\n",
        "\n",
        "        # hidden을 src_len만큼 복사\n",
        "        hidden_repeated = hidden.permute(1, 0, 2).repeat(1, src_len, 1)\n",
        "        # (batch_size, src_len, hidden_size)\n",
        "\n",
        "        # Encoder의 각 hidden state와 현재 decoder hidden을 결합\n",
        "        energy = torch.cat([hidden_repeated, encoder_outputs], dim=2)\n",
        "        # (batch_size, src_len, hidden_size*2)\n",
        "\n",
        "        # Attention score 계산\n",
        "        attention_scores = self.attention(energy).squeeze(2)\n",
        "        # (batch_size, src_len)\n",
        "\n",
        "        # Softmax로 가중치 변환\n",
        "        attention_weights = F.softmax(attention_scores, dim=1)\n",
        "        # (batch_size, src_len)\n",
        "\n",
        "        # Context vector 계산 (가중합)\n",
        "        context = torch.bmm(attention_weights.unsqueeze(1), encoder_outputs)\n",
        "        # (batch_size, 1, hidden_size)\n",
        "\n",
        "        # Context와 embedding을 결합하여 LSTM 입력\n",
        "        lstm_input = torch.cat([embedded, context], dim=2)\n",
        "        # (batch_size, 1, embedding_dim + hidden_size)\n",
        "\n",
        "        output, (hidden, cell) = self.lstm(lstm_input, (hidden, cell))\n",
        "\n",
        "        # 최종 단어 예측\n",
        "        prediction = self.fc(output.squeeze(1))\n",
        "        # (batch_size, output_vocab_size)\n",
        "\n",
        "        return prediction, hidden, cell, attention_weights\n",
        "\n",
        "\n",
        "class Seq2SeqWithAttention(nn.Module):\n",
        "    \"\"\"Attention을 사용하는 완전한 Seq2Seq 모델\"\"\"\n",
        "\n",
        "    def __init__(self, input_vocab_size, output_vocab_size, embedding_dim, hidden_size):\n",
        "        super(Seq2SeqWithAttention, self).__init__()\n",
        "        self.encoder = Encoder(input_vocab_size, embedding_dim, hidden_size)\n",
        "        self.decoder = AttentionDecoder(output_vocab_size, embedding_dim, hidden_size)\n",
        "\n",
        "    def forward(self, src, trg):\n",
        "        # src: (batch_size, src_len) - \"나는 점심 식사로...\"\n",
        "        # trg: (batch_size, trg_len) - \"I am going to...\"\n",
        "\n",
        "        # Encoding\n",
        "        encoder_outputs, hidden, cell = self.encoder(src)\n",
        "\n",
        "        # Decoding with Attention\n",
        "        batch_size = src.shape[0]\n",
        "        trg_len = trg.shape[1]\n",
        "        trg_vocab_size = self.decoder.fc.out_features\n",
        "\n",
        "        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size)\n",
        "        attention_weights_all = []\n",
        "\n",
        "        # 첫 입력은 <START> 토큰\n",
        "        decoder_input = trg[:, 0].unsqueeze(1)\n",
        "\n",
        "        for t in range(1, trg_len):\n",
        "            # 한 단어씩 생성\n",
        "            output, hidden, cell, attention_weights = self.decoder(\n",
        "                decoder_input, encoder_outputs, hidden, cell\n",
        "            )\n",
        "\n",
        "            outputs[:, t] = output\n",
        "            attention_weights_all.append(attention_weights)\n",
        "\n",
        "            # 다음 입력 (teacher forcing 사용)\n",
        "            decoder_input = trg[:, t].unsqueeze(1)\n",
        "\n",
        "        return outputs, attention_weights_all\n",
        "\n",
        "\n",
        "# 사용 예시\n",
        "model = Seq2SeqWithAttention(\n",
        "    input_vocab_size=1000,\n",
        "    output_vocab_size=1000,\n",
        "    embedding_dim=256,\n",
        "    hidden_size=512\n",
        ")\n",
        "\n",
        "# 예시 데이터\n",
        "src = torch.tensor([[10, 25, 43, 87, 102, 156]])  # \"나는 점심 식사로 파스타를 먹을 예정입니다\"\n",
        "trg = torch.tensor([[1, 5, 8, 12, 15, 20, 30]])   # \"<START> I am going to eat pasta\"\n",
        "\n",
        "outputs, attention_weights = model(src, trg)\n",
        "\n",
        "print(f\"Seq2Seq 출력 shape: {outputs.shape}\")\n",
        "print(f\"Attention 가중치 개수: {len(attention_weights)}\")\n",
        "print(\"\\n'eat' 생성 시 Attention 분포 (예시):\")\n",
        "print(\"나는(0.02), 점심(0.05), 식사로(0.1), 파스타를(0.6), 먹을(0.2), 예정입니다(0.03)\")\n",
        "print(\"→ '파스타를'과 '먹을'에 집중!\")"
      ],
      "metadata": {
        "id": "AnfYBufnhpJa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0219e3c4-5ad6-41ae-eea2-44000edcaf9a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seq2Seq 출력 shape: torch.Size([1, 7, 1000])\n",
            "Attention 가중치 개수: 6\n",
            "\n",
            "'eat' 생성 시 Attention 분포 (예시):\n",
            "나는(0.02), 점심(0.05), 식사로(0.1), 파스타를(0.6), 먹을(0.2), 예정입니다(0.03)\n",
            "→ '파스타를'과 '먹을'에 집중!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U_VbPxLPpVQY"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "NJaeCPyBpU-E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3. Transformer구현(간소화 버전)**"
      ],
      "metadata": {
        "id": "UFgoi4WccrQN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\"Transformer의 핵심: Multi-Head Self-Attention\"\"\"\n",
        "\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        assert d_model % num_heads == 0\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.d_k = d_model // num_heads\n",
        "\n",
        "        # Query, Key, Value 변환을 위한 linear layer\n",
        "        self.W_q = nn.Linear(d_model, d_model)\n",
        "        self.W_k = nn.Linear(d_model, d_model)\n",
        "        self.W_v = nn.Linear(d_model, d_model)\n",
        "        self.W_o = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
        "        # Q, K, V: (batch_size, num_heads, seq_len, d_k)\n",
        "\n",
        "        # Attention score 계산\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
        "        # (batch_size, num_heads, seq_len, seq_len)\n",
        "\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "        # Softmax로 가중치 계산\n",
        "        attention_weights = torch.softmax(scores, dim=-1)\n",
        "\n",
        "        # Value에 가중치 적용\n",
        "        output = torch.matmul(attention_weights, V)\n",
        "\n",
        "        return output, attention_weights\n",
        "\n",
        "    def forward(self, Q, K, V, mask=None):\n",
        "        batch_size = Q.shape[0]\n",
        "\n",
        "        # Linear transformation\n",
        "        Q = self.W_q(Q)  # (batch_size, seq_len, d_model)\n",
        "        K = self.W_k(K)\n",
        "        V = self.W_v(V)\n",
        "\n",
        "        # Multi-head로 분할\n",
        "        Q = Q.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        K = K.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        V = V.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        # (batch_size, num_heads, seq_len, d_k)\n",
        "\n",
        "        # Attention 계산\n",
        "        attn_output, attention_weights = self.scaled_dot_product_attention(Q, K, V, mask)\n",
        "\n",
        "        # Head들을 다시 결합\n",
        "        attn_output = attn_output.transpose(1, 2).contiguous()\n",
        "        attn_output = attn_output.view(batch_size, -1, self.d_model)\n",
        "\n",
        "        # 최종 linear transformation\n",
        "        output = self.W_o(attn_output)\n",
        "\n",
        "        return output, attention_weights\n",
        "\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    \"\"\"단어의 위치 정보를 인코딩 (RNN 없이 순서 보존)\"\"\"\n",
        "\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "\n",
        "        # 위치 인코딩 계산\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() *\n",
        "                            (-math.log(10000.0) / d_model))\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        pe = pe.unsqueeze(0)  # (1, max_len, d_model)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch_size, seq_len, d_model)\n",
        "        seq_len = x.shape[1]\n",
        "        return x + self.pe[:, :seq_len, :]\n",
        "\n",
        "\n",
        "class TransformerEncoderLayer(nn.Module):\n",
        "    \"\"\"Transformer Encoder의 한 레이어\"\"\"\n",
        "\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "        super(TransformerEncoderLayer, self).__init__()\n",
        "\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(d_model, d_ff),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(d_ff, d_model)\n",
        "        )\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        # Self-Attention (모든 단어가 서로 관계 파악)\n",
        "        attn_output, attn_weights = self.self_attn(x, x, x, mask)\n",
        "        x = self.norm1(x + self.dropout(attn_output))  # Residual connection\n",
        "\n",
        "        # Feed Forward\n",
        "        ff_output = self.feed_forward(x)\n",
        "        x = self.norm2(x + self.dropout(ff_output))  # Residual connection\n",
        "\n",
        "        return x, attn_weights\n",
        "\n",
        "\n",
        "class SimpleTransformer(nn.Module):\n",
        "    \"\"\"간소화된 Transformer 번역 모델\"\"\"\n",
        "\n",
        "    def __init__(self, src_vocab_size, trg_vocab_size, d_model=512,\n",
        "                 num_heads=8, num_layers=6, d_ff=2048, dropout=0.1):\n",
        "        super(SimpleTransformer, self).__init__()\n",
        "\n",
        "        # Embedding layers\n",
        "        self.src_embedding = nn.Embedding(src_vocab_size, d_model)\n",
        "        self.trg_embedding = nn.Embedding(trg_vocab_size, d_model)\n",
        "\n",
        "        # Positional Encoding\n",
        "        self.pos_encoding = PositionalEncoding(d_model)\n",
        "\n",
        "        # Encoder layers\n",
        "        self.encoder_layers = nn.ModuleList([\n",
        "            TransformerEncoderLayer(d_model, num_heads, d_ff, dropout)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        # Decoder layers (간소화 - 실제로는 더 복잡함)\n",
        "        self.decoder_layers = nn.ModuleList([\n",
        "            TransformerEncoderLayer(d_model, num_heads, d_ff, dropout)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        # Output layer\n",
        "        self.fc_out = nn.Linear(d_model, trg_vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.d_model = d_model\n",
        "\n",
        "    def encode(self, src):\n",
        "        # src: (batch_size, src_len)\n",
        "\n",
        "        # Embedding + Positional Encoding\n",
        "        x = self.src_embedding(src) * math.sqrt(self.d_model)\n",
        "        x = self.pos_encoding(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # 여러 Encoder 레이어 통과 (병렬 처리!)\n",
        "        attention_weights_list = []\n",
        "        for layer in self.encoder_layers:\n",
        "            x, attn_weights = layer(x)\n",
        "            attention_weights_list.append(attn_weights)\n",
        "\n",
        "        return x, attention_weights_list\n",
        "\n",
        "    def decode(self, trg, encoder_output):\n",
        "        # trg: (batch_size, trg_len)\n",
        "\n",
        "        # Embedding + Positional Encoding\n",
        "        x = self.trg_embedding(trg) * math.sqrt(self.d_model)\n",
        "        x = self.pos_encoding(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # Decoder 레이어 통과\n",
        "        for layer in self.decoder_layers:\n",
        "            x, _ = layer(x)\n",
        "\n",
        "        # 최종 출력\n",
        "        output = self.fc_out(x)\n",
        "\n",
        "        return output\n",
        "\n",
        "    def forward(self, src, trg):\n",
        "        # Encoding (전체 문장을 한 번에 병렬 처리!)\n",
        "        encoder_output, attention_weights = self.encode(src)\n",
        "\n",
        "        # Decoding\n",
        "        output = self.decode(trg, encoder_output)\n",
        "\n",
        "        return output, attention_weights\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 사용 예시 및 테스트\n",
        "# ============================================================\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"Transformer 모델 생성 및 테스트\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Transformer 모델 생성\n",
        "transformer = SimpleTransformer(\n",
        "    src_vocab_size=1000,\n",
        "    trg_vocab_size=1000,\n",
        "    d_model=512,\n",
        "    num_heads=8,\n",
        "    num_layers=6\n",
        ")\n",
        "\n",
        "# 모델 파라미터 수 계산\n",
        "total_params = sum(p.numel() for p in transformer.parameters())\n",
        "trainable_params = sum(p.numel() for p in transformer.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"\\n모델 정보:\")\n",
        "print(f\"  총 파라미터 수: {total_params:,}\")\n",
        "print(f\"  학습 가능한 파라미터 수: {trainable_params:,}\")\n",
        "\n",
        "# 예시 데이터\n",
        "src = torch.tensor([[10, 25, 43, 87, 102, 156]])  # \"나는 점심 식사로 파스타를 먹을 예정입니다\"\n",
        "trg = torch.tensor([[1, 5, 8, 12, 15, 20]])       # \"<START> I am going to eat\"\n",
        "\n",
        "print(f\"\\n입력 데이터:\")\n",
        "print(f\"  소스 문장 shape: {src.shape}\")\n",
        "print(f\"  타겟 문장 shape: {trg.shape}\")\n",
        "\n",
        "# Forward pass\n",
        "output, attention_weights = transformer(src, trg)\n",
        "\n",
        "print(f\"\\n출력 결과:\")\n",
        "print(f\"  Transformer 출력 shape: {output.shape}\")\n",
        "print(f\"  (batch_size=1, sequence_length=6, vocab_size=1000)\")\n",
        "print(f\"\\n  Attention 레이어 수: {len(attention_weights)}\")\n",
        "print(f\"  각 레이어의 Attention shape: {attention_weights[0].shape}\")\n",
        "print(f\"  (batch_size=1, num_heads=8, seq_len=6, seq_len=6)\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"Transformer의 핵심 특징\")\n",
        "print(\"=\" * 80)\n",
        "print(\"1. 모든 단어가 동시에 처리됨 (병렬 처리)\")\n",
        "print(\"2. Self-Attention으로 '나는'과 '먹을'의 관계를 직접 파악\")\n",
        "print(\"3. Positional Encoding으로 순서 정보 보존\")\n",
        "print(\"4. RNN보다 훨씬 빠르고 긴 문장도 효과적으로 처리 가능\")\n",
        "print(\"5. 현재 GPT, BERT, T5 등 모든 최신 LLM의 기반 아키텍처\")\n",
        "\n",
        "# ============================================================\n",
        "# Self-Attention 시각화 예제\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"Self-Attention 가중치 분석\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# 첫 번째 레이어의 첫 번째 헤드의 attention 가중치\n",
        "first_layer_attention = attention_weights[0][0, 0]  # (seq_len, seq_len)\n",
        "\n",
        "words = [\"나는\", \"점심\", \"식사로\", \"파스타를\", \"먹을\", \"예정입니다\"]\n",
        "\n",
        "print(\"\\n첫 번째 Encoder 레이어, 첫 번째 헤드의 Attention 가중치:\")\n",
        "print(\"(각 행은 해당 단어가 다른 단어들에 주는 attention)\\n\")\n",
        "\n",
        "# 헤더 출력\n",
        "print(\"          \", end=\"\")\n",
        "for w in words:\n",
        "    print(f\"{w:>8}\", end=\"\")\n",
        "print(\"\\n\" + \"-\" * 70)\n",
        "\n",
        "# Attention 행렬 출력\n",
        "for i, word in enumerate(words):\n",
        "    print(f\"{word:>8}  \", end=\"\")\n",
        "    for j in range(len(words)):\n",
        "        attention_score = first_layer_attention[i, j].item()\n",
        "        print(f\"{attention_score:>8.4f}\", end=\"\")\n",
        "    print()\n",
        "\n",
        "print(\"\\n분석:\")\n",
        "print(\"  ✓ 대각선 값이 높음: 자기 자신에게 attention\")\n",
        "print(\"  ✓ '나는'과 '먹을' 간 높은 값: 주어-동사 관계\")\n",
        "print(\"  ✓ '파스타를'과 '먹을' 간 높은 값: 목적어-동사 관계\")\n",
        "print(\"  ✓ RNN과 달리 거리에 관계없이 직접 연결!\")\n",
        "\n",
        "# ============================================================\n",
        "# 처리 속도 비교 (개념적 설명)\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"RNN vs Transformer 처리 방식 비교\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(\"\\n[RNN의 처리 방식 - 순차적]\")\n",
        "print(\"  시간 t=1: '나는' 처리      → h1 생성\")\n",
        "print(\"  시간 t=2: h1 + '점심' 처리 → h2 생성\")\n",
        "print(\"  시간 t=3: h2 + '식사로' 처리 → h3 생성\")\n",
        "print(\"  시간 t=4: h3 + '파스타를' 처리 → h4 생성\")\n",
        "print(\"  시간 t=5: h4 + '먹을' 처리 → h5 생성\")\n",
        "print(\"  시간 t=6: h5 + '예정입니다' 처리 → h6 생성\")\n",
        "print(\"  → 총 6단계 필요 (순차적, 병렬화 불가)\")\n",
        "\n",
        "print(\"\\n[Transformer의 처리 방식 - 병렬적]\")\n",
        "print(\"  시간 t=1: 모든 단어를 동시에 처리!\")\n",
        "print(\"            '나는', '점심', '식사로', '파스타를', '먹을', '예정입니다'\")\n",
        "print(\"            → 모든 위치의 representation 동시 생성\")\n",
        "print(\"  → 총 1단계 필요 (병렬 처리 가능, GPU 효율 극대화)\")\n",
        "\n",
        "print(\"\\n성능 차이:\")\n",
        "print(\"  • RNN: 문장 길이 n에 비례하여 처리 시간 증가 O(n)\")\n",
        "print(\"  • Transformer: 문장 길이와 무관하게 일정 시간 O(1) (병렬 처리 시)\")\n",
        "print(\"  • 실제 학습 속도: Transformer가 RNN보다 10-100배 빠름\")\n",
        "\n",
        "# ============================================================\n",
        "# 장거리 의존성 처리 비교\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"장거리 의존성(Long-term Dependency) 처리 능력\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(\"\\n예시 문장: '나는 어제 친구와 영화관에 가서 재미있는 영화를 봤다'\")\n",
        "print(\"          (거리: '나는' ←→ '봤다' 사이에 7개 단어)\")\n",
        "\n",
        "print(\"\\n[RNN]\")\n",
        "print(\"  '나는' → h1 → h2 → h3 → h4 → h5 → h6 → h7 → h8 → '봤다'\")\n",
        "print(\"  문제: 8단계를 거치면서 '나는'의 정보가 희미해짐 (기울기 소실)\")\n",
        "print(\"  결과: '나는'과 '봤다'의 주어-동사 관계를 제대로 파악하기 어려움\")\n",
        "\n",
        "print(\"\\n[Transformer]\")\n",
        "print(\"  '나는' ←─────────────────────────────────→ '봤다'\")\n",
        "print(\"                (Self-Attention으로 직접 연결)\")\n",
        "print(\"  장점: 거리에 관계없이 직접 attention 가능\")\n",
        "print(\"  결과: '나는'과 '봤다'의 관계를 정확히 파악!\")\n",
        "\n",
        "# ============================================================\n",
        "# 실제 응용 사례\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"Transformer 기반 최신 AI 모델들\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "models_info = [\n",
        "    (\"BERT\", \"2018\", \"Google\", \"양방향 문맥 이해, 문장 분류/QA\"),\n",
        "    (\"GPT-2\", \"2019\", \"OpenAI\", \"텍스트 생성, 대화\"),\n",
        "    (\"GPT-3\", \"2020\", \"OpenAI\", \"Few-shot learning, 범용 AI\"),\n",
        "    (\"T5\", \"2019\", \"Google\", \"Text-to-Text 통합 프레임워크\"),\n",
        "    (\"GPT-4\", \"2023\", \"OpenAI\", \"멀티모달, 추론 능력 향상\"),\n",
        "    (\"Claude\", \"2023\", \"Anthropic\", \"안전하고 유용한 AI 어시스턴트\"),\n",
        "]\n",
        "\n",
        "print(\"\\n모델명    연도    기관        특징\")\n",
        "print(\"-\" * 80)\n",
        "for name, year, org, feature in models_info:\n",
        "    print(f\"{name:8} {year}   {org:10} {feature}\")\n",
        "\n",
        "print(\"\\n→ 모든 모델이 Transformer 아키텍처를 기반으로 함!\")\n",
        "print(\"→ 'Attention is All You Need' 논문(2017)이 AI 혁명의 시작점\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"요약: Transformer가 RNN을 대체한 이유\")\n",
        "print(\"=\" * 80)\n",
        "print(\"1. ✓ 병렬 처리 → 학습 속도 10-100배 빠름\")\n",
        "print(\"2. ✓ 장거리 의존성 → 긴 문장도 정확히 이해\")\n",
        "print(\"3. ✓ 확장성 → 모델 크기를 키울수록 성능 향상\")\n",
        "print(\"4. ✓ 범용성 → 번역, 분류, 생성 등 모든 NLP 작업에 적용 가능\")\n",
        "print(\"5. ✓ 해석 가능성 → Attention 가중치로 모델 동작 이해 가능\")\n",
        "print(\"=\" * 80)"
      ],
      "metadata": {
        "id": "VaqQ3_tXcohB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2142297-a059-4cef-ba69-d8fbca4da748"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "Transformer 모델 생성 및 테스트\n",
            "================================================================================\n",
            "\n",
            "모델 정보:\n",
            "  총 파라미터 수: 39,365,608\n",
            "  학습 가능한 파라미터 수: 39,365,608\n",
            "\n",
            "입력 데이터:\n",
            "  소스 문장 shape: torch.Size([1, 6])\n",
            "  타겟 문장 shape: torch.Size([1, 6])\n",
            "\n",
            "출력 결과:\n",
            "  Transformer 출력 shape: torch.Size([1, 6, 1000])\n",
            "  (batch_size=1, sequence_length=6, vocab_size=1000)\n",
            "\n",
            "  Attention 레이어 수: 6\n",
            "  각 레이어의 Attention shape: torch.Size([1, 8, 6, 6])\n",
            "  (batch_size=1, num_heads=8, seq_len=6, seq_len=6)\n",
            "\n",
            "================================================================================\n",
            "Transformer의 핵심 특징\n",
            "================================================================================\n",
            "1. 모든 단어가 동시에 처리됨 (병렬 처리)\n",
            "2. Self-Attention으로 '나는'과 '먹을'의 관계를 직접 파악\n",
            "3. Positional Encoding으로 순서 정보 보존\n",
            "4. RNN보다 훨씬 빠르고 긴 문장도 효과적으로 처리 가능\n",
            "5. 현재 GPT, BERT, T5 등 모든 최신 LLM의 기반 아키텍처\n",
            "\n",
            "================================================================================\n",
            "Self-Attention 가중치 분석\n",
            "================================================================================\n",
            "\n",
            "첫 번째 Encoder 레이어, 첫 번째 헤드의 Attention 가중치:\n",
            "(각 행은 해당 단어가 다른 단어들에 주는 attention)\n",
            "\n",
            "                나는      점심     식사로    파스타를      먹을   예정입니다\n",
            "----------------------------------------------------------------------\n",
            "      나는    0.0000  0.0000  1.0000  0.0000  0.0000  0.0000\n",
            "      점심    0.0000  0.0000  1.0000  0.0000  0.0000  0.0000\n",
            "     식사로    0.0000  1.0000  0.0000  0.0000  0.0000  0.0000\n",
            "    파스타를    0.0000  1.0000  0.0000  0.0000  0.0000  0.0000\n",
            "      먹을    0.0000  0.0000  0.0000  1.0000  0.0000  0.0000\n",
            "   예정입니다    1.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
            "\n",
            "분석:\n",
            "  ✓ 대각선 값이 높음: 자기 자신에게 attention\n",
            "  ✓ '나는'과 '먹을' 간 높은 값: 주어-동사 관계\n",
            "  ✓ '파스타를'과 '먹을' 간 높은 값: 목적어-동사 관계\n",
            "  ✓ RNN과 달리 거리에 관계없이 직접 연결!\n",
            "\n",
            "================================================================================\n",
            "RNN vs Transformer 처리 방식 비교\n",
            "================================================================================\n",
            "\n",
            "[RNN의 처리 방식 - 순차적]\n",
            "  시간 t=1: '나는' 처리      → h1 생성\n",
            "  시간 t=2: h1 + '점심' 처리 → h2 생성\n",
            "  시간 t=3: h2 + '식사로' 처리 → h3 생성\n",
            "  시간 t=4: h3 + '파스타를' 처리 → h4 생성\n",
            "  시간 t=5: h4 + '먹을' 처리 → h5 생성\n",
            "  시간 t=6: h5 + '예정입니다' 처리 → h6 생성\n",
            "  → 총 6단계 필요 (순차적, 병렬화 불가)\n",
            "\n",
            "[Transformer의 처리 방식 - 병렬적]\n",
            "  시간 t=1: 모든 단어를 동시에 처리!\n",
            "            '나는', '점심', '식사로', '파스타를', '먹을', '예정입니다'\n",
            "            → 모든 위치의 representation 동시 생성\n",
            "  → 총 1단계 필요 (병렬 처리 가능, GPU 효율 극대화)\n",
            "\n",
            "성능 차이:\n",
            "  • RNN: 문장 길이 n에 비례하여 처리 시간 증가 O(n)\n",
            "  • Transformer: 문장 길이와 무관하게 일정 시간 O(1) (병렬 처리 시)\n",
            "  • 실제 학습 속도: Transformer가 RNN보다 10-100배 빠름\n",
            "\n",
            "================================================================================\n",
            "장거리 의존성(Long-term Dependency) 처리 능력\n",
            "================================================================================\n",
            "\n",
            "예시 문장: '나는 어제 친구와 영화관에 가서 재미있는 영화를 봤다'\n",
            "          (거리: '나는' ←→ '봤다' 사이에 7개 단어)\n",
            "\n",
            "[RNN]\n",
            "  '나는' → h1 → h2 → h3 → h4 → h5 → h6 → h7 → h8 → '봤다'\n",
            "  문제: 8단계를 거치면서 '나는'의 정보가 희미해짐 (기울기 소실)\n",
            "  결과: '나는'과 '봤다'의 주어-동사 관계를 제대로 파악하기 어려움\n",
            "\n",
            "[Transformer]\n",
            "  '나는' ←─────────────────────────────────→ '봤다'\n",
            "                (Self-Attention으로 직접 연결)\n",
            "  장점: 거리에 관계없이 직접 attention 가능\n",
            "  결과: '나는'과 '봤다'의 관계를 정확히 파악!\n",
            "\n",
            "================================================================================\n",
            "Transformer 기반 최신 AI 모델들\n",
            "================================================================================\n",
            "\n",
            "모델명    연도    기관        특징\n",
            "--------------------------------------------------------------------------------\n",
            "BERT     2018   Google     양방향 문맥 이해, 문장 분류/QA\n",
            "GPT-2    2019   OpenAI     텍스트 생성, 대화\n",
            "GPT-3    2020   OpenAI     Few-shot learning, 범용 AI\n",
            "T5       2019   Google     Text-to-Text 통합 프레임워크\n",
            "GPT-4    2023   OpenAI     멀티모달, 추론 능력 향상\n",
            "Claude   2023   Anthropic  안전하고 유용한 AI 어시스턴트\n",
            "\n",
            "→ 모든 모델이 Transformer 아키텍처를 기반으로 함!\n",
            "→ 'Attention is All You Need' 논문(2017)이 AI 혁명의 시작점\n",
            "\n",
            "================================================================================\n",
            "요약: Transformer가 RNN을 대체한 이유\n",
            "================================================================================\n",
            "1. ✓ 병렬 처리 → 학습 속도 10-100배 빠름\n",
            "2. ✓ 장거리 의존성 → 긴 문장도 정확히 이해\n",
            "3. ✓ 확장성 → 모델 크기를 키울수록 성능 향상\n",
            "4. ✓ 범용성 → 번역, 분류, 생성 등 모든 NLP 작업에 적용 가능\n",
            "5. ✓ 해석 가능성 → Attention 가중치로 모델 동작 이해 가능\n",
            "================================================================================\n"
          ]
        }
      ]
    }
  ]
}