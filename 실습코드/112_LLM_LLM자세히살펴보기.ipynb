{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanghyun-ai/ktcloud_genai/blob/main/%EC%8B%A4%EC%8A%B5%EC%BD%94%EB%93%9C/112_LLM_LLM%EC%9E%90%EC%84%B8%ED%9E%88%EC%82%B4%ED%8E%B4%EB%B3%B4%EA%B8%B0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **LLM ìì„¸íˆ ì‚´í´ë³´ê¸°**"
      ],
      "metadata": {
        "id": "_intVJo1FXQF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "- ğŸ’¡ **NOTE**\n",
        "    - ì´ ë…¸íŠ¸ë¶ì˜ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ë ¤ë©´ GPUë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤. êµ¬ê¸€ ì½”ë©ì—ì„œëŠ” **ëŸ°íƒ€ì„ > ëŸ°íƒ€ì„ ìœ í˜• ë³€ê²½ > í•˜ë“œì›¨ì–´ ê°€ì†ê¸° > T4 GPU**ë¥¼ ì„ íƒí•˜ì„¸ìš”.\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "81_Ybs4LI7IX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "78YkTOy0LSXb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "158efec4-06d0-4977-d44b-0372813e0b38"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers==4.48.3\n",
            "  Downloading transformers-4.48.3-py3-none-any.whl.metadata (44 kB)\n",
            "\u001b[?25l     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/44.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers==4.48.3) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.48.3) (0.35.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.48.3) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.48.3) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.48.3) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.48.3) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers==4.48.3) (2.32.4)\n",
            "Collecting tokenizers<0.22,>=0.21 (from transformers==4.48.3)\n",
            "  Downloading tokenizers-0.21.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.48.3) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers==4.48.3) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers==4.48.3) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers==4.48.3) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers==4.48.3) (1.1.10)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.48.3) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.48.3) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.48.3) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.48.3) (2025.10.5)\n",
            "Downloading transformers-4.48.3-py3-none-any.whl (9.7 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m65.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.21.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m68.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tokenizers, transformers\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.22.1\n",
            "    Uninstalling tokenizers-0.22.1:\n",
            "      Successfully uninstalled tokenizers-0.22.1\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.57.0\n",
            "    Uninstalling transformers-4.57.0:\n",
            "      Successfully uninstalled transformers-4.57.0\n",
            "Successfully installed tokenizers-0.21.4 transformers-4.48.3\n"
          ]
        }
      ],
      "source": [
        "# Phi-3 ëª¨ë¸ê³¼ í˜¸í™˜ì„± ë•Œë¬¸ì— transformers 4.48.3 ë²„ì „ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
        "!pip install transformers==4.48.3"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ê¹ƒí—ˆë¸Œì—ì„œ ìœ„ì ¯ ìƒíƒœ ì˜¤ë¥˜ë¥¼ í”¼í•˜ê¸° ìœ„í•´ ì§„í–‰ í‘œì‹œì¤„ì„ ë‚˜íƒ€ë‚´ì§€ ì•Šë„ë¡ ì„¤ì •í•©ë‹ˆë‹¤.\n",
        "from transformers.utils import logging\n",
        "\n",
        "logging.disable_progress_bar()"
      ],
      "metadata": {
        "id": "8lVE_3idzVyF"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_23Z_do-faF"
      },
      "source": [
        "## **LLM ë¡œë“œí•˜ê¸°**\n",
        "\n",
        "- **pipeline** : ë³µì¡í•œ ì „ì²˜ë¦¬/í›„ì²˜ë¦¬ ê³¼ì •ì„ ìë™í™”í•´ì£¼ëŠ” ê³ ìˆ˜ì¤€ API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "-5RLd6dI-Ytm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f0c735e-b248-44f8-8c66-5108104120ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-3-mini-4k-instruct:\n",
            "- configuration_phi3.py\n",
            ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
            "A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-3-mini-4k-instruct:\n",
            "- modeling_phi3.py\n",
            ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
            "WARNING:transformers_modules.microsoft.Phi-3-mini-4k-instruct.0a67737cc96d2554230f90338b163bc6380a2a85.modeling_phi3:`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
            "WARNING:transformers_modules.microsoft.Phi-3-mini-4k-instruct.0a67737cc96d2554230f90338b163bc6380a2a85.modeling_phi3:Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n",
            "Device set to use cuda\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "\n",
        "# ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"microsoft/Phi-3-mini-4k-instruct\",\n",
        "    device_map=\"cuda\",\n",
        "    torch_dtype=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "# Create a pipeline\n",
        "generator = pipeline(\n",
        "    \"text-generation\",      # ìˆ˜í–‰í•  ì‘ì—…(task) ìœ í˜•\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    return_full_text=False, # ìƒì„±ëœ í…ìŠ¤íŠ¸ë§Œ ë°˜í™˜í• ì§€, ì…ë ¥ í”„ë¡¬í”„íŠ¸ë¥¼ í¬í•¨í• ì§€ ê²°ì •(í”„ë¡¬í”„íŠ¸ ì œì™¸)\n",
        "    max_new_tokens=50,      # ìƒì„±í•  ìƒˆë¡œìš´ í† í°ì˜ ìµœëŒ€ ê°œìˆ˜\n",
        "    do_sample=False,        # ìƒ˜í”Œë§(ë¬´ì‘ìœ„ì„±) ì‚¬ìš© ì—¬ë¶€\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "rMZqvzy97sRS"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "REqcz-ID_XgV"
      },
      "source": [
        "## **í›ˆë ¨ëœ íŠ¸ëœìŠ¤í¬ë¨¸ LLMì˜ ì…ë ¥ê³¼ ì¶œë ¥**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "V_hAhjmEOiUJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "17h6TPHluJ-i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fab3c081-f963-45ed-9040-e3e62f0dc9cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n",
            "`get_max_cache()` is deprecated for all Cache classes. Use `get_max_cache_shape()` instead. Calling `get_max_cache()` will raise error from v4.48\n",
            "WARNING:transformers_modules.microsoft.Phi-3-mini-4k-instruct.0a67737cc96d2554230f90338b163bc6380a2a85.modeling_phi3:You are not running the flash-attention implementation, expect numerical differences.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Mention the steps you're taking to prevent it in the future.\n",
            "\n",
            "Dear Sarah,\n",
            "\n",
            "I hope this message finds you well. I am writing to express my deepest apologies for the unfortunate incident that occurred in\n"
          ]
        }
      ],
      "source": [
        "prompt = \"Write an email apologizing to Sarah for the tragic gardening mishap. Explain how it happened.\"\n",
        "\n",
        "output = generator(prompt)\n",
        "\n",
        "print(output[0]['generated_text'])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "id": "ZTAaaGKl_ITP",
        "outputId": "0584d40f-4410-459e-e8aa-26f7fe5b4d75",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Phi3ForCausalLM(\n",
            "  (model): Phi3Model(\n",
            "    (embed_tokens): Embedding(32064, 3072, padding_idx=32000)\n",
            "    (embed_dropout): Dropout(p=0.0, inplace=False)\n",
            "    (layers): ModuleList(\n",
            "      (0-31): 32 x Phi3DecoderLayer(\n",
            "        (self_attn): Phi3Attention(\n",
            "          (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
            "          (qkv_proj): Linear(in_features=3072, out_features=9216, bias=False)\n",
            "          (rotary_emb): Phi3RotaryEmbedding()\n",
            "        )\n",
            "        (mlp): Phi3MLP(\n",
            "          (gate_up_proj): Linear(in_features=3072, out_features=16384, bias=False)\n",
            "          (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
            "          (activation_fn): SiLU()\n",
            "        )\n",
            "        (input_layernorm): Phi3RMSNorm()\n",
            "        (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
            "        (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
            "        (post_attention_layernorm): Phi3RMSNorm()\n",
            "      )\n",
            "    )\n",
            "    (norm): Phi3RMSNorm()\n",
            "  )\n",
            "  (lm_head): Linear(in_features=3072, out_features=32064, bias=False)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **[ì‹¤ìŠµ] max_new_tokens ê°’ ì¡°ì •í•˜ê¸°**\n",
        "max_new_tokens ê°’ì„ í¬ê²Œ ëŠ˜ë ¤ì„œ ê²°ê³¼ í™•ì¸í•˜ê¸°\n",
        "- max_new_tokens = 200\n",
        "- max_new_tokens = 1000 (--> OutOfMemoryError)"
      ],
      "metadata": {
        "id": "YHbSvXnvxprg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generator = pipeline(\n",
        "    \"text-generation\",      # ìˆ˜í–‰í•  ì‘ì—…(task) ìœ í˜•\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    return_full_text=False, # ìƒì„±ëœ í…ìŠ¤íŠ¸ë§Œ ë°˜í™˜í• ì§€, ì…ë ¥ í”„ë¡¬í”„íŠ¸ë¥¼ í¬í•¨í• ì§€ ê²°ì •(í”„ë¡¬í”„íŠ¸ ì œì™¸)\n",
        "    max_new_tokens=200,      # ìƒì„±í•  ìƒˆë¡œìš´ í† í°ì˜ ìµœëŒ€ ê°œìˆ˜\n",
        "    do_sample=False,        # ìƒ˜í”Œë§(ë¬´ì‘ìœ„ì„±) ì‚¬ìš© ì—¬ë¶€\n",
        ")\n",
        "\n",
        "prompt = \"Write an email apologizing to Sarah for the tragic gardening mishap. Explain how it happened.\"\n",
        "\n",
        "output = generator(prompt)\n",
        "\n",
        "print(output[0]['generated_text'])"
      ],
      "metadata": {
        "id": "J_J-EuoS7nG1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01e5dbd3-1235-4461-a936-31cac7c71558"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Mention the steps you're taking to prevent it in the future.\n",
            "\n",
            "Dear Sarah,\n",
            "\n",
            "I hope this message finds you well. I am writing to express my deepest apologies for the unfortunate incident that occurred in your garden. It was a tragic mishap that I never intended, and I am truly sorry for any distress it may have caused you.\n",
            "\n",
            "The incident happened when I was attempting to help you with your gardening project. I had been researching various techniques to improve the health and growth of your plants, and I thought I had found the perfect solution. However, in my eagerness to implement this new method, I accidentally damaged some of your delicate flowers and disrupted the balance of your carefully cultivated ecosystem.\n",
            "\n",
            "I understand that my actions have caused significant harm to your garden, and I am sincerely sorry for the consequences. Please know that I take full responsibility for my\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "eoFkdTd6_g5o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a20ae23a-001c-498c-bef3-a9c8a3b44aa4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Phi3ForCausalLM(\n",
            "  (model): Phi3Model(\n",
            "    (embed_tokens): Embedding(32064, 3072, padding_idx=32000)\n",
            "    (embed_dropout): Dropout(p=0.0, inplace=False)\n",
            "    (layers): ModuleList(\n",
            "      (0-31): 32 x Phi3DecoderLayer(\n",
            "        (self_attn): Phi3Attention(\n",
            "          (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
            "          (qkv_proj): Linear(in_features=3072, out_features=9216, bias=False)\n",
            "          (rotary_emb): Phi3RotaryEmbedding()\n",
            "        )\n",
            "        (mlp): Phi3MLP(\n",
            "          (gate_up_proj): Linear(in_features=3072, out_features=16384, bias=False)\n",
            "          (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
            "          (activation_fn): SiLU()\n",
            "        )\n",
            "        (input_layernorm): Phi3RMSNorm()\n",
            "        (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
            "        (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
            "        (post_attention_layernorm): Phi3RMSNorm()\n",
            "      )\n",
            "    )\n",
            "    (norm): Phi3RMSNorm()\n",
            "  )\n",
            "  (lm_head): Linear(in_features=3072, out_features=32064, bias=False)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "MJzSPgvI7q6V"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTrwzB67BYVY"
      },
      "source": [
        "## **í™•ë¥  ë¶„í¬ë¡œë¶€í„° í•˜ë‚˜ì˜ í† í° ì„ íƒí•˜ê¸°(ìƒ˜í”Œë§/ë””ì½”ë”©)**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- í™•ë¥  ë¶„í¬ì—ì„œ í•˜ë‚˜ì˜ í† í°ì„ ì„ íƒí•˜ëŠ” ë°©ë²• --> **ë””ì½”ë”© ì „ëµ**\n",
        "- ë¥  ì ìˆ˜ê°€ ê°€ì¥ ë†’ì€ í† í°ì„ ê³ ë¥´ëŠ” ê²½ìš° --> **Greedy Decoding(íƒìš•ì  ë””ì½”ë”©)**\n",
        "    - **LLMì˜ ì˜¨ë„(temperature) ë§¤ê°œë³€ìˆ˜ë¥¼ 0ìœ¼ë¡œ ì„¤ì •í•˜ëŠ” ë°©ì‹**"
      ],
      "metadata": {
        "id": "qpAld57j6qll"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "sEcxYgJxBYbJ"
      },
      "outputs": [],
      "source": [
        "prompt = \"The capital of France is\"\n",
        "\n",
        "# ì…ë ¥ í”„ë¡¬í”„íŠ¸ë¥¼ í† í°í™”í•©ë‹ˆë‹¤.\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
        "\n",
        "# ì…ë ¥ í† í°ì„ GPUì— ë°°ì¹˜í•©ë‹ˆë‹¤.\n",
        "input_ids = input_ids.to(\"cuda\")\n",
        "\n",
        "# lm_head ì•ì— ìˆëŠ” modelì˜ ì¶œë ¥ì„ ì–»ìŠµë‹ˆë‹¤.\n",
        "model_output = model.model(input_ids)\n",
        "\n",
        "# lm_headì˜ ì¶œë ¥ì„ ì–»ìŠµë‹ˆë‹¤.\n",
        "lm_head_output = model.lm_head(model_output[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "68YUSS4GBf9Q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "3bb73c8f-9004-47f3-e815-34bd6b32af3a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Paris'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "token_id = lm_head_output[0,-1].argmax(-1) # ë§ˆì§€ë§‰ì— ìƒì„±ëœ í† í°ID / ì²«ë²ˆì§¸ í–‰ì—ì„œ ê°€ì¥ ë§ˆì§€ë§‰ í† í°\n",
        "tokenizer.decode(token_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "cWWrfC5oBjwp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2fe4628c-8839-4d35-a4de-4f3dea8e7bdc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 5, 3072])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "model_output[0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "nC1PdOnTBnxZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8889c66-430a-4a64-8b2f-9c60b317cdae"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 5, 32064])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "lm_head_output.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Of2_rP4QBqrZ"
      },
      "source": [
        "## **í‚¤ì™€ ê°’ì„ ìºì‹±(kv cache)í•˜ì—¬ ìƒì„± ì†ë„ ë†’ì´ê¸°**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "B0n6JhNHBrin"
      },
      "outputs": [],
      "source": [
        "prompt = \"Write a very long email apologizing to Sarah for the tragic gardening mishap. Explain how it happened.\"\n",
        "\n",
        "# ì…ë ¥ í”„ë¡¬í”„íŠ¸ë¥¼ í† í°í™”í•©ë‹ˆë‹¤.\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
        "input_ids = input_ids.to(\"cuda\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ë¬¸ì¥ ìƒì„± ì†ë„ ì¸¡ì •(ìºì‹± ì‚¬ìš©)"
      ],
      "metadata": {
        "id": "l8mS6oB0C0Dl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "BwIvt6jSByAF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7533fa29-3be0-48f7-e34f-d6b8270df841"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.61 s Â± 243 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)\n"
          ]
        }
      ],
      "source": [
        "%%timeit -n 1\n",
        "# í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n",
        "generation_output = model.generate(\n",
        "  input_ids=input_ids,\n",
        "  max_new_tokens=100,\n",
        "  use_cache=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ë¬¸ì¥ ìƒì„± ì†ë„ ì¸¡ì •(ìºì‹± ì‚¬ìš© ì•ˆí•¨)"
      ],
      "metadata": {
        "id": "wGdM6V-dC4ZW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "dFb1dcvJByCW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d9e532c-5ad2-4e08-f589-e14e6c3044ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "33 s Â± 253 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)\n"
          ]
        }
      ],
      "source": [
        "%%timeit -n 1\n",
        "# í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n",
        "generation_output = model.generate(\n",
        "  input_ids=input_ids,\n",
        "  max_new_tokens=100,\n",
        "  use_cache=False\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}