{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanghyun-ai/ktcloud_genai/blob/main/%EC%8B%A4%EC%8A%B5%EC%BD%94%EB%93%9C/401_LLM_%EA%B3%A0%EA%B8%89_%ED%85%8D%EC%8A%A4%ED%8A%B8_%EC%83%9D%EC%84%B1_%EA%B8%B0%EC%88%A0%EA%B3%BC_%EB%8F%84%EA%B5%AC_langchain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **고급 텍스트 생성 기술과 도구**\n"
      ],
      "metadata": {
        "id": "3g3yQqxBnXOp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "\n",
        "- 💡 **NOTE**\n",
        "    - 이 노트북의 코드를 실행하려면 GPU를 사용하는 것이 좋습니다. 구글 코랩에서는 **런타임 > 런타임 유형 변경 > 하드웨어 가속기 > T4 GPU**를 선택하세요.\n",
        "\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "DLl8lovjBLgP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **모델을 미세튜닝하지 않고 LLM에서 얻은 출력과 경험을 더 향상시키려면 어떻게 해야 할까?**\n",
        "\n",
        "- **생성 텍스트의 품질을 향상시키기 위한 몇 가지 기법과 개념**\n",
        "    - **모델 I/O** : LLM을 로드하고 실행하기\n",
        "    - **체인**(Chain) : 방법과 도구를 연결하기\n",
        "    - **메모리** : LLM이 기억하도록 돕기\n",
        "    - **에이전트**(Agent) : 복잡한 동작을 외부 도구와 연결하기\n",
        "\n"
      ],
      "metadata": {
        "id": "rg0qLUE-jjoi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://drive.google.com/uc?export=view&id=1LEVm8upycOb0PS454PVTH8tZtg7n0KqD\" width=\"80%\">\n"
      ],
      "metadata": {
        "id": "u8QMoLhUsXSO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "bUWBl_VubQNJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **LangChain 소개**"
      ],
      "metadata": {
        "id": "eNx4LxbLuvex"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **LangChain이란?** :\n",
        "- LangChain은 **LLM을 활용한 애플리케이션(에이전트, RAG 등)을 쉽게 만들 수 있도록 도와주는 <mark>오픈소스 프레임워크</mark>**\n",
        "    - “**AI 파이프라인**”을 구축하도록 돕는 프레임워크\n",
        "    - 추상화를 통해 LLM 작업을 단순화해 주는 프레임워크 (--> 최근 모듈화 진행)\n",
        "    - LLM을 단순한 챗봇이 아니라, 데이터베이스·API·문서·도구들과 연동해 복잡한 작업을 수행할 수 있게 해주는 중간 플랫폼\n",
        "- **개발자**: Harrison Chase (2022년 발표)\n",
        "- **주요 목적** : LLM을 이용해 실제 “엔터프라이즈급” 애플리케이션을 쉽게 구축\n",
        "- **주요 언어** : \tPython, JavaScript/TypeScript\n",
        "- **공식 문서** : https://www.langchain.com\n",
        "    - https://github.com/langchain-ai/langchain\n",
        "    - https://docs.langchain.com/oss/python/langchain/overview\n",
        "- **한국어 문서** : https://github.com/teddylee777/langchain-kr\n"
      ],
      "metadata": {
        "id": "oz9w1Co6o82Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **LangChain 핵심 전략**\n",
        "- 이러한 기술은 그 자체로 강력하지만 독립적으로 사용될 때는 진정한 가치를 발휘하지 못함\n",
        "- 이런 기법을 **모두 연결해야 놀라운 성능의 LLM 기반 시스템을 얻을 수 있다**.\n",
        "- 이런 기술이 최고조에 도달할 때 LLM이 진정한 빛을 발한다."
      ],
      "metadata": {
        "id": "8Kr7ojGGbTBQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **LangChain 핵심 구성 요소**\n",
        "\n",
        "| 구성 요소 | 설명 | 예시 |\n",
        "| ---  | --- | --- |\n",
        "| **LLMs** | GPT, Claude, Gemini 등과 같은 대규모 언어 모델  | `OpenAI`, `HuggingFace`, `Ollama`, `VertexAI` |\n",
        "| **Prompt Templates** | 일관성 있는 프롬프트 구조를 관리 | “{question}에 대해 전문가처럼 답변해줘” |\n",
        "| **Chains** | 여러 LLM 호출 및 연산을 단계적으로 연결 | 질문 → 요약 →  데이터베이스 검색 → 결과 생성 |\n",
        "| **Memory** | 대화 기록을 저장하여 문맥을 유지 | 챗봇이 이전 대화  기억하기 |\n",
        "| **Retrievers / VectorStores** | 문서 검색 및 벡터 기반 임베딩 검색 | `FAISS`, `Chroma`, `Pinecone` |\n",
        "| **Agents & Tools** | LLM이 “도구(계산기, API, 검색엔진 등)”를 스스로 호출하여 문제 해결 | “날씨 알려줘” → LLM이 OpenWeather API 호출 |\n",
        "| **Callbacks** | 실행 과정을 추적하거나 시각화 | Streamlit, LangSmith 연동 |\n"
      ],
      "metadata": {
        "id": "yWLeVsOWyJPg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **LangChain의 동작 구조**\n",
        "**Prompt → Model → Output**의 단순한 흐름을 확장하여 **다단계 파이프라인을 구성**\n",
        "\n",
        "`User → PromptTemplate → LLMChain → (Memory + Retriever + Tools) → Output`\n",
        "\n",
        "- **동작 예시**:\n",
        "1. 사용자의 질문 수집\n",
        "2. 프롬프트 템플릿에 질문 삽입\n",
        "3. 벡터스토어에서 관련 문서 검색\n",
        "4. 결과를 정리하여 LLM에게 전달\n",
        "5. LLM이 최종 답변 생성"
      ],
      "metadata": {
        "id": "yEVILOgKzgSM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **LangChain의 주요 응용 분야**\n",
        "\n",
        "|분야| 설명| 예시|\n",
        "| ---| ---| ---|\n",
        "| **RAG (Retrieval-Augmented Generation)** | 외부 문서를 검색해 답변 정확도 향상   | 논문 기반 Q&A, 내부 문서 요약  |\n",
        "| **LLM Agents**  | LLM이 스스로 툴을 선택하여 실행    | ChatGPT Plugins, AutoGPT   |\n",
        "| **Workflow Automation** | 여러 LLM 호출을 파이프라인으로 자동화 | 이메일 요약 → 일정 등록 → 보고서 생성    |\n",
        "| **Conversational Chatbot** | 기억 기반 대화형 시스템 구축 | 고객 상담 챗봇, 교육용 튜터 |\n",
        "| **데이터 분석 자동화** | 자연어로 데이터 탐색  | “CSV 데이터 요약해줘” → Pandas 실행 |\n"
      ],
      "metadata": {
        "id": "q4AW-HjP0lYq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **LangChain의 발전 이력**\n",
        "\n",
        "| 시기 | 주요 내용 |\n",
        "| --- | --- |\n",
        "| **2022년 초**  | Harrison Chase가 LangChain 공개 (LLM 연결을 위한 체인 기반 프레임워크) |\n",
        "| **2023년 중반** | VectorStore 통합 및 Memory 기능 강화 → RAG 구조 대중화 |\n",
        "| **2023년 말**  | LangSmith/LangServe 출시로 추적·디버깅·API 배포 가능 |\n",
        "| **2024년 이후** | LangGraph(LLM 기반 상태 머신) 등장 — 복잡한 워크플로우 구현 강화 |\n"
      ],
      "metadata": {
        "id": "elBKA1AU1WRt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### **LangChain vs 다른 프레임워크**:\n",
        "- **LlamaIndex** : 문서 인덱싱 및 검색 강화\n",
        "    - LlamaIndex : https://www.llamaindex.ai/\n",
        "- **Haystack**: RAG 중심의 검색엔진\n",
        "    - https://github.com/deepset-ai/haystack\n",
        "- **DSPy**: 프롬프트·파이프라인을 데이터로 최적화(“컴파일”)\n",
        "    - https://github.com/stanfordnlp/dspy\n",
        "    - “프롬프트 엔지니어링을 코드가 아니라 데이터와 지표로 최적화”하는 프레임워크.\n",
        "    - 입력·출력 사양(Signature)과 작은 검증 데이터셋, 성능 지표를 주면, 내부 teleprompter/컴파일러가 적절한 프롬프트·설정(예: few-shot 예시, 도구 호출 방식 등)을 자동 탐색해 일관되게 재현 가능한 최적 파이프라인을 만들어\n",
        "\n",
        "\n",
        "| 비교 항목  | **LangChain**  |**LlamaIndex** | **Haystack** |\n",
        "| --- | --- | --- | --- |\n",
        "| 주 목적   | LLM 파이프라인 구성   | 문서 인덱싱 및 검색 강화 | RAG 중심의 검색엔진 |\n",
        "| 구조 | 모듈식 체인/에이전트 기반 | Graph 구조 기반 | Retriever + Generator 구조 |\n",
        "| 학습 난이도 | 중간 | 쉬움 | 중간 |\n",
        "| 강점 | 도구/에이전트 연동 | 문서 관계 모델링  | 대규모 데이터 처리 |\n",
        "| --- | --- | https://www.llamaindex.ai/ | https://github.com/deepset-ai/haystack |\n"
      ],
      "metadata": {
        "id": "FmQVSnwiiuVs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "YsuY48iBx1sb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **모델 I/O**"
      ],
      "metadata": {
        "id": "lwkkxTQq5nw4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **라이브러리 설치**\n"
      ],
      "metadata": {
        "id": "vJLRwHnMlJF0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "import logging\n",
        "\n",
        "# 1. 일반적인 Python 경고(DeprecationWarning 등) 숨기기\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "# 2. Transformers 라이브러리 로그 수준 조절 (Warning 이하는 숨기기)\n",
        "logging.getLogger(\"transformers\").setLevel(logging.ERROR)"
      ],
      "metadata": {
        "id": "i28AXkBQfyp0"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 깃허브에서 위젯 상태 오류를 피하기 위해 진행 표시줄을 나타내지 않도록 설정합니다.\n",
        "import os\n",
        "import tqdm\n",
        "from transformers.utils import logging\n",
        "\n",
        "# tqdm 비활성화\n",
        "tqdm.tqdm = lambda *args, **kwargs: iter([])\n",
        "tqdm.auto.tqdm = lambda *args, **kwargs: iter([])\n",
        "tqdm.notebook.tqdm = lambda *args, **kwargs: iter([])\n",
        "os.environ[\"DISABLE_TQDM\"] = \"1\"\n",
        "\n",
        "logging.disable_progress_bar()"
      ],
      "metadata": {
        "id": "ILBN7AMQ0SfM"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Phi-3 모델과 호환성 때문에 transformers 4.48.3 버전을 사용합니다.\n",
        "!pip install transformers==4.48.3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uzQRZTt_c4Ug",
        "outputId": "28b4904d-4b78-4cfa-e8e2-d74ce4165a48"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers==4.48.3 in /usr/local/lib/python3.12/dist-packages (4.48.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers==4.48.3) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.48.3) (0.35.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.48.3) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.48.3) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.48.3) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.48.3) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers==4.48.3) (2.32.5)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.12/dist-packages (from transformers==4.48.3) (0.21.4)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.48.3) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers==4.48.3) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers==4.48.3) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers==4.48.3) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers==4.48.3) (1.1.10)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.48.3) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.48.3) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.48.3) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.48.3) (2025.10.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Txh47zAxCAYd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "8d55cada-63f1-48fc-b5d8-03a5e5c814e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (1.0.2)\n",
            "Requirement already satisfied: langchain-openai in /usr/local/lib/python3.12/dist-packages (1.0.1)\n",
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.12/dist-packages (0.4)\n",
            "Requirement already satisfied: duckduckgo-search in /usr/local/lib/python3.12/dist-packages (8.1.1)\n",
            "Requirement already satisfied: ddgs in /usr/local/lib/python3.12/dist-packages (9.6.1)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langchain) (1.0.0)\n",
            "Requirement already satisfied: langgraph<1.1.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langchain) (1.0.1)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.11.10)\n",
            "Requirement already satisfied: openai<3.0.0,>=1.109.1 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (1.109.1)\n",
            "Requirement already satisfied: tiktoken<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (0.12.0)\n",
            "Requirement already satisfied: langchain-classic<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (1.0.0)\n",
            "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.44)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.32.5 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.32.5)\n",
            "Requirement already satisfied: PyYAML<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (6.0.3)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (3.13.0)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (8.5.0)\n",
            "Requirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.11.0)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.1.125 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.37)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.3)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: click>=8.1.8 in /usr/local/lib/python3.12/dist-packages (from duckduckgo-search) (8.3.0)\n",
            "Requirement already satisfied: primp>=0.15.0 in /usr/local/lib/python3.12/dist-packages (from duckduckgo-search) (0.15.0)\n",
            "Requirement already satisfied: lxml>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from duckduckgo-search) (6.0.2)\n",
            "Requirement already satisfied: httpx>=0.28.1 in /usr/local/lib/python3.12/dist-packages (from httpx[brotli,http2,socks]>=0.28.1->ddgs) (0.28.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.22.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.28.1->httpx[brotli,http2,socks]>=0.28.1->ddgs) (4.11.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.28.1->httpx[brotli,http2,socks]>=0.28.1->ddgs) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.28.1->httpx[brotli,http2,socks]>=0.28.1->ddgs) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.28.1->httpx[brotli,http2,socks]>=0.28.1->ddgs) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.28.1->httpx[brotli,http2,socks]>=0.28.1->ddgs) (0.16.0)\n",
            "Requirement already satisfied: brotli in /usr/local/lib/python3.12/dist-packages (from httpx[brotli,http2,socks]>=0.28.1->ddgs) (1.1.0)\n",
            "Requirement already satisfied: h2<5,>=3 in /usr/local/lib/python3.12/dist-packages (from httpx[brotli,http2,socks]>=0.28.1->ddgs) (4.3.0)\n",
            "Requirement already satisfied: socksio==1.* in /usr/local/lib/python3.12/dist-packages (from httpx[brotli,http2,socks]>=0.28.1->ddgs) (1.0.0)\n",
            "Requirement already satisfied: langchain-text-splitters<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langchain-classic<2.0.0,>=1.0.0->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain) (25.0)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain) (4.15.0)\n",
            "Requirement already satisfied: langgraph-checkpoint<4.0.0,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.0->langchain) (3.0.0)\n",
            "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.0->langchain) (1.0.1)\n",
            "Requirement already satisfied: langgraph-sdk<0.3.0,>=0.2.2 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.0->langchain) (0.2.9)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.0->langchain) (3.6.0)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (3.11.3)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (0.25.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (0.11.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (4.67.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.2)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (1.1.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2.5.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain-community) (3.2.4)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1.0.0,>=0.7.0->langchain-openai) (2024.11.6)\n",
            "Requirement already satisfied: hyperframe<7,>=6.1 in /usr/local/lib/python3.12/dist-packages (from h2<5,>=3->httpx[brotli,http2,socks]>=0.28.1->ddgs) (6.1.0)\n",
            "Requirement already satisfied: hpack<5,>=4.1 in /usr/local/lib/python3.12/dist-packages (from h2<5,>=3->httpx[brotli,http2,socks]>=0.28.1->ddgs) (4.1.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.0->langchain) (3.0.0)\n",
            "Requirement already satisfied: ormsgpack>=1.10.0 in /usr/local/lib/python3.12/dist-packages (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.0->langchain) (1.11.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community) (1.1.0)\n"
          ]
        }
      ],
      "source": [
        "# 라이브러리 설치\n",
        "# 키워드를 넣으면 알아서 검색해주는 라이브러리\n",
        "!pip install -U langchain langchain-openai langchain-community duckduckgo-search ddgs\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 라이브러리 버전 확인\n",
        "%pip list | grep -E 'langchain|duckduckgo|ddgs'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Igi53cwrcvPW",
        "outputId": "b7785cbc-6e32-4f0a-bc65-148a64071ce2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ddgs                                     9.6.1\n",
            "duckduckgo_search                        8.1.1\n",
            "langchain                                1.0.2\n",
            "langchain-classic                        1.0.0\n",
            "langchain-community                      0.4\n",
            "langchain-core                           1.0.0\n",
            "langchain-openai                         1.0.1\n",
            "langchain-text-splitters                 1.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **langchain_community** :\n",
        "    - LangChain의 핵심 파트너사(OpenAI, Google 등) 외의 다양한 커뮤니티 기반 LLM, 벡터DB, 도구 연동 기능들을 모아놓은 라이브러리\n",
        "- **langchain_openai** :\n",
        "    - LangChain에서 ChatOpenAI (GPT-4o 등)나 OpenAIEmbeddings처럼 OpenAI 및 Azure OpenAI의 모델을 사용하기 위한 공식 통합 라이브러리\n",
        "- **duckduckgo-search** :\n",
        "    - API 키 없이도 파이썬 코드에서 DuckDuckGo 웹 검색을 실행하고 그 결과를 받아올 수 있게 해주는 간단한 독립 라이브러리\n",
        "    - **duckduckgo** : 사용자의 개인정보 보호(프라이버시)를 최우선으로 하는 **검색 엔진**\n",
        "    - https://duckduckgo.com/\n",
        "    - 사용자의 검색 기록, IP 주소, 클릭한 링크 등 어떤 개인정보도 저장하거나 추적하지 X\n",
        "    - DuckDuckGo는 추적을 안 하므로 모든 사용자에게 동일하고 객관적인 검색 결과를 보여짐\n",
        "    - DuckDuckGo는 이 과정을 매우 단순화(사실상 무료 개방)해 주었기 때문에, LangChain 에이전트(Agent)에게 '실시간 정보 검색 능력'을 부여할 때 가장 기본적이고 인기 있는 도구(Tool)로 사용됨"
      ],
      "metadata": {
        "id": "hdHPA98IjU2P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 파이썬 버전 확인(3.12)\n",
        "!python --version"
      ],
      "metadata": {
        "id": "6jBSq5FKdprY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33cdc4e8-3b1c-46e1-a51f-8702f4b5ee36"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.12.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CUDA 버전 확인 (12.5)\n",
        "!nvcc --version | grep cuda_"
      ],
      "metadata": {
        "id": "5ZcvURivdpyp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7548cd7-3cb8-4732-f42e-4a6fd76900c0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Build cuda_12.5.r12.5/compiler.34385749_0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- llama-cpp-python\n",
        "    - C++로 작성된 고성능 대규모 언어 모델(LLM) 추론 엔진인 llama.cpp의 Python 바인딩(wrapper) 라이브러리\n",
        "    - https://github.com/abetlen/llama-cpp-python/releases\n",
        "    - llama_cpp_python-0.3.16-cp312-cp312-linux_x86_64.whl"
      ],
      "metadata": {
        "id": "ggfr7XV9mwGP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "# 사용하는 파이썬과 CUDA 버전에 맞는 llama-cpp-python 패키지를 설치하세요.\n",
        "!pip install https://github.com/abetlen/llama-cpp-python/releases/download/v0.3.16-cu124/llama_cpp_python-0.3.16-cp312-cp312-linux_x86_64.whl"
      ],
      "metadata": {
        "id": "smrLK_oimz_G"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "0pBa6FKgx39V"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rerbJgwAigbK"
      },
      "source": [
        "## **LLM 로드하기**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **일반 모델 로드하기**"
      ],
      "metadata": {
        "id": "htQHTrHOTlvL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "\n",
        "# 모델과 토크나이저를 로드합니다.\n",
        "model_id = \"microsoft/Phi-3-mini-4k-instruct\"\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=\"cuda\",\n",
        "    torch_dtype=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "print(f'\\n✅ 사용된 모델:\\n{model_id}')\n",
        "\n",
        "# 파이프라인을 만듭니다.\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    return_full_text=False,\n",
        "    max_new_tokens=500,\n",
        "    do_sample=False,\n",
        ")\n",
        "\n",
        "# 프롬프트\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Create a funny joke about chickens.\"}\n",
        "]\n",
        "\n",
        "# 프롬프트 템플릿을 적용합니다.\n",
        "prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False)\n",
        "print(f'\\n✅ 사용된 프롬프트 템플릿:\\n{prompt}')\n",
        "\n",
        "# 출력을 생성합니다.\n",
        "output = pipe(messages, do_sample=True, temperature=1)\n",
        "print(f'\\n✅ 출력결과:\\n{output[0][\"generated_text\"]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OMi0ho_UZ6WZ",
        "outputId": "e1ba0730-e2a8-4578-e67c-42a5ed23a906"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:transformers_modules.microsoft.Phi-3-mini-4k-instruct.0a67737cc96d2554230f90338b163bc6380a2a85.modeling_phi3:`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
            "WARNING:transformers_modules.microsoft.Phi-3-mini-4k-instruct.0a67737cc96d2554230f90338b163bc6380a2a85.modeling_phi3:Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n",
            "Device set to use cuda\n",
            "The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n",
            "`get_max_cache()` is deprecated for all Cache classes. Use `get_max_cache_shape()` instead. Calling `get_max_cache()` will raise error from v4.48\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ 사용된 모델:\n",
            "microsoft/Phi-3-mini-4k-instruct\n",
            "\n",
            "✅ 사용된 프롬프트 템플릿:\n",
            "<|user|>\n",
            "Create a funny joke about chickens.<|end|>\n",
            "<|endoftext|>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:transformers_modules.microsoft.Phi-3-mini-4k-instruct.0a67737cc96d2554230f90338b163bc6380a2a85.modeling_phi3:You are not running the flash-attention implementation, expect numerical differences.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ 출력결과:\n",
            " Why do chickens make terrible singers? Because they can't seem to find the right note to cluck!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **예제: 대화형 챗봇으로 사용**"
      ],
      "metadata": {
        "id": "Gon3X0gKffnY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"대화형 챗봇을 시작합니다. 종료하려면 'q'를 입력하세요.\\n\")\n",
        "\n",
        "# 대화 히스토리\n",
        "messages = []\n",
        "\n",
        "# 메인 루프\n",
        "while True:\n",
        "    user_input = input(\"✅ You: \").strip()\n",
        "\n",
        "    if user_input.lower() == 'q':\n",
        "        print(\"챗봇을 종료합니다.\")\n",
        "        break\n",
        "\n",
        "    if not user_input:\n",
        "        continue\n",
        "\n",
        "    # 사용자 메시지 추가\n",
        "    messages.append({\"role\": \"user\", \"content\": user_input})\n",
        "\n",
        "    # 응답 생성\n",
        "    output = pipe(messages, do_sample=True, temperature=0.7)\n",
        "    assistant_response = output[0][\"generated_text\"]\n",
        "\n",
        "    # 어시스턴트 메시지 추가\n",
        "    messages.append({\"role\": \"assistant\", \"content\": assistant_response})\n",
        "\n",
        "    print(f\"🤖 Bot: {assistant_response}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GD_dl7fifgMo",
        "outputId": "b0c64e1c-f54c-439b-e32e-5620ee005a35"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "대화형 챗봇을 시작합니다. 종료하려면 'q'를 입력하세요.\n",
            "\n",
            "✅ You: 제주도에 있는 맛집 추천\n",
            "🤖 Bot:  제주도에서 유명한 맛집 추천은 다음과 같습니다.\n",
            "\n",
            "\n",
            "1. **저주한 맛집** - 제주도의 전통적인 음식 중 하나, 맛있는 샐러드를 제공합니다.\n",
            "\n",
            "2. **해변 산뜻히 반쨌** - 해변 숲 속에 있는 예쁜 건물로, 저장소 및 맛집으로 전통적인 제주 음식을 처음으로 익히는 좋은 장소입니다.\n",
            "\n",
            "3. **제주 해바라기** - 따뜻한 해바라기를 찾기 힘든 순간을 가져와 있습니다.\n",
            "\n",
            "4. **더럽이 맛집** - 저주의 대표적인 음식점, 맛있는 전통 음식찾기입니다.\n",
            "\n",
            "5. **델스타그램 오라크 해변** - 해변 골목 같은 풍선을 볼 수 있는 더 예쁜 해바라기 맛집입니다.\n",
            "\n",
            "\n",
            "이 맛집들은 주로 저주의 전통 음식에 의해 유명하고 인기\n",
            "\n",
            "✅ You: q\n",
            "챗봇을 종료합니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **예제: 챗봇의 성능을 높여보자.**\n",
        "\n",
        "1. **모델 4-bit 양자화 설정**\n",
        "2. **torch.compile()로 모델 최적화** (PyTorch 2.0+) reduce overhead\n",
        "3. **대화 길이 제한**"
      ],
      "metadata": {
        "id": "YM6CueTEhH79"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 먼저 이 셀을 실행하세요\n",
        "!pip install -U bitsandbytes accelerate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "nSIHw70dhqjc",
        "outputId": "d381e03b-d3db-4f8e-98d2-c4ac4401f11e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.12/dist-packages (0.48.1)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.11.0)\n",
            "Requirement already satisfied: torch<3,>=2.3 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.8.0+cu126)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from accelerate) (6.0.3)\n",
            "Requirement already satisfied: huggingface_hub>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (0.35.3)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from accelerate) (0.6.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (2025.3.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (2.32.5)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (1.1.10)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<3,>=2.3->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<3,>=2.3->bitsandbytes) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2025.10.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, BitsAndBytesConfig\n",
        "\n",
        "# 1.4-bit 양자화 설정\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\"\n",
        ")\n",
        "\n",
        "# 모델과 토크나이저 로드\n",
        "model_id = \"microsoft/Phi-3-mini-4k-instruct\"\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=\"cuda\",\n",
        "    quantization_config=quantization_config,\n",
        "    trust_remote_code=True,\n",
        "    torch_dtype=torch.float16,\n",
        ")\n",
        "\n",
        "# torch.compile()로 모델 최적화 (PyTorch 2.0+)\n",
        "try:\n",
        "    model = torch.compile(model, mode=\"reduce-overhead\")\n",
        "    print(\"✅ torch.compile() 최적화 적용됨\")\n",
        "except:\n",
        "    print(\"⚠️ torch.compile() 사용 불가 (PyTorch 2.0+ 필요)\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "# 파이프라인 생성\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    return_full_text=False,\n",
        "    max_new_tokens=300,  # 500 -> 300으로 줄임\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DVuTdw7qcFFT",
        "outputId": "06e9d5c8-ffb7-43df-f33f-a8c912133de6"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ torch.compile() 최적화 적용됨\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda\n",
            "The model 'OptimizedModule' is not supported for text-generation. Supported models are ['AriaTextForCausalLM', 'BambaForCausalLM', 'BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'Cohere2ForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'DiffLlamaForCausalLM', 'ElectraForCausalLM', 'Emu3ForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'Olmo2ForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM', 'ZambaForCausalLM', 'Phi3ForCausalLM'].\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"대화형 챗봇을 시작합니다. 종료하려면 'q'를 입력하세요.\\n\")\n",
        "\n",
        "# 대화 히스토리 (최대 10개 메시지로 제한)\n",
        "messages = []\n",
        "MAX_HISTORY = 10      # 3. 대화 길이 제한\n",
        "\n",
        "# 메인 루프\n",
        "while True:\n",
        "    user_input = input(\"✅ You: \").strip()\n",
        "\n",
        "    if user_input.lower() == 'q':\n",
        "        print(\"\\n🔚 챗봇을 종료합니다.\")\n",
        "        break\n",
        "\n",
        "    if not user_input:\n",
        "        continue\n",
        "\n",
        "    # 사용자 메시지 추가\n",
        "    messages.append({\"role\": \"user\", \"content\": user_input})\n",
        "\n",
        "    # 히스토리 제한 (최신 10개만 유지)\n",
        "    if len(messages) > MAX_HISTORY:\n",
        "        messages = messages[-MAX_HISTORY:]\n",
        "\n",
        "    # 응답 생성\n",
        "    output = pipe(\n",
        "        messages,\n",
        "        do_sample=True,\n",
        "        temperature=0.7,\n",
        "        top_p=0.9,\n",
        "        repetition_penalty=1.1\n",
        "    )\n",
        "    assistant_response = output[0][\"generated_text\"]\n",
        "\n",
        "    # 어시스턴트 메시지 추가\n",
        "    messages.append({\"role\": \"assistant\", \"content\": assistant_response})\n",
        "\n",
        "    print(f\"🤖 Bot: {assistant_response}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "44feT-USkq31",
        "outputId": "e38b9e8c-0091-46b8-83b5-00607a7c2db1"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "대화형 챗봇을 시작합니다. 종료하려면 'q'를 입력하세요.\n",
            "\n",
            "✅ You: 제주도에 있는 맛집 추천\n",
            "🤖 Bot:  제주도의 네 가지 인기 축구가 있는 맛집은 다음과 같습니다:\n",
            "\n",
            "1. **아이스크림 리스트** - 제주시의 맛집 중 하나, 아이스크림 리스트는 고기를 익히면서 만들어 주는 일종의 식사 매장입니다. 그들은 향수로 주택 내부에 산수 먹사를 시키고, 옷을 입하면 항상 좋은 음식으로 전통적인 한국식 음식으로 조롱해야 합니다.\n",
            "\n",
            "2. **바다팀** - 이 맛집은 제주시의 인라인 메뉴셋을 엿보는 데에 경흅적입\n",
            "\n",
            "✅ You: 무지개의 7가지 색은?\n",
            "🤖 Bot:  무지개의 7가지 색은 다음과 같습니다:\n",
            "\n",
            "1. 청닭 (초록색)\n",
            "\n",
            "2. 코뿔 (노란색)\n",
            "\n",
            "3. 솔잎 (빨강색)\n",
            "\n",
            "4. 죽엿 (회색)\n",
            "\n",
            "5. 마음 (남색)\n",
            "\n",
            "6. 돌 (청색)\n",
            "\n",
            "7. 쇠솟이 (빨간색)\n",
            "\n",
            "\n",
            "무지개의 생선은 매년 여름 중에 첫 날부터 목욕 곡으로 분류됩니다. 이들은 대릴모통 등에 매우 최적의 수요가 있어 이전 목욕 곡에서는 더 많은 생선을 수여\n",
            "\n",
            "✅ You: q\n",
            "\n",
            "🔚 챗봇을 종료합니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **랭체인으로 양자화된 모델 로드하기**"
      ],
      "metadata": {
        "id": "N5FufrvYTpDf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **양자화된 모델** : **Phi-3-mini-4k-instruct-fp16.gguf**\n",
        "- **GGUF**란? : (GPT-Generated Unified Format)\n",
        "    - Phi-3, Llama-3, Mistral, Qwen, Gemma 같은 모델을 LangChain이나 Ollama, llama.cpp, LM Studio, GPT4All 등에서 쓸 때 자주 사용되는 포맷\n",
        "        - ex:  **LM Studio**  https://lmstudio.ai/download\n",
        "        - **LM Studio**는 로컬 LLM 실행용 GUI/엔진이며,\n",
        "        - **GGUF**는 빠른 로딩/추론을 위한 바이너리 포맷\n",
        "    - 개발자/배경** : \tllama.cpp 프로젝트(Georgi Gerganov) 팀에서 기존 GGML → GGUF로 확장한 새로운 포맷\n",
        "    - 등장 배경 :\t다양한 LLM(예: Llama2, Phi3, Mistral)을 CPU·GPU·모바일에서도 **빠르게 추론하기 위해 가볍게 저장하고 불러올 수 있는 공통 포맷이 필요**했기 때문\n",
        "    - **주요 목적**\t: **모델 가중치(weight)를 효율적으로 저장**하고, **로컬 디바이스에서 빠르게 불러와 추론**할 수 있도록 함\n",
        "    - **Huggingface GGUF 관련** : https://huggingface.co/docs/hub/gguf\n",
        "        - **gguf 모델 검색** : https://huggingface.co/models?library=gguf\n",
        "        - **gguf 모델 변환** : https://huggingface.co/spaces/ggml-org/gguf-my-repo\n",
        "        - 허깅페이스에서 모델 다운로드 링크 찾는 방법\n",
        "            - 모델 검색 > 모델 페이지에서 File 탭 선택 > 해당 모델 클릭 > Copy download link\n"
      ],
      "metadata": {
        "id": "lbrudSwylipt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4EYKJi4bCAYf",
        "outputId": "1d470d60-134d-4beb-f37c-4ce54ddae281"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-10-22 02:18:05--  https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/Phi-3-mini-4k-instruct-fp16.gguf\n",
            "Resolving huggingface.co (huggingface.co)... 13.35.202.121, 13.35.202.97, 13.35.202.34, ...\n",
            "Connecting to huggingface.co (huggingface.co)|13.35.202.121|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cas-bridge.xethub.hf.co/xet-bridge-us/662698108f7573e6a6478546/a9cdcf6e9514941ea9e596583b3d3c44dd99359fb7dd57f322bb84a0adc12ad4?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20251022%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20251022T021806Z&X-Amz-Expires=3600&X-Amz-Signature=1487c1afc95cb5fa8bf036438e7cb23f56913ce24ea76574de1bdfe662253cf0&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27Phi-3-mini-4k-instruct-fp16.gguf%3B+filename%3D%22Phi-3-mini-4k-instruct-fp16.gguf%22%3B&x-id=GetObject&Expires=1761103086&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc2MTEwMzA4Nn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82NjI2OTgxMDhmNzU3M2U2YTY0Nzg1NDYvYTljZGNmNmU5NTE0OTQxZWE5ZTU5NjU4M2IzZDNjNDRkZDk5MzU5ZmI3ZGQ1N2YzMjJiYjg0YTBhZGMxMmFkNCoifV19&Signature=LeuaLlyfkmXgCvsStdxJe3dVICzfljBjK23hDzyUx0ZosTdvBMdlHNXccxqob1q3qMjQtEj%7EPxU-PgsyS11zfBsSLcphsi-WopdT3yZftzbXgIvM1xPHTF8CfqpD24AqmtQ%7EjRuN5lxMNZl8fg2Qqc5bGC03YI-exOk8z-QYJr8hRkvnkK2dN6BtBcUF2CE-Z9%7Eu07xl21QdeciiO-Xtm0Hf2DlPnjeyfCKtintykwGAPdLnwwsSShJSSlAB5nw0XUrEW6pvPyr3D4seF8c874t4f%7EzasuNx8%7E4zgRUmLX-gctWfiraWdV0Ll02zz3A7Td9tFTRb77he62GrjhXN1A__&Key-Pair-Id=K2L8F4GPSG1IFC [following]\n",
            "--2025-10-22 02:18:06--  https://cas-bridge.xethub.hf.co/xet-bridge-us/662698108f7573e6a6478546/a9cdcf6e9514941ea9e596583b3d3c44dd99359fb7dd57f322bb84a0adc12ad4?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20251022%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20251022T021806Z&X-Amz-Expires=3600&X-Amz-Signature=1487c1afc95cb5fa8bf036438e7cb23f56913ce24ea76574de1bdfe662253cf0&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27Phi-3-mini-4k-instruct-fp16.gguf%3B+filename%3D%22Phi-3-mini-4k-instruct-fp16.gguf%22%3B&x-id=GetObject&Expires=1761103086&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc2MTEwMzA4Nn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82NjI2OTgxMDhmNzU3M2U2YTY0Nzg1NDYvYTljZGNmNmU5NTE0OTQxZWE5ZTU5NjU4M2IzZDNjNDRkZDk5MzU5ZmI3ZGQ1N2YzMjJiYjg0YTBhZGMxMmFkNCoifV19&Signature=LeuaLlyfkmXgCvsStdxJe3dVICzfljBjK23hDzyUx0ZosTdvBMdlHNXccxqob1q3qMjQtEj%7EPxU-PgsyS11zfBsSLcphsi-WopdT3yZftzbXgIvM1xPHTF8CfqpD24AqmtQ%7EjRuN5lxMNZl8fg2Qqc5bGC03YI-exOk8z-QYJr8hRkvnkK2dN6BtBcUF2CE-Z9%7Eu07xl21QdeciiO-Xtm0Hf2DlPnjeyfCKtintykwGAPdLnwwsSShJSSlAB5nw0XUrEW6pvPyr3D4seF8c874t4f%7EzasuNx8%7E4zgRUmLX-gctWfiraWdV0Ll02zz3A7Td9tFTRb77he62GrjhXN1A__&Key-Pair-Id=K2L8F4GPSG1IFC\n",
            "Resolving cas-bridge.xethub.hf.co (cas-bridge.xethub.hf.co)... 18.155.68.46, 18.155.68.69, 18.155.68.14, ...\n",
            "Connecting to cas-bridge.xethub.hf.co (cas-bridge.xethub.hf.co)|18.155.68.46|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7643295904 (7.1G)\n",
            "Saving to: ‘Phi-3-mini-4k-instruct-fp16.gguf’\n",
            "\n",
            "Phi-3-mini-4k-instr 100%[===================>]   7.12G   458MB/s    in 19s     \n",
            "\n",
            "2025-10-22 02:18:25 (379 MB/s) - ‘Phi-3-mini-4k-instruct-fp16.gguf’ saved [7643295904/7643295904]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/Phi-3-mini-4k-instruct-fp16.gguf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langchain langchain-core langchain-openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tmlBE6Rs9I-q",
        "outputId": "0173cbea-73c4-40c7-8bef-8f3c197b2d36"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (1.0.2)\n",
            "Requirement already satisfied: langchain-core in /usr/local/lib/python3.12/dist-packages (1.0.0)\n",
            "Requirement already satisfied: langchain-openai in /usr/local/lib/python3.12/dist-packages (1.0.1)\n",
            "Requirement already satisfied: langgraph<1.1.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langchain) (1.0.1)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.11.10)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (0.4.37)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (25.0)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (6.0.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (8.5.0)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (4.15.0)\n",
            "Requirement already satisfied: openai<3.0.0,>=1.109.1 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (1.109.1)\n",
            "Requirement already satisfied: tiktoken<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (0.12.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core) (3.0.0)\n",
            "Requirement already satisfied: langgraph-checkpoint<4.0.0,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.0->langchain) (3.0.0)\n",
            "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.0->langchain) (1.0.1)\n",
            "Requirement already satisfied: langgraph-sdk<0.3.0,>=0.2.2 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.0->langchain) (0.2.9)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.0->langchain) (3.6.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (3.11.3)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (1.0.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (2.32.5)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (0.25.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (0.11.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (4.67.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.2)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1.0.0,>=0.7.0->langchain-openai) (2024.11.6)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai<3.0.0,>=1.109.1->langchain-openai) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core) (0.16.0)\n",
            "Requirement already satisfied: ormsgpack>=1.10.0 in /usr/local/lib/python3.12/dist-packages (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.0->langchain) (1.11.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core) (2.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 라이브러리 버전 확인\n",
        "%pip list | grep -E 'langchain'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eSRUi0tJ952a",
        "outputId": "fec6d657-c889-47bb-829a-54949a2faac3"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "langchain                                1.0.2\n",
            "langchain-classic                        1.0.0\n",
            "langchain-community                      0.4\n",
            "langchain-core                           1.0.0\n",
            "langchain-openai                         1.0.1\n",
            "langchain-text-splitters                 1.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **예제: 양자화된 모델 로드하기**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MnGSxOySuPlD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LQcht_ZFijW7",
        "outputId": "2ff6b599-ef7c-4c79-be14-1b66d2b325ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_context: n_batch is less than GGML_KQ_MASK_PAD - increasing to 64\n"
          ]
        }
      ],
      "source": [
        "# ✅ 올바른 방식 (langchain v1.0.0+)\n",
        "from langchain_community.llms import LlamaCpp\n",
        "# c++로 된 라이브러리를 파이썬에서 가져와서 쓸수있게 함\n",
        "\n",
        "# 여러분의 컴퓨터에 다운로드한 모델의 경로를 입력하세요!\n",
        "llm = LlamaCpp(\n",
        "    model_path=\"./Phi-3-mini-4k-instruct-fp16.gguf\",  # 16비트 양자화된 모델\n",
        "    n_gpu_layers=-1,\n",
        "    max_tokens=500,\n",
        "    n_ctx=4096,\n",
        "    seed=42,\n",
        "    verbose=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "3SNhQF9WthzV",
        "outputId": "68342936-2dc6-47fa-c10d-1263bdad02e8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "''"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "llm.invoke(\"Hi! My name is Maarten. What is 1 + 1?\")\n",
        "\n",
        "# 실행하면 아무것도 출력되지 않는다.\n",
        "# 양자화된 모델은 그냥 사용할 수 없고\n",
        "# 특별한 프롬프트 템플릿 방법인 '체인'을 사용해야 한다."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **[실습] LM Studio 사용하기**\n",
        "\n",
        "- **LM Studio** : PC 에서 LLM 모델 사용/확인하기 위한 툴\n",
        "LM Studio를 PC에 설치하고  LM Studio에서 모델 사용해보기\n",
        "**굵은 텍스트**\n",
        "1. LM Studio 설치\n",
        "2. LM Studio에서 모델 사용\n",
        "    - 모델 검색: LM Studio 실행 → Models 탭 → 검색창에 phi-3 mini gguf 입력\n",
        "    - 모델 다운로드\n",
        "    - 다운로드가 끝나면 Chat 탭에서 바로 사용"
      ],
      "metadata": {
        "id": "cZb5-8CvwKDp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "5Ej9imzLNA1D"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wwx2AIuGfCoP"
      },
      "source": [
        "# **체인(Chain)**\n",
        "\n",
        "- **체인**은 **랭체인의 가장 기본적인 형태**, **단일 체인**\n",
        "- **체인을 사용해 LLM의 기능을 확장하거나 연결할 수 있다.**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **단일 체인** : 프롬프트 템플릿"
      ],
      "metadata": {
        "id": "i5k7ilDZwcsU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **예제 : 변수를 가진 템플릿 사용**"
      ],
      "metadata": {
        "id": "GLiBj4xu3lRU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "kF--Q5me_-X1"
      },
      "outputs": [],
      "source": [
        "# (LangChain v1.0.0+)\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "# \"input_prompt\" 변수를 가진 프롬프트 템플릿을 만듭니다.\n",
        "# |user|, |end| 등 = gpt계열에서 사용되는 특수 토큰들\n",
        "template = \"\"\"<|user|>\n",
        "{input_prompt}<|end|>\n",
        "<|assistant|>\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    template=template,\n",
        "    input_variables=[\"input_prompt\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "ogWsGeg6hElt"
      },
      "outputs": [],
      "source": [
        "# 첫 번째 체인 : 프롬프트 템플릿과 LLM을 연결\n",
        "basic_chain = prompt | llm      # __or__() 메서드 로 | 연산자를 오버로딩하여 사용함"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "KINQxKAINXgG",
        "outputId": "2991f6eb-3ff6-4ec4-8ebd-2fdd94018de4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Hello Maarten! The answer to 1 + 1 is 2.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "# 체인을 사용합니다.\n",
        "basic_chain.invoke(\n",
        "    {\n",
        "        \"input_prompt\": \"Hi! My name is Maarten. What is 1 + 1?\",\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 이름 생성을 위한 체인\n",
        "template = \"Create a funny name for a business that sells {product}.\"\n",
        "name_template = PromptTemplate(\n",
        "    template=template,\n",
        "    input_variables=[\"product\"],\n",
        ")\n",
        "\n",
        "# name_template 을 사용해서 체인 사용하기\n",
        "name_chain = name_template | llm\n",
        "\n",
        "# name_chain 실행하기\n",
        "# name_chain.invoke({\"product\": \"colorful socks\"})\n",
        "name_chain.invoke({\"product\": \"Colorful polka dot socks\"})\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "VWHh83P8zDWQ",
        "outputId": "35556363-a652-4dc4-e82b-1fcedaa7a1c4"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n<|assistant|> Polka Dots & Puns: The Sock-tacular Emporium!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSMBMRxB8gFW"
      },
      "source": [
        "## **여러 템플릿을 가진 체인**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **예제 : 프롬프트를 쪼개서 하위 작업을 순차적으로 실행**\n",
        "- 이야기를 생성하는 과정(단계별 처리)\n",
        "    - 제목\n",
        "    - 주요 캐릭터에 대한 설명\n",
        "    - 이야기 요약"
      ],
      "metadata": {
        "id": "pBRimaj_3Gql"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# llm 객체는 이미 정의되어 있다고 가정\n",
        "# 예: llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
        "# 또는: llm = LlamaCpp(model_path=\"...\")\n",
        "\n",
        "# 이야기 제목을 위한 프롬프트 템플릿\n",
        "# ==========================================\n",
        "# 1단계: 제목 생성 체인\n",
        "# ==========================================\n",
        "template = \"\"\"<|user|>\n",
        "Create a title for a story about {summary}. Only return the title.<|end|>\n",
        "<|assistant|>\"\"\"\n",
        "\n",
        "title_prompt = PromptTemplate(\n",
        "    template=template,\n",
        "    input_variables=[\"summary\"]\n",
        ")\n",
        "\n",
        "# LCEL 체인 구성 (파이프 연산자)\n",
        "title_chain = title_prompt | llm | StrOutputParser()\n",
        "\n",
        "# 실행 (invoke 메서드 사용)\n",
        "result = title_chain.invoke({\"summary\": \"a brave knight fighting a dragon\"})\n",
        "print(f\"생성된 제목: {result}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iJA4xfquAtle",
        "outputId": "1accd6ed-1844-4265-b61d-526d1826cc8a"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "생성된 제목:  \"Sir Valor's Flame: The Dragon's Tale\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "igFIyg73OtaL",
        "outputId": "cab40c90-b30c-4496-f167-a39b5982c78d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' \"Finding Light in the Shadow: A Motherless Journey\"'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "title_chain.invoke({\"summary\": \"a girl that lost her mother\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "zTtFEmANOhyE"
      },
      "outputs": [],
      "source": [
        "# 요약과 제목을 사용하여 캐릭터 설명을 생성하는 체인을 만듭니다.\n",
        "# ==========================================\n",
        "# 2단계: 캐릭터 생성 체인\n",
        "# ==========================================\n",
        "template = \"\"\"<|user|>\n",
        "Describe the main character of a story about {summary} with the title {title}.\n",
        "Use only two sentences.<|end|>\n",
        "<|assistant|>\"\"\"\n",
        "\n",
        "character_prompt = PromptTemplate(\n",
        "    template=template, input_variables=[\"summary\", \"title\"]\n",
        ")\n",
        "\n",
        "# 파이프 연산자로 체인 구성\n",
        "character_chain = character_prompt | llm | StrOutputParser()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "Xjf-avW8NAqZ"
      },
      "outputs": [],
      "source": [
        "# 요약, 제목, 캐릭터 설명을 사용해 이야기를 생성하는 체인을 만듭니다.\n",
        "# ==========================================\n",
        "# 3단계: 이야기 생성 체인\n",
        "# ==========================================\n",
        "template = \"\"\"<|user|>\n",
        "Create a story about {summary} with the title {title}.\n",
        "The main charachter is: {character}.\n",
        "Only return the story and it cannot be longer than one paragraph<|end|>\n",
        "<|assistant|>\"\"\"\n",
        "\n",
        "story_prompt = PromptTemplate(\n",
        "    template=template,\n",
        "    input_variables=[\"summary\", \"title\", \"character\"]\n",
        ")\n",
        "\n",
        "# 파이프 연산자로 체인 구성\n",
        "story_chain = story_prompt | llm | StrOutputParser()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "epNudKyyPClO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd8658be-9422-4610-a8e2-0aa8b303a8c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "요약: a girl that lost her mother\n",
            "제목:  \"Losing Her Light: A Mother's Legacy in Emily's Heart\"\n",
            "캐릭터 설명:  Emily is a resilient and compassionate young girl, who struggles to come to terms with the loss of her mother. As she navigates through grief, she discovers strength in cherishing her mother's enduring love and wisdom that lives on within her heart.\n",
            "스토리:  Emily's heart ached as the emptiness of her mother's absence consumed her, yet within that void shone a steadfast light - an undying love and wisdom passed down through generations. Each day became a journey to reconnect with her mother's legacy, finding solace in cherished memories and embracing strength born from the compassionate lessons she imparted. In this heart-wrenching voyage of loss, Emily discovered that though her mother's physical presence was gone, her light continued to guide her, igniting a resilience within her young soul that promised enduring hope and unbreakable love in the face of grief.\n"
          ]
        }
      ],
      "source": [
        "# 세 개의 요소를 연결하여 최종 체인을 만듭니다.\n",
        "# ==========================================\n",
        "# 전체 파이프라인 실행 함수\n",
        "# ==========================================\n",
        "def generate_complete_story(summary):\n",
        "    \"\"\"3단계 체인을 순차적으로 실행\"\"\"\n",
        "\n",
        "    # 1단계: 제목 생성\n",
        "    title = title_chain.invoke({\"summary\": summary})\n",
        "\n",
        "    # 2단계: 캐릭터 생성 (summary + title 사용)\n",
        "    character = character_chain.invoke({\n",
        "        \"summary\": summary,\n",
        "        \"title\": title\n",
        "    })\n",
        "\n",
        "    # 3단계: 이야기 생성 (summary + title + character 사용)\n",
        "    story = story_chain.invoke({\n",
        "        \"summary\": summary,\n",
        "        \"title\": title,\n",
        "        \"character\": character\n",
        "    })\n",
        "\n",
        "    return {\n",
        "        \"summary\": summary,\n",
        "        \"title\": title,\n",
        "        \"character\": character,\n",
        "        \"story\": story\n",
        "    }\n",
        "\n",
        "# 실행\n",
        "result = generate_complete_story(\"a girl that lost her mother\")\n",
        "\n",
        "# 결과 출력\n",
        "print(f\"요약: {result['summary']}\")\n",
        "print(f\"제목: {result['title']}\")\n",
        "print(f\"캐릭터 설명: {result['character']}\")\n",
        "print(f\"스토리: {result['story']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **[실습] 한국어 지원 경량화 모델을 이용하여 이야기 만들기**\n",
        "1. **한국어 지원 경량화 모델 다운로드 받기**\n",
        "    - 허깅페이스 / LM studio 등\n",
        "    - ex: llama-3-Korean-Bllossom-3B, Konan-LLM-OND-gguf 등 경량화 모델 사용\n",
        "2. **3단계 체인 템플릿을 만들고 Story 출력하기**\n",
        "    - 앞에서 사용한 여러 템플릿을 가진 체인 내용을 참고하여 3단계 체인 템플릿을 만들고 Story 출력하기\n",
        "    - story = summary + title + character"
      ],
      "metadata": {
        "id": "R69wJNaZIHgG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://huggingface.co/Bllossom/llama-3.2-Korean-Bllossom-3B-gguf-Q4_K_M/resolve/main/llama-3.2-Korean-Bllossom-3B-gguf-Q4_K_M.gguf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pg8vuE9aNEpR",
        "outputId": "11fb8498-ac9a-4674-9c14-49fcfdaa4288"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-10-22 02:53:55--  https://huggingface.co/Bllossom/llama-3.2-Korean-Bllossom-3B-gguf-Q4_K_M/resolve/main/llama-3.2-Korean-Bllossom-3B-gguf-Q4_K_M.gguf\n",
            "Resolving huggingface.co (huggingface.co)... 13.35.202.97, 13.35.202.34, 13.35.202.40, ...\n",
            "Connecting to huggingface.co (huggingface.co)|13.35.202.97|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cas-bridge.xethub.hf.co/xet-bridge-us/670664a02d3883ec801acb9a/2eca4ef8aad90644a92e57d6138d0fee3083d5f1bf970faf11c226ea7146b75a?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20251022%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20251022T025356Z&X-Amz-Expires=3600&X-Amz-Signature=328b83bf3304d4736372a466577f491c3213dab5f9f93700fe820d50b9141da2&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27llama-3.2-Korean-Bllossom-3B-gguf-Q4_K_M.gguf%3B+filename%3D%22llama-3.2-Korean-Bllossom-3B-gguf-Q4_K_M.gguf%22%3B&x-id=GetObject&Expires=1761105236&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc2MTEwNTIzNn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82NzA2NjRhMDJkMzg4M2VjODAxYWNiOWEvMmVjYTRlZjhhYWQ5MDY0NGE5MmU1N2Q2MTM4ZDBmZWUzMDgzZDVmMWJmOTcwZmFmMTFjMjI2ZWE3MTQ2Yjc1YSoifV19&Signature=gY4%7E25qCN3Bdx4Gzq2PcsVRR1bXDhGTuLpJOp%7E0mL6-KtvSnFJ2sk2sfL6tnDD5ml4t%7EkpCNS8pVmeHq7utjjp01yAL8t%7E6NMrGcWQ2737-NkmY9SFaceK07ddgcCToZZyWRPOgOg0eqO4dFilFIBqNpFVW6T2zXidc4dPd5fL1cq7xsbRfVKJwbT-%7EqLnO%7EfnKDUxfHwS1ACuktYVSC0gQTbQitsDwPDWs59ja0czPA6AQIs2Y%7EjWpc1ptzptcnXoa916CO9wnOIAlH9OWg%7EPj1U-u9-Fl-kzYHur%7EMkyGjbfqZ-DU9npbhcw6GcsFi76OroyPTWZESzaWPLsaVfA__&Key-Pair-Id=K2L8F4GPSG1IFC [following]\n",
            "--2025-10-22 02:53:56--  https://cas-bridge.xethub.hf.co/xet-bridge-us/670664a02d3883ec801acb9a/2eca4ef8aad90644a92e57d6138d0fee3083d5f1bf970faf11c226ea7146b75a?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20251022%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20251022T025356Z&X-Amz-Expires=3600&X-Amz-Signature=328b83bf3304d4736372a466577f491c3213dab5f9f93700fe820d50b9141da2&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27llama-3.2-Korean-Bllossom-3B-gguf-Q4_K_M.gguf%3B+filename%3D%22llama-3.2-Korean-Bllossom-3B-gguf-Q4_K_M.gguf%22%3B&x-id=GetObject&Expires=1761105236&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc2MTEwNTIzNn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82NzA2NjRhMDJkMzg4M2VjODAxYWNiOWEvMmVjYTRlZjhhYWQ5MDY0NGE5MmU1N2Q2MTM4ZDBmZWUzMDgzZDVmMWJmOTcwZmFmMTFjMjI2ZWE3MTQ2Yjc1YSoifV19&Signature=gY4%7E25qCN3Bdx4Gzq2PcsVRR1bXDhGTuLpJOp%7E0mL6-KtvSnFJ2sk2sfL6tnDD5ml4t%7EkpCNS8pVmeHq7utjjp01yAL8t%7E6NMrGcWQ2737-NkmY9SFaceK07ddgcCToZZyWRPOgOg0eqO4dFilFIBqNpFVW6T2zXidc4dPd5fL1cq7xsbRfVKJwbT-%7EqLnO%7EfnKDUxfHwS1ACuktYVSC0gQTbQitsDwPDWs59ja0czPA6AQIs2Y%7EjWpc1ptzptcnXoa916CO9wnOIAlH9OWg%7EPj1U-u9-Fl-kzYHur%7EMkyGjbfqZ-DU9npbhcw6GcsFi76OroyPTWZESzaWPLsaVfA__&Key-Pair-Id=K2L8F4GPSG1IFC\n",
            "Resolving cas-bridge.xethub.hf.co (cas-bridge.xethub.hf.co)... 18.155.68.46, 18.155.68.69, 18.155.68.125, ...\n",
            "Connecting to cas-bridge.xethub.hf.co (cas-bridge.xethub.hf.co)|18.155.68.46|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2019377664 (1.9G)\n",
            "Saving to: ‘llama-3.2-Korean-Bllossom-3B-gguf-Q4_K_M.gguf’\n",
            "\n",
            "llama-3.2-Korean-Bl 100%[===================>]   1.88G   107MB/s    in 7.9s    \n",
            "\n",
            "2025-10-22 02:54:04 (242 MB/s) - ‘llama-3.2-Korean-Bllossom-3B-gguf-Q4_K_M.gguf’ saved [2019377664/2019377664]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget !wget https://huggingface.co/mykor/Konan-LLM-OND-gguf/resolve/main/Konan-LLM-OND-Q4_K_M.gguf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E5aQLfWxQMBa",
        "outputId": "8b9f598a-d746-4746-9d26-6458e2e404ff"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-10-22 02:54:04--  http://!wget/\n",
            "Resolving !wget (!wget)... failed: Name or service not known.\n",
            "wget: unable to resolve host address ‘!wget’\n",
            "--2025-10-22 02:54:04--  https://huggingface.co/mykor/Konan-LLM-OND-gguf/resolve/main/Konan-LLM-OND-Q4_K_M.gguf\n",
            "Resolving huggingface.co (huggingface.co)... 13.35.202.97, 13.35.202.34, 13.35.202.121, ...\n",
            "Connecting to huggingface.co (huggingface.co)|13.35.202.97|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cas-bridge.xethub.hf.co/xet-bridge-us/68803db977925a8b83368521/ec9f7e7a45e6ae32b79e8928592791e61234cb8f26f1d08db9b88b4d5695db00?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20251022%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20251022T025404Z&X-Amz-Expires=3600&X-Amz-Signature=df20df24dda4155498cc83cb489f8da92d2ccc2d12bf140439f354ba2bf09c1a&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27Konan-LLM-OND-Q4_K_M.gguf%3B+filename%3D%22Konan-LLM-OND-Q4_K_M.gguf%22%3B&x-id=GetObject&Expires=1761105244&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc2MTEwNTI0NH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82ODgwM2RiOTc3OTI1YThiODMzNjg1MjEvZWM5ZjdlN2E0NWU2YWUzMmI3OWU4OTI4NTkyNzkxZTYxMjM0Y2I4ZjI2ZjFkMDhkYjliODhiNGQ1Njk1ZGIwMCoifV19&Signature=HIVtO2l3FgZCxXiaIa4YwOriqmZLmSlTQfPu5aPf9uAeoV1-Ja2yOKapbU5e8nm0rDOiUkbaQbQmJl3RltOsU2n-IHzkW-YXjxEOurUSBFRMlrSbTgxzbxrYBf10Da%7ES9G1MdKGLmVWQp9X8p13BXWWcDC1sGw%7EzeBEVC%7EY2mCfXpp5cyUrOMDrnsGn3hesIGo5A%7EeTLbhT2eWRYIThGFSqRcVg7U4cvuHgy8d8IRHa3IvQFtE6rjXt%7EADZxex0UvJ0ewcye3C1CrUn8XrG6cT54j8%7EQNois0wC6GARET%7ELtBaZ0InDK4p3lEoIvXpxpoPRQY0PSNLzd5vWkLolrbw__&Key-Pair-Id=K2L8F4GPSG1IFC [following]\n",
            "--2025-10-22 02:54:04--  https://cas-bridge.xethub.hf.co/xet-bridge-us/68803db977925a8b83368521/ec9f7e7a45e6ae32b79e8928592791e61234cb8f26f1d08db9b88b4d5695db00?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20251022%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20251022T025404Z&X-Amz-Expires=3600&X-Amz-Signature=df20df24dda4155498cc83cb489f8da92d2ccc2d12bf140439f354ba2bf09c1a&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27Konan-LLM-OND-Q4_K_M.gguf%3B+filename%3D%22Konan-LLM-OND-Q4_K_M.gguf%22%3B&x-id=GetObject&Expires=1761105244&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc2MTEwNTI0NH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82ODgwM2RiOTc3OTI1YThiODMzNjg1MjEvZWM5ZjdlN2E0NWU2YWUzMmI3OWU4OTI4NTkyNzkxZTYxMjM0Y2I4ZjI2ZjFkMDhkYjliODhiNGQ1Njk1ZGIwMCoifV19&Signature=HIVtO2l3FgZCxXiaIa4YwOriqmZLmSlTQfPu5aPf9uAeoV1-Ja2yOKapbU5e8nm0rDOiUkbaQbQmJl3RltOsU2n-IHzkW-YXjxEOurUSBFRMlrSbTgxzbxrYBf10Da%7ES9G1MdKGLmVWQp9X8p13BXWWcDC1sGw%7EzeBEVC%7EY2mCfXpp5cyUrOMDrnsGn3hesIGo5A%7EeTLbhT2eWRYIThGFSqRcVg7U4cvuHgy8d8IRHa3IvQFtE6rjXt%7EADZxex0UvJ0ewcye3C1CrUn8XrG6cT54j8%7EQNois0wC6GARET%7ELtBaZ0InDK4p3lEoIvXpxpoPRQY0PSNLzd5vWkLolrbw__&Key-Pair-Id=K2L8F4GPSG1IFC\n",
            "Resolving cas-bridge.xethub.hf.co (cas-bridge.xethub.hf.co)... 18.155.68.46, 18.155.68.69, 18.155.68.125, ...\n",
            "Connecting to cas-bridge.xethub.hf.co (cas-bridge.xethub.hf.co)|18.155.68.46|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2542139648 (2.4G)\n",
            "Saving to: ‘Konan-LLM-OND-Q4_K_M.gguf’\n",
            "\n",
            "Konan-LLM-OND-Q4_K_ 100%[===================>]   2.37G   324MB/s    in 8.4s    \n",
            "\n",
            "2025-10-22 02:54:12 (289 MB/s) - ‘Konan-LLM-OND-Q4_K_M.gguf’ saved [2542139648/2542139648]\n",
            "\n",
            "FINISHED --2025-10-22 02:54:12--\n",
            "Total wall clock time: 8.7s\n",
            "Downloaded: 1 files, 2.4G in 8.4s (289 MB/s)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.llms import LlamaCpp\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# 여러분의 컴퓨터에 다운로드한 모델의 경로를 입력하세요!\n",
        "llm = LlamaCpp(\n",
        "    # model_path=\"./Phi-3-mini-4k-instruct-fp16.gguf\",  # 16비트 양자화된 모델\n",
        "    # model_path = \"llama-3.2-Korean-Bllossom-3B-gguf-Q4_K_M.gguf\",\n",
        "    model_path = \"./Konan-LLM-OND-Q4_K_M.gguf\",\n",
        "    n_gpu_layers=-1,\n",
        "    max_tokens=500,\n",
        "    n_ctx=4096,\n",
        "    seed=42,\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "\n",
        "# 이야기 제목을 위한 프롬프트 템플릿\n",
        "# ==========================================\n",
        "# 1단계: 제목 생성 체인\n",
        "# ==========================================\n",
        "template = \"\"\"<|user|>\n",
        "{summary}에 대한 기사의 제목을 만듭니다. 제목만 반환합니다.<|end|>\n",
        "<|assistant|>\"\"\"\n",
        "\n",
        "title_prompt = PromptTemplate(\n",
        "    template=template,\n",
        "    input_variables=[\"summary\"]\n",
        ")\n",
        "\n",
        "# LCEL 체인 구성 (파이프 연산자)\n",
        "title_chain = title_prompt | llm | StrOutputParser()\n",
        "\n",
        "\n",
        "# ==========================================\n",
        "# 2단계: 캐릭터 생성 체인\n",
        "# ==========================================\n",
        "template = \"\"\"<|user|>\n",
        "{summary}에 대한 이야기의 주인공을 설명하세요. 제목은 {title}입니다.\n",
        "두 문장만 사용하세요.<|end|>\n",
        "<|assistant|>\"\"\"\n",
        "\n",
        "character_prompt = PromptTemplate(\n",
        "    template=template, input_variables=[\"summary\", \"title\"]\n",
        ")\n",
        "\n",
        "# 파이프 연산자로 체인 구성\n",
        "character_chain = character_prompt | llm | StrOutputParser()\n",
        "\n",
        "# ==========================================\n",
        "# 3단계: 이야기 생성 체인\n",
        "# ==========================================\n",
        "template = \"\"\"<|user|>\n",
        "{summary}에 대한 스토리를 {title}이라는 제목으로 작성하세요.\n",
        "주인공은 {character}입니다.\n",
        "스토리만 반환하며, 한 단락을 넘을 수 없습니다.<|end|>\n",
        "<|assistant|>\"\"\"\n",
        "\n",
        "story_prompt = PromptTemplate(\n",
        "    template=template,\n",
        "    input_variables=[\"summary\", \"title\", \"character\"]\n",
        ")\n",
        "\n",
        "# 파이프 연산자로 체인 구성\n",
        "story_chain = story_prompt | llm | StrOutputParser()\n",
        "\n",
        "\n",
        "# ==========================================\n",
        "# 전체 파이프라인 실행 함수\n",
        "# ==========================================\n",
        "def generate_complete_story(summary):\n",
        "    \"\"\"3단계 체인을 순차적으로 실행\"\"\"\n",
        "\n",
        "    # 1단계: 제목 생성\n",
        "    title = title_chain.invoke({\"summary\": summary})\n",
        "\n",
        "    # 2단계: 캐릭터 생성 (summary + title 사용)\n",
        "    character = character_chain.invoke({\n",
        "        \"summary\": summary,\n",
        "        \"title\": title\n",
        "    })\n",
        "\n",
        "    # 3단계: 이야기 생성 (summary + title + character 사용)\n",
        "    story = story_chain.invoke({\n",
        "        \"summary\": summary,\n",
        "        \"title\": title,\n",
        "        \"character\": character\n",
        "    })\n",
        "\n",
        "    return {\n",
        "        \"summary\": summary,\n",
        "        \"title\": title,\n",
        "        \"character\": character,\n",
        "        \"story\": story\n",
        "    }\n",
        "\n",
        "# 실행\n",
        "result = generate_complete_story(\"어머니를 잃은 소녀\")\n",
        "\n",
        "# 결과 출력\n",
        "print(f\"✅ 요약: {result['summary']}\")\n",
        "print(f\"✅ 제목: {result['title']}\")\n",
        "print(f\"✅ 캐릭터 설명: {result['character']}\")\n",
        "print(f\"✅ 스토리: {result['story']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lNVj5JcTIHrL",
        "outputId": "f38fcb2a-e0d6-4e7a-dc59-b75d9e2991db"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_context: n_batch is less than GGML_KQ_MASK_PAD - increasing to 64\n",
            "llama_context: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ 요약: 어머니를 잃은 소녀\n",
            "✅ 제목:  \"어머니의 사랑을 품고, 소녀는 다시 일어섰다.\"\n",
            "✅ 캐릭터 설명:  소녀는 어머니를 잃은 깊은 상처 속에서도 강인하게 성장했다. 그녀는 어머니의 사랑과 가르침을 가슴에 품고, 다시 일어나 세상을 향해 자신의 길을 걸어나갔다.\n",
            "✅ 스토리:  어머니를 잃은 깊은 상처 속에서도 소녀는 강인하게 성장했다. 그녀는 어머니의 사랑과 가르침을 가슴에 품고, 다시 일어나 세상을 향해 자신의 길을 걸어나갔다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "5gTcrpdUUu6J"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7UQ-DZ71P-D-"
      },
      "source": [
        "# **메모리**\n",
        "대화를 기억하도록 LLM 돕기\n",
        "\n",
        "- **LLM을 그대로 사용하면대화의 내용을 기억하지 못한다.** --> **메모리가 없다.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "-15Eoey5EJUO",
        "outputId": "46678d08-a76b-48f5-b7bd-70b7d03fca33"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Hello Maarten! The answer to 1 + 1 is 2.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "# LLM에게 이름을 알려 줍니다.\n",
        "basic_chain.invoke({\"input_prompt\": \"Hi! My name is Maarten. What is 1 + 1?\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "N42wQRl-Lykt",
        "outputId": "df62e835-ba82-428e-d08d-7255ed9688d1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" I'm sorry, but as a digital assistant, I don't have the ability to know personal information about individuals unless it has been shared with me in the course of our conversation. To ensure your privacy and data protection, please do not share sensitive personal details.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "# LLM에게 이름을 묻습니다.\n",
        "basic_chain.invoke({\"input_prompt\": \"What is my name?\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- <mark>**LLM이 대화를 기억하도록 돕는 방법**</mark>\n",
        "    - **대화 버퍼**(conversation buffer)\n",
        "    - **대화 요약**(conversation summary)"
      ],
      "metadata": {
        "id": "xNJuWEVRV7zD"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PfqATEZjMgET"
      },
      "source": [
        "### **대화 버퍼**\n",
        "\n",
        "- **가장 간단한 LLM 메모리 형태**\n",
        "- **과거의 대화를 그대로 전달** : 대화 이력을 모두 복사하여 프롬프트에 추가\n",
        "- llangchain_community : **ChatMessageHistory**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "Zoo0PA1fUs70"
      },
      "outputs": [],
      "source": [
        "# 대화 기록을 담을 수 있도록 프롬프트를 업데이트합니다.\n",
        "template = \"\"\"<|user|>Current conversation:{chat_history}\n",
        "\n",
        "{input_prompt}<|end|>\n",
        "<|assistant|>\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    template=template,\n",
        "    input_variables=[\"input_prompt\", \"chat_history\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.llms import LlamaCpp\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_community.chat_message_histories import ChatMessageHistory\n",
        "\n",
        "# ==========================================\n",
        "# 전역 설정\n",
        "# ==========================================\n",
        "\n",
        "# LlamaCpp 모델 초기화\n",
        "llm = LlamaCpp(\n",
        "    model_path=\"Konan-LLM-OND-Q4_K_M.gguf\",\n",
        "    n_gpu_layers=-1,\n",
        "    max_tokens=500,\n",
        "    n_ctx=4096,\n",
        "    seed=42,\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "# 기본 프롬프트 템플릿\n",
        "template = \"\"\"<|user|>이전 대화:\n",
        "{chat_history}\n",
        "\n",
        "현재 질문: {input_prompt}<|end|>\n",
        "<|assistant|>\"\"\"\n",
        "\n",
        "# 프롬프트와 체인 구성\n",
        "prompt = PromptTemplate(\n",
        "    template=template,\n",
        "    input_variables=[\"input_prompt\", \"chat_history\"]\n",
        ")\n",
        "\n",
        "chain = prompt | llm | StrOutputParser()\n",
        "\n",
        "# 세션 저장소\n",
        "sessions = {}\n",
        "\n",
        "# ==========================================\n",
        "# 핵심 함수들\n",
        "# ==========================================\n",
        "\n",
        "def get_history(session_id: str = \"default\") -> ChatMessageHistory:\n",
        "    \"\"\"세션 히스토리 가져오기\"\"\"\n",
        "    if session_id not in sessions:\n",
        "        sessions[session_id] = ChatMessageHistory()\n",
        "    return sessions[session_id]\n",
        "\n",
        "def format_history(messages) -> str:\n",
        "    \"\"\"메시지를 문자열로 변환\"\"\"\n",
        "    if not messages:\n",
        "        return \"대화 기록 없음\"\n",
        "    return \"\\n\".join(\n",
        "        f\"{'사용자' if msg.type == 'human' else 'AI'}: {msg.content}\"\n",
        "        for msg in messages\n",
        "    )\n",
        "\n",
        "def chat(user_input: str, session_id: str = \"default\") -> str:\n",
        "    \"\"\"대화 실행\"\"\"\n",
        "    history = get_history(session_id)\n",
        "\n",
        "    response = chain.invoke({\n",
        "        \"input_prompt\": user_input,\n",
        "        \"chat_history\": format_history(history.messages)\n",
        "    })\n",
        "\n",
        "    history.add_user_message(user_input)\n",
        "    history.add_ai_message(response)\n",
        "\n",
        "    return response\n",
        "\n",
        "def get_chat_history(session_id: str = \"default\") -> list:\n",
        "    \"\"\"특정 세션의 대화 기록 반환\"\"\"\n",
        "    history = get_history(session_id)\n",
        "    return [\n",
        "        {\"role\": msg.type, \"content\": msg.content}\n",
        "        for msg in history.messages\n",
        "    ]\n",
        "\n",
        "def clear_history(session_id: str = \"default\"):\n",
        "    \"\"\"특정 세션의 대화 기록 초기화\"\"\"\n",
        "    if session_id in sessions:\n",
        "        sessions[session_id].clear()\n",
        "\n",
        "def get_all_sessions() -> list:\n",
        "    \"\"\"모든 세션 ID 반환\"\"\"\n",
        "    return list(sessions.keys())\n",
        "\n",
        "# ==========================================\n",
        "# 사용 예시\n",
        "# ==========================================\n",
        "\n",
        "# 대화 실행\n",
        "response1 = chat(\"안녕하세요!\", session_id=\"user_001\")\n",
        "response2 = chat(\"파이썬이란?\", session_id=\"user_001\")\n",
        "response3 = chat(\"제가 처음에 뭐라고 했죠?\", session_id=\"user_001\")\n",
        "\n",
        "# 대화 기록 확인\n",
        "history = get_chat_history(session_id=\"user_001\")\n",
        "\n",
        "print(f\"✅ 응답 1: {response1}\")\n",
        "print(f\"✅ 응답 2: {response2}\")\n",
        "print(f\"✅ 응답 3: {response3}\")\n",
        "print(f\"\\n✅ 대화 기록: {len(history)}개 메시지\")\n",
        "print(f\"✅ 대화 기록: {history}\")\n",
        "\n",
        "# 대화 기록 삭제\n",
        "clear_history(session_id=\"user_001\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jbqStW_seFyO",
        "outputId": "f0334e00-ee60-4967-fc1d-077e4e8095e7"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_context: n_batch is less than GGML_KQ_MASK_PAD - increasing to 64\n",
            "llama_context: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ 응답 1: 안녕하세요! 어떻게 도와드릴까요?\n",
            "✅ 응답 2: 파이썬(Python)은 1990년대 초에 귀도 반 로섬(Guido van Rossum)이 개발한 고급 프로그래밍 언어입니다.\n",
            "\n",
            "### 파이썬의 주요 특징:\n",
            "1. **쉬운 문법**: 파이썬은 읽기 쉽고 간결한 문법을 가지고 있어, 초보자부터 전문가까지 폭넓게 사용됩니다.\n",
            "\n",
            "2. **인터프리터 언어**: 파이썬은 컴파일 없이 바로 실행할 수 있는 인터프리터 언어입니다.\n",
            "\n",
            "3. **다양한 라이브러리와 프레임워크**: 파이썬은 데이터 분석, 머신러닝, 웹 개발 등 다양한 분야에서 사용할 수 있도록 풍부한 라이브러리와 프레임워크를 제공합니다.\n",
            "\n",
            "4. **플랫폼 독립적**: 파이썬은 윈도우, 맥OS, 리눅스 등 다양한 운영체제에서 동일하게 실행될 수 있습니다.\n",
            "\n",
            "### 파이썬의 주요 활용 분야:\n",
            "- **웹 개발**: Django, Flask 등의 프레임워크를 사용하여 웹 애플리케이션을 개발할 수 있습니다.\n",
            "- **데이터 분석 및 시각화**: Pandas, NumPy, Matplotlib, Seaborn 등의 라이브러리를 사용하여 데이터를 분석하고 시각화할 수 있습니다.\n",
            "- **머신러닝 및 인공지능**: Scikit-learn, TensorFlow, PyTorch 등의 라이브러리와 프레임워크를 사용하여 머신러닝 모델을 개발하고 훈련시킬 수 있습니다.\n",
            "\n",
            "### 결론:\n",
            "파이썬은 그 간결한 문법과 풍부한 라이브러리 덕분에 초보자부터 전문가까지 폭넓게 사랑받는 프로그래밍 언어입니다.\n",
            "✅ 응답 3: \n",
            "\n",
            "✅ 대화 기록: 6개 메시지\n",
            "✅ 대화 기록: [{'role': 'human', 'content': '안녕하세요!'}, {'role': 'ai', 'content': '안녕하세요! 어떻게 도와드릴까요?'}, {'role': 'human', 'content': '파이썬이란?'}, {'role': 'ai', 'content': '파이썬(Python)은 1990년대 초에 귀도 반 로섬(Guido van Rossum)이 개발한 고급 프로그래밍 언어입니다.\\n\\n### 파이썬의 주요 특징:\\n1. **쉬운 문법**: 파이썬은 읽기 쉽고 간결한 문법을 가지고 있어, 초보자부터 전문가까지 폭넓게 사용됩니다.\\n\\n2. **인터프리터 언어**: 파이썬은 컴파일 없이 바로 실행할 수 있는 인터프리터 언어입니다.\\n\\n3. **다양한 라이브러리와 프레임워크**: 파이썬은 데이터 분석, 머신러닝, 웹 개발 등 다양한 분야에서 사용할 수 있도록 풍부한 라이브러리와 프레임워크를 제공합니다.\\n\\n4. **플랫폼 독립적**: 파이썬은 윈도우, 맥OS, 리눅스 등 다양한 운영체제에서 동일하게 실행될 수 있습니다.\\n\\n### 파이썬의 주요 활용 분야:\\n- **웹 개발**: Django, Flask 등의 프레임워크를 사용하여 웹 애플리케이션을 개발할 수 있습니다.\\n- **데이터 분석 및 시각화**: Pandas, NumPy, Matplotlib, Seaborn 등의 라이브러리를 사용하여 데이터를 분석하고 시각화할 수 있습니다.\\n- **머신러닝 및 인공지능**: Scikit-learn, TensorFlow, PyTorch 등의 라이브러리와 프레임워크를 사용하여 머신러닝 모델을 개발하고 훈련시킬 수 있습니다.\\n\\n### 결론:\\n파이썬은 그 간결한 문법과 풍부한 라이브러리 덕분에 초보자부터 전문가까지 폭넓게 사랑받는 프로그래밍 언어입니다.'}, {'role': 'human', 'content': '제가 처음에 뭐라고 했죠?'}, {'role': 'ai', 'content': ''}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sw3ELCg6Rpsk"
      },
      "source": [
        "### **윈도 대화 버퍼**\n",
        "\n",
        "- 대화가 늘어남에 따라 입력 **프롬프트의 크기도 커져 최대 토큰 개수를 초과할 수 있다**.\n",
        "- 문맥 윈도 크기를 최소화하는  방법은 전체 채팅 기록을 사용하지 않고 **마지막 k개의 대화만 사용**하는 것이다.\n",
        "- llangchain_community : **ChatMessageHistory**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G0DRT7kjRtiC",
        "outputId": "3b900fd2-f5c6-4b01-e70f-1de878424059"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_context: n_batch is less than GGML_KQ_MASK_PAD - increasing to 64\n",
            "llama_context: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ 응답 1: 1 + 1 = 2.\n",
            "✅ 응답 2: 3 + 3 = 6.\n",
            "\n",
            "✅ 대화 기록: 4개 메시지\n",
            "✅ 대화 기록: [HumanMessage(content='Hi! My name is Maarten and I am 33 years old. What is 1 + 1?', additional_kwargs={}, response_metadata={}), AIMessage(content='1 + 1 = 2.', additional_kwargs={}, response_metadata={}), HumanMessage(content='What is 3 + 3?', additional_kwargs={}, response_metadata={}), AIMessage(content='3 + 3 = 6.', additional_kwargs={}, response_metadata={})]\n"
          ]
        }
      ],
      "source": [
        "from langchain_community.llms import LlamaCpp\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_community.chat_message_histories import ChatMessageHistory\n",
        "\n",
        "# 모델 초기화\n",
        "llm = LlamaCpp(\n",
        "    model_path=\"Konan-LLM-OND-Q4_K_M.gguf\",\n",
        "    n_gpu_layers=-1,\n",
        "    max_tokens=500,\n",
        "    n_ctx=4096,\n",
        "    seed=42,\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "\n",
        "# 기본 프롬프트 템플릿\n",
        "template = \"\"\"<|user|>이전 대화:\n",
        "{chat_history}\n",
        "\n",
        "현재 질문: {input_prompt}<|end|>\n",
        "<|assistant|>\"\"\"\n",
        "\n",
        "# 프롬프트와 체인 구성\n",
        "prompt = PromptTemplate(\n",
        "    template=template,\n",
        "    input_variables=[\"input_prompt\", \"chat_history\"]\n",
        ")\n",
        "\n",
        "# 체인 구성\n",
        "chain = prompt | llm | StrOutputParser()\n",
        "# chain = (\n",
        "#    PromptTemplate.from_template(\n",
        "#        \"<|user|>이전 대화:\\n{chat_history}\\n\\n현재 질문: {input_prompt}<|end|>\\n<|assistant|>\"\n",
        "#    )\n",
        "#    | llm\n",
        "#    | StrOutputParser()\n",
        "#)\n",
        "\n",
        "# 세션 저장소 (k 값 포함)\n",
        "sessions = {}\n",
        "\n",
        "def chat(user_input: str, session_id: str = \"default\", k: int = 2) -> str:\n",
        "    \"\"\"윈도우 메모리 대화 함수\"\"\"\n",
        "    if session_id not in sessions:\n",
        "        sessions[session_id] = {'history': ChatMessageHistory(), 'k': k}\n",
        "\n",
        "    session = sessions[session_id]\n",
        "    history = session['history']\n",
        "    window_size = session['k']\n",
        "\n",
        "    # 최근 k턴만 사용\n",
        "    recent = history.messages[-(window_size * 2):] if history.messages else []\n",
        "    chat_history = \"\\n\".join(\n",
        "        f\"{'사용자' if m.type == 'human' else 'AI'}: {m.content}\"\n",
        "        for m in recent\n",
        "    ) or \"대화 기록 없음\"\n",
        "\n",
        "    response = chain.invoke({\n",
        "        \"input_prompt\": user_input,\n",
        "        \"chat_history\": chat_history\n",
        "    })\n",
        "\n",
        "    history.add_user_message(user_input)\n",
        "    history.add_ai_message(response)\n",
        "\n",
        "    return response\n",
        "\n",
        "# 사용\n",
        "response1 = chat(\"Hi! My name is Maarten and I am 33 years old. What is 1 + 1?\", k=2)\n",
        "response2 = chat(\"What is 3 + 3?\")\n",
        "history = sessions['default']['history'].messages\n",
        "\n",
        "print(f\"✅ 응답 1: {response1}\")\n",
        "print(f\"✅ 응답 2: {response2}\")\n",
        "print(f\"\\n✅ 대화 기록: {len(history)}개 메시지\")\n",
        "print(f\"✅ 대화 기록: {history}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nvSLfKWpR5h5",
        "outputId": "c3482a82-9c8a-4081-f829-34acf6255b7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ 응답 : Your name is Maarten.\n"
          ]
        }
      ],
      "source": [
        "# 이름을 기억하는고 있는지 확인합니다.\n",
        "response = chat(\"What is my name?\")\n",
        "print(f\"✅ 응답 : {response}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YW7qEyctcqeJ",
        "outputId": "79f29d5a-8810-4e50-8f8b-c2319b4a4ee4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ 응답 :  It seems like there's no way for me to know your actual age. I only have the information we've exchanged so far, which does not include your age.\n",
            "\n",
            "If you have any other questions or need further assistance, feel free to ask!\n"
          ]
        }
      ],
      "source": [
        "# 나이를 기억하는고 있는지 확인합니다.\n",
        "response = chat(\"What is my age?\")\n",
        "print(f\"✅ 응답 : {response}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSb5OnANMhu2"
      },
      "source": [
        "### **대화 요약**\n",
        "\n",
        "- 문맥 윈도우가 큰 LLM을 사용하여 대화를 기억하는 기능을 어느정도 해결할 수 있지만 이 방법은 토큰을 생성하기 전에 대화 기록에 담긴 토큰을 처리해야 하므로 계산 시간이 늘어난다.\n",
        "- **대화 기록을 입력으로 사용**하여 **다른 LLM에게 간결한 요약을 생성하라고 요청**한다.\n",
        "- **LLM에게 질문할 때마다 두 번의 요청(호출)이 발생**한다.\n",
        "    - **사용자 프롬프트**\n",
        "    - **요약 프롬프트**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "lWHZlJUbwpqE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d663bb44-035c-40d3-e4ab-6f0fcd18fb4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_context: n_batch is less than GGML_KQ_MASK_PAD - increasing to 64\n",
            "llama_context: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "📝 Summary Memory 모드\n",
            "======================================================================\n",
            "✅ 응답 1:  Hi, Maarten! The answer to 1 + 1 is **2**. 😊\n",
            "✅ 응답 2:  Your name is Maarten.\n",
            "✅ 요약: \n",
            "✅ 대화 기록: 4개 메시지\n"
          ]
        }
      ],
      "source": [
        "from langchain_community.llms import LlamaCpp\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_community.chat_message_histories import ChatMessageHistory\n",
        "\n",
        "# 모델 초기화\n",
        "llm = LlamaCpp(\n",
        "    model_path=\"Konan-LLM-OND-Q4_K_M.gguf\",\n",
        "    n_gpu_layers=-1,\n",
        "    max_tokens=500,\n",
        "    n_ctx=4096,\n",
        "    seed=42,\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "# 프롬프트 템플릿\n",
        "template = \"\"\"<|user|>이전 대화 요약:\n",
        "{summary}\n",
        "\n",
        "최근 대화:\n",
        "{chat_history}\n",
        "\n",
        "현재 질문: {input_prompt}<|end|>\n",
        "<|assistant|>\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    template=template,\n",
        "    input_variables=[\"input_prompt\", \"summary\", \"chat_history\"]\n",
        ")\n",
        "\n",
        "# 요약 템플릿\n",
        "summary_prompt_template = \"\"\"<|user|>Summarize the conversations and update with the new lines.\n",
        "\n",
        "Current summary:\n",
        "{summary}\n",
        "\n",
        "new lines of conversation:\n",
        "{new_lines}\n",
        "\n",
        "New summary:<|end|>\n",
        "<|assistant|>\"\"\"\n",
        "\n",
        "summary_prompt = PromptTemplate(\n",
        "    input_variables=[\"new_lines\", \"summary\"],\n",
        "    template=summary_prompt_template\n",
        ")\n",
        "\n",
        "# 체인 구성\n",
        "chain = prompt | llm | StrOutputParser()\n",
        "summary_chain = summary_prompt | llm | StrOutputParser()\n",
        "\n",
        "# 세션 저장소\n",
        "sessions = {}\n",
        "\n",
        "def chat(user_input: str, session_id: str = \"default\", k: int = None) -> str:\n",
        "    \"\"\"\n",
        "    대화 함수\n",
        "    - k=None: Summary Memory 사용 (max_history=4)\n",
        "    - k=숫자: Window Memory 사용\n",
        "    \"\"\"\n",
        "    if session_id not in sessions:\n",
        "        sessions[session_id] = {\n",
        "            'history': ChatMessageHistory(),\n",
        "            'summary': \"\",\n",
        "            'k': k\n",
        "        }\n",
        "\n",
        "    session = sessions[session_id]\n",
        "    history = session['history']\n",
        "    window_size = k if k is not None else session['k']\n",
        "\n",
        "    # Window Memory 모드\n",
        "    if window_size is not None:\n",
        "        recent = history.messages[-(window_size * 2):] if history.messages else []\n",
        "        chat_history = \"\\n\".join(\n",
        "            f\"{'사용자' if m.type == 'human' else 'AI'}: {m.content}\"\n",
        "            for m in recent\n",
        "        ) or \"대화 기록 없음\"\n",
        "        summary = \"없음\"\n",
        "\n",
        "    # Summary Memory 모드\n",
        "    else:\n",
        "        max_history = 4\n",
        "\n",
        "        # 메시지가 많으면 요약\n",
        "        if len(history.messages) > max_history:\n",
        "            messages_to_summarize = history.messages[:-max_history]\n",
        "            new_lines = \"\\n\".join(\n",
        "                f\"{'사용자' if m.type == 'human' else 'AI'}: {m.content}\"\n",
        "                for m in messages_to_summarize\n",
        "            )\n",
        "\n",
        "            session['summary'] = summary_chain.invoke({\n",
        "                \"summary\": session['summary'] if session['summary'] else \"없음\",\n",
        "                \"new_lines\": new_lines\n",
        "            })\n",
        "\n",
        "            recent = history.messages[-max_history:]\n",
        "            history.clear()\n",
        "            for msg in recent:\n",
        "                if msg.type == \"human\":\n",
        "                    history.add_user_message(msg.content)\n",
        "                else:\n",
        "                    history.add_ai_message(msg.content)\n",
        "\n",
        "        chat_history = \"\\n\".join(\n",
        "            f\"{'사용자' if m.type == 'human' else 'AI'}: {m.content}\"\n",
        "            for m in history.messages\n",
        "        ) or \"대화 기록 없음\"\n",
        "\n",
        "        summary = session['summary'] if session['summary'] else \"없음\"\n",
        "\n",
        "    # 응답 생성\n",
        "    response = chain.invoke({\n",
        "        \"input_prompt\": user_input,\n",
        "        \"summary\": summary,\n",
        "        \"chat_history\": chat_history\n",
        "    })\n",
        "\n",
        "    history.add_user_message(user_input)\n",
        "    history.add_ai_message(response)\n",
        "\n",
        "    return response\n",
        "\n",
        "# ==========================================\n",
        "# 사용 예시\n",
        "# ==========================================\n",
        "\n",
        "# Summary Memory 모드 (k=None)\n",
        "print(\"=\"*70)\n",
        "print(\"📝 Summary Memory 모드\")\n",
        "print(\"=\"*70)\n",
        "response1 = chat(\"Hi! My name is Maarten. What is 1 + 1?\", session_id=\"user1\")\n",
        "response2 = chat(\"What is my name?\", session_id=\"user1\")\n",
        "\n",
        "print(f\"✅ 응답 1: {response1}\")\n",
        "print(f\"✅ 응답 2: {response2}\")\n",
        "print(f\"✅ 요약: {sessions['user1']['summary']}\")\n",
        "print(f\"✅ 대화 기록: {len(sessions['user1']['history'].messages)}개 메시지\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 지금까지 내용이 요약되어 있는지 확인합니다.\n",
        "response = chat(\"What was the first question I asked?\", session_id=\"user1\")\n",
        "print(f\"✅ 응답 : {response}\")\n",
        "print(f\"✅ 요약 : {sessions['user1']['summary']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jK2epxcem6mt",
        "outputId": "99660661-e8d2-4c53-dd62-82d939c86c4f"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ 응답 : \n",
            "✅ 요약 : \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 여러 대화를 진행하여 요약 트리거\n",
        "conversations = [\n",
        "    \"Hi! My name is Maarten.\",\n",
        "    \"I am 33 years old.\",\n",
        "    \"I live in Amsterdam.\",\n",
        "    \"What is 1 + 1?\",\n",
        "    \"What is my name?\",  # 이 시점에서 요약 확인 가능\n",
        "]\n",
        "\n",
        "for i, conv in enumerate(conversations, 1):\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"대화 {i}: {conv}\")\n",
        "    print(f\"{'='*70}\")\n",
        "\n",
        "    response = chat(conv, session_id=\"user_001\")\n",
        "    print(f\"🤖 응답: {response[:150]}...\")\n",
        "\n",
        "    # 현재 세션 상태\n",
        "    session = sessions['user_001']\n",
        "    print(f\"\\n📊 현재 상태:\")\n",
        "    print(f\"   - 현재 메시지 수: {len(session['history'].messages)}개\")\n",
        "    print(f\"   - 요약 상태: {'있음' if session['summary'] else '없음'}\")\n",
        "    if session['summary']:\n",
        "        print(f\"   - 요약 내용: {session['summary'][:100]}...\")\n",
        "\n",
        "# ==========================================\n",
        "# 최종 확인\n",
        "# ==========================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"📄 최종 세션 상태\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "session = sessions['user_001']\n",
        "history = session['history'].messages\n",
        "summary = session['summary']\n",
        "\n",
        "print(f\"\\n✅ 대화 기록: {len(history)}개 메시지\")\n",
        "for i, msg in enumerate(history, 1):\n",
        "    print(f\"   [{i}] {msg.type}: {msg.content[:60]}...\")\n",
        "\n",
        "print(f\"\\n✅ 요약:\")\n",
        "print(f\"   {summary if summary else '(요약 없음)'}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IZV5s5T_n8Hp",
        "outputId": "dad53ecc-3feb-407a-f688-52911db66e0c"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "대화 1: Hi! My name is Maarten.\n",
            "======================================================================\n",
            "🤖 응답: Hello, Maarten! It's nice to meet you. How can I assist you today?...\n",
            "\n",
            "📊 현재 상태:\n",
            "   - 현재 메시지 수: 2개\n",
            "   - 요약 상태: 없음\n",
            "\n",
            "======================================================================\n",
            "대화 2: I am 33 years old.\n",
            "======================================================================\n",
            "🤖 응답: I'm glad to hear that you've already taken some time for yourself. At 33, you're at a stage in life where personal growth and development are particul...\n",
            "\n",
            "📊 현재 상태:\n",
            "   - 현재 메시지 수: 4개\n",
            "   - 요약 상태: 없음\n",
            "\n",
            "======================================================================\n",
            "대화 3: I live in Amsterdam.\n",
            "======================================================================\n",
            "🤖 응답:  It sounds like you're in a vibrant and culturally rich city. Amsterdam is known for its iconic landmarks, such as the Rijksmuseum or the Van Gogh Mus...\n",
            "\n",
            "📊 현재 상태:\n",
            "   - 현재 메시지 수: 6개\n",
            "   - 요약 상태: 없음\n",
            "\n",
            "======================================================================\n",
            "대화 4: What is 1 + 1?\n",
            "======================================================================\n",
            "🤖 응답: 1 + 1 is equal to 2....\n",
            "\n",
            "📊 현재 상태:\n",
            "   - 현재 메시지 수: 6개\n",
            "   - 요약 상태: 없음\n",
            "\n",
            "======================================================================\n",
            "대화 5: What is my name?\n",
            "======================================================================\n",
            "🤖 응답:  I don't have access to your personal information. However, I can say that you need to check the name field in your account settings or profile inform...\n",
            "\n",
            "📊 현재 상태:\n",
            "   - 현재 메시지 수: 6개\n",
            "   - 요약 상태: 없음\n",
            "\n",
            "======================================================================\n",
            "📄 최종 세션 상태\n",
            "======================================================================\n",
            "\n",
            "✅ 대화 기록: 6개 메시지\n",
            "   [1] human: I live in Amsterdam....\n",
            "   [2] ai:  It sounds like you're in a vibrant and culturally rich city...\n",
            "   [3] human: What is 1 + 1?...\n",
            "   [4] ai: 1 + 1 is equal to 2....\n",
            "   [5] human: What is my name?...\n",
            "   [6] ai:  I don't have access to your personal information. However, ...\n",
            "\n",
            "✅ 요약:\n",
            "   (요약 없음)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "ndsVUZZCojv8"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BG5sJa1qvS4N"
      },
      "source": [
        "# **에이전트**\n",
        "LLM 시스템 구축하기"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **에이전트**(**Agent**) : 언어 모델을 사용해 어떤 행동을 어떤 순서로 수행힐지 결정하는 시스템\n",
        "- **에이전트 추가 핵심 구성 요소**\n",
        "    - 에이전트가 스스로 수행할 수 없는 작업을 위해 사용할 도구(tool)\n",
        "    - 수행할 행동 또는 사용할 도구를 계획하는 에이전트 유형(agent type)\n",
        "\n",
        "- 에이전트는 목표를 달성하기 위한 로드맵을 만들고 스스로 수정하는 등의 고차원적 행동을 할 수 있습니다.\n",
        "- 도구를 사용하기 위해 실세계와 상호작용을 할 수 있다. (--> 다양한 작업을 할 수 있다.)"
      ],
      "metadata": {
        "id": "BaHDNJyRotpP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **ReAct**: 추론과 행동을 연결하는 강력한 프레임워크\n",
        "    - Reasoning(추론) + Acting(행동)\n",
        "- **ReAct의 사이클 단계**\n",
        "    - 사고 : 생각하고 추론하는 과정 (Thought), 해야할 일\n",
        "    - 행동 : 실제 행동을 취하는 과정 (Action), 할 일\n",
        "    - 관측 : 해동의 결과\n",
        "    - 추론과 행동을 번갈아가며 수행하여 문제 해결\n",
        "\n"
      ],
      "metadata": {
        "id": "VO-rZUGHqI7_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **랭체인의 ReACT**"
      ],
      "metadata": {
        "id": "r8XYxpIvrSJw"
      }
    }
  ]
}