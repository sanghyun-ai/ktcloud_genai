{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanghyun-ai/ktcloud_genai/blob/main/%EC%8B%A4%EC%8A%B5%EC%BD%94%EB%93%9C/401_LLM_%EA%B3%A0%EA%B8%89_%ED%85%8D%EC%8A%A4%ED%8A%B8_%EC%83%9D%EC%84%B1_%EA%B8%B0%EC%88%A0%EA%B3%BC_%EB%8F%84%EA%B5%AC_langchain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ê³ ê¸‰ í…ìŠ¤íŠ¸ ìƒì„± ê¸°ìˆ ê³¼ ë„êµ¬**\n"
      ],
      "metadata": {
        "id": "3g3yQqxBnXOp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "\n",
        "- ğŸ’¡ **NOTE**\n",
        "    - ì´ ë…¸íŠ¸ë¶ì˜ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ë ¤ë©´ GPUë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤. êµ¬ê¸€ ì½”ë©ì—ì„œëŠ” **ëŸ°íƒ€ì„ > ëŸ°íƒ€ì„ ìœ í˜• ë³€ê²½ > í•˜ë“œì›¨ì–´ ê°€ì†ê¸° > T4 GPU**ë¥¼ ì„ íƒí•˜ì„¸ìš”.\n",
        "\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "DLl8lovjBLgP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **ëª¨ë¸ì„ ë¯¸ì„¸íŠœë‹í•˜ì§€ ì•Šê³  LLMì—ì„œ ì–»ì€ ì¶œë ¥ê³¼ ê²½í—˜ì„ ë” í–¥ìƒì‹œí‚¤ë ¤ë©´ ì–´ë–»ê²Œ í•´ì•¼ í• ê¹Œ?**\n",
        "\n",
        "- **ìƒì„± í…ìŠ¤íŠ¸ì˜ í’ˆì§ˆì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•œ ëª‡ ê°€ì§€ ê¸°ë²•ê³¼ ê°œë…**\n",
        "    - **ëª¨ë¸ I/O** : LLMì„ ë¡œë“œí•˜ê³  ì‹¤í–‰í•˜ê¸°\n",
        "    - **ì²´ì¸**(Chain) : ë°©ë²•ê³¼ ë„êµ¬ë¥¼ ì—°ê²°í•˜ê¸°\n",
        "    - **ë©”ëª¨ë¦¬** : LLMì´ ê¸°ì–µí•˜ë„ë¡ ë•ê¸°\n",
        "    - **ì—ì´ì „íŠ¸**(Agent) : ë³µì¡í•œ ë™ì‘ì„ ì™¸ë¶€ ë„êµ¬ì™€ ì—°ê²°í•˜ê¸°\n",
        "\n"
      ],
      "metadata": {
        "id": "rg0qLUE-jjoi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://drive.google.com/uc?export=view&id=1LEVm8upycOb0PS454PVTH8tZtg7n0KqD\" width=\"80%\">\n"
      ],
      "metadata": {
        "id": "u8QMoLhUsXSO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "bUWBl_VubQNJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **LangChain ì†Œê°œ**"
      ],
      "metadata": {
        "id": "eNx4LxbLuvex"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **LangChainì´ë€?** :\n",
        "- LangChainì€ **LLMì„ í™œìš©í•œ ì• í”Œë¦¬ì¼€ì´ì…˜(ì—ì´ì „íŠ¸, RAG ë“±)ì„ ì‰½ê²Œ ë§Œë“¤ ìˆ˜ ìˆë„ë¡ ë„ì™€ì£¼ëŠ” <mark>ì˜¤í”ˆì†ŒìŠ¤ í”„ë ˆì„ì›Œí¬</mark>**\n",
        "    - â€œ**AI íŒŒì´í”„ë¼ì¸**â€ì„ êµ¬ì¶•í•˜ë„ë¡ ë•ëŠ” í”„ë ˆì„ì›Œí¬\n",
        "    - ì¶”ìƒí™”ë¥¼ í†µí•´ LLM ì‘ì—…ì„ ë‹¨ìˆœí™”í•´ ì£¼ëŠ” í”„ë ˆì„ì›Œí¬ (--> ìµœê·¼ ëª¨ë“ˆí™” ì§„í–‰)\n",
        "    - LLMì„ ë‹¨ìˆœí•œ ì±—ë´‡ì´ ì•„ë‹ˆë¼, ë°ì´í„°ë² ì´ìŠ¤Â·APIÂ·ë¬¸ì„œÂ·ë„êµ¬ë“¤ê³¼ ì—°ë™í•´ ë³µì¡í•œ ì‘ì—…ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆê²Œ í•´ì£¼ëŠ” ì¤‘ê°„ í”Œë«í¼\n",
        "- **ê°œë°œì**: Harrison Chase (2022ë…„ ë°œí‘œ)\n",
        "- **ì£¼ìš” ëª©ì ** : LLMì„ ì´ìš©í•´ ì‹¤ì œ â€œì—”í„°í”„ë¼ì´ì¦ˆê¸‰â€ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ì‰½ê²Œ êµ¬ì¶•\n",
        "- **ì£¼ìš” ì–¸ì–´** : \tPython, JavaScript/TypeScript\n",
        "- **ê³µì‹ ë¬¸ì„œ** : https://www.langchain.com\n",
        "    - https://github.com/langchain-ai/langchain\n",
        "    - https://docs.langchain.com/oss/python/langchain/overview\n",
        "- **í•œêµ­ì–´ ë¬¸ì„œ** : https://github.com/teddylee777/langchain-kr\n"
      ],
      "metadata": {
        "id": "oz9w1Co6o82Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **LangChain í•µì‹¬ ì „ëµ**\n",
        "- ì´ëŸ¬í•œ ê¸°ìˆ ì€ ê·¸ ìì²´ë¡œ ê°•ë ¥í•˜ì§€ë§Œ ë…ë¦½ì ìœ¼ë¡œ ì‚¬ìš©ë  ë•ŒëŠ” ì§„ì •í•œ ê°€ì¹˜ë¥¼ ë°œíœ˜í•˜ì§€ ëª»í•¨\n",
        "- ì´ëŸ° ê¸°ë²•ì„ **ëª¨ë‘ ì—°ê²°í•´ì•¼ ë†€ë¼ìš´ ì„±ëŠ¥ì˜ LLM ê¸°ë°˜ ì‹œìŠ¤í…œì„ ì–»ì„ ìˆ˜ ìˆë‹¤**.\n",
        "- ì´ëŸ° ê¸°ìˆ ì´ ìµœê³ ì¡°ì— ë„ë‹¬í•  ë•Œ LLMì´ ì§„ì •í•œ ë¹›ì„ ë°œí•œë‹¤."
      ],
      "metadata": {
        "id": "8Kr7ojGGbTBQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **LangChain í•µì‹¬ êµ¬ì„± ìš”ì†Œ**\n",
        "\n",
        "| êµ¬ì„± ìš”ì†Œ | ì„¤ëª… | ì˜ˆì‹œ |\n",
        "| ---  | --- | --- |\n",
        "| **LLMs** | GPT, Claude, Gemini ë“±ê³¼ ê°™ì€ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸  | `OpenAI`, `HuggingFace`, `Ollama`, `VertexAI` |\n",
        "| **Prompt Templates** | ì¼ê´€ì„± ìˆëŠ” í”„ë¡¬í”„íŠ¸ êµ¬ì¡°ë¥¼ ê´€ë¦¬ | â€œ{question}ì— ëŒ€í•´ ì „ë¬¸ê°€ì²˜ëŸ¼ ë‹µë³€í•´ì¤˜â€ |\n",
        "| **Chains** | ì—¬ëŸ¬ LLM í˜¸ì¶œ ë° ì—°ì‚°ì„ ë‹¨ê³„ì ìœ¼ë¡œ ì—°ê²° | ì§ˆë¬¸ â†’ ìš”ì•½ â†’  ë°ì´í„°ë² ì´ìŠ¤ ê²€ìƒ‰ â†’ ê²°ê³¼ ìƒì„± |\n",
        "| **Memory** | ëŒ€í™” ê¸°ë¡ì„ ì €ì¥í•˜ì—¬ ë¬¸ë§¥ì„ ìœ ì§€ | ì±—ë´‡ì´ ì´ì „ ëŒ€í™”  ê¸°ì–µí•˜ê¸° |\n",
        "| **Retrievers / VectorStores** | ë¬¸ì„œ ê²€ìƒ‰ ë° ë²¡í„° ê¸°ë°˜ ì„ë² ë”© ê²€ìƒ‰ | `FAISS`, `Chroma`, `Pinecone` |\n",
        "| **Agents & Tools** | LLMì´ â€œë„êµ¬(ê³„ì‚°ê¸°, API, ê²€ìƒ‰ì—”ì§„ ë“±)â€ë¥¼ ìŠ¤ìŠ¤ë¡œ í˜¸ì¶œí•˜ì—¬ ë¬¸ì œ í•´ê²° | â€œë‚ ì”¨ ì•Œë ¤ì¤˜â€ â†’ LLMì´ OpenWeather API í˜¸ì¶œ |\n",
        "| **Callbacks** | ì‹¤í–‰ ê³¼ì •ì„ ì¶”ì í•˜ê±°ë‚˜ ì‹œê°í™” | Streamlit, LangSmith ì—°ë™ |\n"
      ],
      "metadata": {
        "id": "yWLeVsOWyJPg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **LangChainì˜ ë™ì‘ êµ¬ì¡°**\n",
        "**Prompt â†’ Model â†’ Output**ì˜ ë‹¨ìˆœí•œ íë¦„ì„ í™•ì¥í•˜ì—¬ **ë‹¤ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ì„ êµ¬ì„±**\n",
        "\n",
        "`User â†’ PromptTemplate â†’ LLMChain â†’ (Memory + Retriever + Tools) â†’ Output`\n",
        "\n",
        "- **ë™ì‘ ì˜ˆì‹œ**:\n",
        "1. ì‚¬ìš©ìì˜ ì§ˆë¬¸ ìˆ˜ì§‘\n",
        "2. í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì— ì§ˆë¬¸ ì‚½ì…\n",
        "3. ë²¡í„°ìŠ¤í† ì–´ì—ì„œ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰\n",
        "4. ê²°ê³¼ë¥¼ ì •ë¦¬í•˜ì—¬ LLMì—ê²Œ ì „ë‹¬\n",
        "5. LLMì´ ìµœì¢… ë‹µë³€ ìƒì„±"
      ],
      "metadata": {
        "id": "yEVILOgKzgSM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **LangChainì˜ ì£¼ìš” ì‘ìš© ë¶„ì•¼**\n",
        "\n",
        "|ë¶„ì•¼| ì„¤ëª…| ì˜ˆì‹œ|\n",
        "| ---| ---| ---|\n",
        "| **RAG (Retrieval-Augmented Generation)** | ì™¸ë¶€ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•´ ë‹µë³€ ì •í™•ë„ í–¥ìƒ   | ë…¼ë¬¸ ê¸°ë°˜ Q&A, ë‚´ë¶€ ë¬¸ì„œ ìš”ì•½  |\n",
        "| **LLM Agents**  | LLMì´ ìŠ¤ìŠ¤ë¡œ íˆ´ì„ ì„ íƒí•˜ì—¬ ì‹¤í–‰    | ChatGPT Plugins, AutoGPT   |\n",
        "| **Workflow Automation** | ì—¬ëŸ¬ LLM í˜¸ì¶œì„ íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ ìë™í™” | ì´ë©”ì¼ ìš”ì•½ â†’ ì¼ì • ë“±ë¡ â†’ ë³´ê³ ì„œ ìƒì„±    |\n",
        "| **Conversational Chatbot** | ê¸°ì–µ ê¸°ë°˜ ëŒ€í™”í˜• ì‹œìŠ¤í…œ êµ¬ì¶• | ê³ ê° ìƒë‹´ ì±—ë´‡, êµìœ¡ìš© íŠœí„° |\n",
        "| **ë°ì´í„° ë¶„ì„ ìë™í™”** | ìì—°ì–´ë¡œ ë°ì´í„° íƒìƒ‰  | â€œCSV ë°ì´í„° ìš”ì•½í•´ì¤˜â€ â†’ Pandas ì‹¤í–‰ |\n"
      ],
      "metadata": {
        "id": "q4AW-HjP0lYq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **LangChainì˜ ë°œì „ ì´ë ¥**\n",
        "\n",
        "| ì‹œê¸° | ì£¼ìš” ë‚´ìš© |\n",
        "| --- | --- |\n",
        "| **2022ë…„ ì´ˆ**  | Harrison Chaseê°€ LangChain ê³µê°œ (LLM ì—°ê²°ì„ ìœ„í•œ ì²´ì¸ ê¸°ë°˜ í”„ë ˆì„ì›Œí¬) |\n",
        "| **2023ë…„ ì¤‘ë°˜** | VectorStore í†µí•© ë° Memory ê¸°ëŠ¥ ê°•í™” â†’ RAG êµ¬ì¡° ëŒ€ì¤‘í™” |\n",
        "| **2023ë…„ ë§**  | LangSmith/LangServe ì¶œì‹œë¡œ ì¶”ì Â·ë””ë²„ê¹…Â·API ë°°í¬ ê°€ëŠ¥ |\n",
        "| **2024ë…„ ì´í›„** | LangGraph(LLM ê¸°ë°˜ ìƒíƒœ ë¨¸ì‹ ) ë“±ì¥ â€” ë³µì¡í•œ ì›Œí¬í”Œë¡œìš° êµ¬í˜„ ê°•í™” |\n"
      ],
      "metadata": {
        "id": "elBKA1AU1WRt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### **LangChain vs ë‹¤ë¥¸ í”„ë ˆì„ì›Œí¬**:\n",
        "- **LlamaIndex** : ë¬¸ì„œ ì¸ë±ì‹± ë° ê²€ìƒ‰ ê°•í™”\n",
        "    - LlamaIndex : https://www.llamaindex.ai/\n",
        "- **Haystack**: RAG ì¤‘ì‹¬ì˜ ê²€ìƒ‰ì—”ì§„\n",
        "    - https://github.com/deepset-ai/haystack\n",
        "- **DSPy**: í”„ë¡¬í”„íŠ¸Â·íŒŒì´í”„ë¼ì¸ì„ ë°ì´í„°ë¡œ ìµœì í™”(â€œì»´íŒŒì¼â€)\n",
        "    - https://github.com/stanfordnlp/dspy\n",
        "    - â€œí”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ì„ ì½”ë“œê°€ ì•„ë‹ˆë¼ ë°ì´í„°ì™€ ì§€í‘œë¡œ ìµœì í™”â€í•˜ëŠ” í”„ë ˆì„ì›Œí¬.\n",
        "    - ì…ë ¥Â·ì¶œë ¥ ì‚¬ì–‘(Signature)ê³¼ ì‘ì€ ê²€ì¦ ë°ì´í„°ì…‹, ì„±ëŠ¥ ì§€í‘œë¥¼ ì£¼ë©´, ë‚´ë¶€ teleprompter/ì»´íŒŒì¼ëŸ¬ê°€ ì ì ˆí•œ í”„ë¡¬í”„íŠ¸Â·ì„¤ì •(ì˜ˆ: few-shot ì˜ˆì‹œ, ë„êµ¬ í˜¸ì¶œ ë°©ì‹ ë“±)ì„ ìë™ íƒìƒ‰í•´ ì¼ê´€ë˜ê²Œ ì¬í˜„ ê°€ëŠ¥í•œ ìµœì  íŒŒì´í”„ë¼ì¸ì„ ë§Œë“¤ì–´\n",
        "\n",
        "\n",
        "| ë¹„êµ í•­ëª©  | **LangChain**  |**LlamaIndex** | **Haystack** |\n",
        "| --- | --- | --- | --- |\n",
        "| ì£¼ ëª©ì    | LLM íŒŒì´í”„ë¼ì¸ êµ¬ì„±   | ë¬¸ì„œ ì¸ë±ì‹± ë° ê²€ìƒ‰ ê°•í™” | RAG ì¤‘ì‹¬ì˜ ê²€ìƒ‰ì—”ì§„ |\n",
        "| êµ¬ì¡° | ëª¨ë“ˆì‹ ì²´ì¸/ì—ì´ì „íŠ¸ ê¸°ë°˜ | Graph êµ¬ì¡° ê¸°ë°˜ | Retriever + Generator êµ¬ì¡° |\n",
        "| í•™ìŠµ ë‚œì´ë„ | ì¤‘ê°„ | ì‰¬ì›€ | ì¤‘ê°„ |\n",
        "| ê°•ì  | ë„êµ¬/ì—ì´ì „íŠ¸ ì—°ë™ | ë¬¸ì„œ ê´€ê³„ ëª¨ë¸ë§  | ëŒ€ê·œëª¨ ë°ì´í„° ì²˜ë¦¬ |\n",
        "| --- | --- | https://www.llamaindex.ai/ | https://github.com/deepset-ai/haystack |\n"
      ],
      "metadata": {
        "id": "FmQVSnwiiuVs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "YsuY48iBx1sb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ëª¨ë¸ I/O**"
      ],
      "metadata": {
        "id": "lwkkxTQq5nw4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜**\n"
      ],
      "metadata": {
        "id": "vJLRwHnMlJF0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "import logging\n",
        "\n",
        "# 1. ì¼ë°˜ì ì¸ Python ê²½ê³ (DeprecationWarning ë“±) ìˆ¨ê¸°ê¸°\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "# 2. Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œê·¸ ìˆ˜ì¤€ ì¡°ì ˆ (Warning ì´í•˜ëŠ” ìˆ¨ê¸°ê¸°)\n",
        "logging.getLogger(\"transformers\").setLevel(logging.ERROR)"
      ],
      "metadata": {
        "id": "i28AXkBQfyp0"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ê¹ƒí—ˆë¸Œì—ì„œ ìœ„ì ¯ ìƒíƒœ ì˜¤ë¥˜ë¥¼ í”¼í•˜ê¸° ìœ„í•´ ì§„í–‰ í‘œì‹œì¤„ì„ ë‚˜íƒ€ë‚´ì§€ ì•Šë„ë¡ ì„¤ì •í•©ë‹ˆë‹¤.\n",
        "import os\n",
        "import tqdm\n",
        "from transformers.utils import logging\n",
        "\n",
        "# tqdm ë¹„í™œì„±í™”\n",
        "tqdm.tqdm = lambda *args, **kwargs: iter([])\n",
        "tqdm.auto.tqdm = lambda *args, **kwargs: iter([])\n",
        "tqdm.notebook.tqdm = lambda *args, **kwargs: iter([])\n",
        "os.environ[\"DISABLE_TQDM\"] = \"1\"\n",
        "\n",
        "logging.disable_progress_bar()"
      ],
      "metadata": {
        "id": "ILBN7AMQ0SfM"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Phi-3 ëª¨ë¸ê³¼ í˜¸í™˜ì„± ë•Œë¬¸ì— transformers 4.48.3 ë²„ì „ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
        "!pip install transformers==4.48.3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uzQRZTt_c4Ug",
        "outputId": "28b4904d-4b78-4cfa-e8e2-d74ce4165a48"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers==4.48.3 in /usr/local/lib/python3.12/dist-packages (4.48.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers==4.48.3) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.48.3) (0.35.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.48.3) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.48.3) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.48.3) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.48.3) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers==4.48.3) (2.32.5)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.12/dist-packages (from transformers==4.48.3) (0.21.4)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.48.3) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers==4.48.3) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers==4.48.3) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers==4.48.3) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers==4.48.3) (1.1.10)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.48.3) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.48.3) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.48.3) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.48.3) (2025.10.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Txh47zAxCAYd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "8d55cada-63f1-48fc-b5d8-03a5e5c814e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (1.0.2)\n",
            "Requirement already satisfied: langchain-openai in /usr/local/lib/python3.12/dist-packages (1.0.1)\n",
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.12/dist-packages (0.4)\n",
            "Requirement already satisfied: duckduckgo-search in /usr/local/lib/python3.12/dist-packages (8.1.1)\n",
            "Requirement already satisfied: ddgs in /usr/local/lib/python3.12/dist-packages (9.6.1)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langchain) (1.0.0)\n",
            "Requirement already satisfied: langgraph<1.1.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langchain) (1.0.1)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.11.10)\n",
            "Requirement already satisfied: openai<3.0.0,>=1.109.1 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (1.109.1)\n",
            "Requirement already satisfied: tiktoken<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (0.12.0)\n",
            "Requirement already satisfied: langchain-classic<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (1.0.0)\n",
            "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.44)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.32.5 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.32.5)\n",
            "Requirement already satisfied: PyYAML<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (6.0.3)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (3.13.0)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (8.5.0)\n",
            "Requirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.11.0)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.1.125 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.37)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.3)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: click>=8.1.8 in /usr/local/lib/python3.12/dist-packages (from duckduckgo-search) (8.3.0)\n",
            "Requirement already satisfied: primp>=0.15.0 in /usr/local/lib/python3.12/dist-packages (from duckduckgo-search) (0.15.0)\n",
            "Requirement already satisfied: lxml>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from duckduckgo-search) (6.0.2)\n",
            "Requirement already satisfied: httpx>=0.28.1 in /usr/local/lib/python3.12/dist-packages (from httpx[brotli,http2,socks]>=0.28.1->ddgs) (0.28.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.22.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.28.1->httpx[brotli,http2,socks]>=0.28.1->ddgs) (4.11.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.28.1->httpx[brotli,http2,socks]>=0.28.1->ddgs) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.28.1->httpx[brotli,http2,socks]>=0.28.1->ddgs) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.28.1->httpx[brotli,http2,socks]>=0.28.1->ddgs) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.28.1->httpx[brotli,http2,socks]>=0.28.1->ddgs) (0.16.0)\n",
            "Requirement already satisfied: brotli in /usr/local/lib/python3.12/dist-packages (from httpx[brotli,http2,socks]>=0.28.1->ddgs) (1.1.0)\n",
            "Requirement already satisfied: h2<5,>=3 in /usr/local/lib/python3.12/dist-packages (from httpx[brotli,http2,socks]>=0.28.1->ddgs) (4.3.0)\n",
            "Requirement already satisfied: socksio==1.* in /usr/local/lib/python3.12/dist-packages (from httpx[brotli,http2,socks]>=0.28.1->ddgs) (1.0.0)\n",
            "Requirement already satisfied: langchain-text-splitters<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langchain-classic<2.0.0,>=1.0.0->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain) (25.0)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain) (4.15.0)\n",
            "Requirement already satisfied: langgraph-checkpoint<4.0.0,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.0->langchain) (3.0.0)\n",
            "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.0->langchain) (1.0.1)\n",
            "Requirement already satisfied: langgraph-sdk<0.3.0,>=0.2.2 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.0->langchain) (0.2.9)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.0->langchain) (3.6.0)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (3.11.3)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (0.25.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (0.11.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (4.67.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.2)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (1.1.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2.5.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain-community) (3.2.4)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1.0.0,>=0.7.0->langchain-openai) (2024.11.6)\n",
            "Requirement already satisfied: hyperframe<7,>=6.1 in /usr/local/lib/python3.12/dist-packages (from h2<5,>=3->httpx[brotli,http2,socks]>=0.28.1->ddgs) (6.1.0)\n",
            "Requirement already satisfied: hpack<5,>=4.1 in /usr/local/lib/python3.12/dist-packages (from h2<5,>=3->httpx[brotli,http2,socks]>=0.28.1->ddgs) (4.1.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.0->langchain) (3.0.0)\n",
            "Requirement already satisfied: ormsgpack>=1.10.0 in /usr/local/lib/python3.12/dist-packages (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.0->langchain) (1.11.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community) (1.1.0)\n"
          ]
        }
      ],
      "source": [
        "# ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜\n",
        "# í‚¤ì›Œë“œë¥¼ ë„£ìœ¼ë©´ ì•Œì•„ì„œ ê²€ìƒ‰í•´ì£¼ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
        "!pip install -U langchain langchain-openai langchain-community duckduckgo-search ddgs\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ë¼ì´ë¸ŒëŸ¬ë¦¬ ë²„ì „ í™•ì¸\n",
        "%pip list | grep -E 'langchain|duckduckgo|ddgs'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Igi53cwrcvPW",
        "outputId": "b7785cbc-6e32-4f0a-bc65-148a64071ce2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ddgs                                     9.6.1\n",
            "duckduckgo_search                        8.1.1\n",
            "langchain                                1.0.2\n",
            "langchain-classic                        1.0.0\n",
            "langchain-community                      0.4\n",
            "langchain-core                           1.0.0\n",
            "langchain-openai                         1.0.1\n",
            "langchain-text-splitters                 1.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **langchain_community** :\n",
        "    - LangChainì˜ í•µì‹¬ íŒŒíŠ¸ë„ˆì‚¬(OpenAI, Google ë“±) ì™¸ì˜ ë‹¤ì–‘í•œ ì»¤ë®¤ë‹ˆí‹° ê¸°ë°˜ LLM, ë²¡í„°DB, ë„êµ¬ ì—°ë™ ê¸°ëŠ¥ë“¤ì„ ëª¨ì•„ë†“ì€ ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
        "- **langchain_openai** :\n",
        "    - LangChainì—ì„œ ChatOpenAI (GPT-4o ë“±)ë‚˜ OpenAIEmbeddingsì²˜ëŸ¼ OpenAI ë° Azure OpenAIì˜ ëª¨ë¸ì„ ì‚¬ìš©í•˜ê¸° ìœ„í•œ ê³µì‹ í†µí•© ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
        "- **duckduckgo-search** :\n",
        "    - API í‚¤ ì—†ì´ë„ íŒŒì´ì¬ ì½”ë“œì—ì„œ DuckDuckGo ì›¹ ê²€ìƒ‰ì„ ì‹¤í–‰í•˜ê³  ê·¸ ê²°ê³¼ë¥¼ ë°›ì•„ì˜¬ ìˆ˜ ìˆê²Œ í•´ì£¼ëŠ” ê°„ë‹¨í•œ ë…ë¦½ ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
        "    - **duckduckgo** : ì‚¬ìš©ìì˜ ê°œì¸ì •ë³´ ë³´í˜¸(í”„ë¼ì´ë²„ì‹œ)ë¥¼ ìµœìš°ì„ ìœ¼ë¡œ í•˜ëŠ” **ê²€ìƒ‰ ì—”ì§„**\n",
        "    - https://duckduckgo.com/\n",
        "    - ì‚¬ìš©ìì˜ ê²€ìƒ‰ ê¸°ë¡, IP ì£¼ì†Œ, í´ë¦­í•œ ë§í¬ ë“± ì–´ë–¤ ê°œì¸ì •ë³´ë„ ì €ì¥í•˜ê±°ë‚˜ ì¶”ì í•˜ì§€ X\n",
        "    - DuckDuckGoëŠ” ì¶”ì ì„ ì•ˆ í•˜ë¯€ë¡œ ëª¨ë“  ì‚¬ìš©ìì—ê²Œ ë™ì¼í•˜ê³  ê°ê´€ì ì¸ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë³´ì—¬ì§\n",
        "    - DuckDuckGoëŠ” ì´ ê³¼ì •ì„ ë§¤ìš° ë‹¨ìˆœí™”(ì‚¬ì‹¤ìƒ ë¬´ë£Œ ê°œë°©)í•´ ì£¼ì—ˆê¸° ë•Œë¬¸ì—, LangChain ì—ì´ì „íŠ¸(Agent)ì—ê²Œ 'ì‹¤ì‹œê°„ ì •ë³´ ê²€ìƒ‰ ëŠ¥ë ¥'ì„ ë¶€ì—¬í•  ë•Œ ê°€ì¥ ê¸°ë³¸ì ì´ê³  ì¸ê¸° ìˆëŠ” ë„êµ¬(Tool)ë¡œ ì‚¬ìš©ë¨"
      ],
      "metadata": {
        "id": "hdHPA98IjU2P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# íŒŒì´ì¬ ë²„ì „ í™•ì¸(3.12)\n",
        "!python --version"
      ],
      "metadata": {
        "id": "6jBSq5FKdprY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33cdc4e8-3b1c-46e1-a51f-8702f4b5ee36"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.12.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CUDA ë²„ì „ í™•ì¸ (12.5)\n",
        "!nvcc --version | grep cuda_"
      ],
      "metadata": {
        "id": "5ZcvURivdpyp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7548cd7-3cb8-4732-f42e-4a6fd76900c0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Build cuda_12.5.r12.5/compiler.34385749_0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- llama-cpp-python\n",
        "    - C++ë¡œ ì‘ì„±ëœ ê³ ì„±ëŠ¥ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM) ì¶”ë¡  ì—”ì§„ì¸ llama.cppì˜ Python ë°”ì¸ë”©(wrapper) ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
        "    - https://github.com/abetlen/llama-cpp-python/releases\n",
        "    - llama_cpp_python-0.3.16-cp312-cp312-linux_x86_64.whl"
      ],
      "metadata": {
        "id": "ggfr7XV9mwGP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "# ì‚¬ìš©í•˜ëŠ” íŒŒì´ì¬ê³¼ CUDA ë²„ì „ì— ë§ëŠ” llama-cpp-python íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜í•˜ì„¸ìš”.\n",
        "!pip install https://github.com/abetlen/llama-cpp-python/releases/download/v0.3.16-cu124/llama_cpp_python-0.3.16-cp312-cp312-linux_x86_64.whl"
      ],
      "metadata": {
        "id": "smrLK_oimz_G"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "0pBa6FKgx39V"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rerbJgwAigbK"
      },
      "source": [
        "## **LLM ë¡œë“œí•˜ê¸°**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **ì¼ë°˜ ëª¨ë¸ ë¡œë“œí•˜ê¸°**"
      ],
      "metadata": {
        "id": "htQHTrHOTlvL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "\n",
        "# ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.\n",
        "model_id = \"microsoft/Phi-3-mini-4k-instruct\"\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=\"cuda\",\n",
        "    torch_dtype=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "print(f'\\nâœ… ì‚¬ìš©ëœ ëª¨ë¸:\\n{model_id}')\n",
        "\n",
        "# íŒŒì´í”„ë¼ì¸ì„ ë§Œë“­ë‹ˆë‹¤.\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    return_full_text=False,\n",
        "    max_new_tokens=500,\n",
        "    do_sample=False,\n",
        ")\n",
        "\n",
        "# í”„ë¡¬í”„íŠ¸\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Create a funny joke about chickens.\"}\n",
        "]\n",
        "\n",
        "# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì„ ì ìš©í•©ë‹ˆë‹¤.\n",
        "prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False)\n",
        "print(f'\\nâœ… ì‚¬ìš©ëœ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿:\\n{prompt}')\n",
        "\n",
        "# ì¶œë ¥ì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
        "output = pipe(messages, do_sample=True, temperature=1)\n",
        "print(f'\\nâœ… ì¶œë ¥ê²°ê³¼:\\n{output[0][\"generated_text\"]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OMi0ho_UZ6WZ",
        "outputId": "e1ba0730-e2a8-4578-e67c-42a5ed23a906"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:transformers_modules.microsoft.Phi-3-mini-4k-instruct.0a67737cc96d2554230f90338b163bc6380a2a85.modeling_phi3:`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
            "WARNING:transformers_modules.microsoft.Phi-3-mini-4k-instruct.0a67737cc96d2554230f90338b163bc6380a2a85.modeling_phi3:Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n",
            "Device set to use cuda\n",
            "The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n",
            "`get_max_cache()` is deprecated for all Cache classes. Use `get_max_cache_shape()` instead. Calling `get_max_cache()` will raise error from v4.48\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "âœ… ì‚¬ìš©ëœ ëª¨ë¸:\n",
            "microsoft/Phi-3-mini-4k-instruct\n",
            "\n",
            "âœ… ì‚¬ìš©ëœ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿:\n",
            "<|user|>\n",
            "Create a funny joke about chickens.<|end|>\n",
            "<|endoftext|>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:transformers_modules.microsoft.Phi-3-mini-4k-instruct.0a67737cc96d2554230f90338b163bc6380a2a85.modeling_phi3:You are not running the flash-attention implementation, expect numerical differences.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "âœ… ì¶œë ¥ê²°ê³¼:\n",
            " Why do chickens make terrible singers? Because they can't seem to find the right note to cluck!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **ì˜ˆì œ: ëŒ€í™”í˜• ì±—ë´‡ìœ¼ë¡œ ì‚¬ìš©**"
      ],
      "metadata": {
        "id": "Gon3X0gKffnY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"ëŒ€í™”í˜• ì±—ë´‡ì„ ì‹œì‘í•©ë‹ˆë‹¤. ì¢…ë£Œí•˜ë ¤ë©´ 'q'ë¥¼ ì…ë ¥í•˜ì„¸ìš”.\\n\")\n",
        "\n",
        "# ëŒ€í™” íˆìŠ¤í† ë¦¬\n",
        "messages = []\n",
        "\n",
        "# ë©”ì¸ ë£¨í”„\n",
        "while True:\n",
        "    user_input = input(\"âœ… You: \").strip()\n",
        "\n",
        "    if user_input.lower() == 'q':\n",
        "        print(\"ì±—ë´‡ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.\")\n",
        "        break\n",
        "\n",
        "    if not user_input:\n",
        "        continue\n",
        "\n",
        "    # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€\n",
        "    messages.append({\"role\": \"user\", \"content\": user_input})\n",
        "\n",
        "    # ì‘ë‹µ ìƒì„±\n",
        "    output = pipe(messages, do_sample=True, temperature=0.7)\n",
        "    assistant_response = output[0][\"generated_text\"]\n",
        "\n",
        "    # ì–´ì‹œìŠ¤í„´íŠ¸ ë©”ì‹œì§€ ì¶”ê°€\n",
        "    messages.append({\"role\": \"assistant\", \"content\": assistant_response})\n",
        "\n",
        "    print(f\"ğŸ¤– Bot: {assistant_response}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GD_dl7fifgMo",
        "outputId": "b0c64e1c-f54c-439b-e32e-5620ee005a35"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ëŒ€í™”í˜• ì±—ë´‡ì„ ì‹œì‘í•©ë‹ˆë‹¤. ì¢…ë£Œí•˜ë ¤ë©´ 'q'ë¥¼ ì…ë ¥í•˜ì„¸ìš”.\n",
            "\n",
            "âœ… You: ì œì£¼ë„ì— ìˆëŠ” ë§›ì§‘ ì¶”ì²œ\n",
            "ğŸ¤– Bot:  ì œì£¼ë„ì—ì„œ ìœ ëª…í•œ ë§›ì§‘ ì¶”ì²œì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n",
            "\n",
            "\n",
            "1. **ì €ì£¼í•œ ë§›ì§‘** - ì œì£¼ë„ì˜ ì „í†µì ì¸ ìŒì‹ ì¤‘ í•˜ë‚˜, ë§›ìˆëŠ” ìƒëŸ¬ë“œë¥¼ ì œê³µí•©ë‹ˆë‹¤.\n",
            "\n",
            "2. **í•´ë³€ ì‚°ëœ»íˆ ë°˜ì¨Œ** - í•´ë³€ ìˆ² ì†ì— ìˆëŠ” ì˜ˆìœ ê±´ë¬¼ë¡œ, ì €ì¥ì†Œ ë° ë§›ì§‘ìœ¼ë¡œ ì „í†µì ì¸ ì œì£¼ ìŒì‹ì„ ì²˜ìŒìœ¼ë¡œ ìµíˆëŠ” ì¢‹ì€ ì¥ì†Œì…ë‹ˆë‹¤.\n",
            "\n",
            "3. **ì œì£¼ í•´ë°”ë¼ê¸°** - ë”°ëœ»í•œ í•´ë°”ë¼ê¸°ë¥¼ ì°¾ê¸° í˜ë“  ìˆœê°„ì„ ê°€ì ¸ì™€ ìˆìŠµë‹ˆë‹¤.\n",
            "\n",
            "4. **ë”ëŸ½ì´ ë§›ì§‘** - ì €ì£¼ì˜ ëŒ€í‘œì ì¸ ìŒì‹ì , ë§›ìˆëŠ” ì „í†µ ìŒì‹ì°¾ê¸°ì…ë‹ˆë‹¤.\n",
            "\n",
            "5. **ë¸ìŠ¤íƒ€ê·¸ë¨ ì˜¤ë¼í¬ í•´ë³€** - í•´ë³€ ê³¨ëª© ê°™ì€ í’ì„ ì„ ë³¼ ìˆ˜ ìˆëŠ” ë” ì˜ˆìœ í•´ë°”ë¼ê¸° ë§›ì§‘ì…ë‹ˆë‹¤.\n",
            "\n",
            "\n",
            "ì´ ë§›ì§‘ë“¤ì€ ì£¼ë¡œ ì €ì£¼ì˜ ì „í†µ ìŒì‹ì— ì˜í•´ ìœ ëª…í•˜ê³  ì¸ê¸°\n",
            "\n",
            "âœ… You: q\n",
            "ì±—ë´‡ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **ì˜ˆì œ: ì±—ë´‡ì˜ ì„±ëŠ¥ì„ ë†’ì—¬ë³´ì.**\n",
        "\n",
        "1. **ëª¨ë¸ 4-bit ì–‘ìí™” ì„¤ì •**\n",
        "2. **torch.compile()ë¡œ ëª¨ë¸ ìµœì í™”** (PyTorch 2.0+) reduce overhead\n",
        "3. **ëŒ€í™” ê¸¸ì´ ì œí•œ**"
      ],
      "metadata": {
        "id": "YM6CueTEhH79"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ë¨¼ì € ì´ ì…€ì„ ì‹¤í–‰í•˜ì„¸ìš”\n",
        "!pip install -U bitsandbytes accelerate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "nSIHw70dhqjc",
        "outputId": "d381e03b-d3db-4f8e-98d2-c4ac4401f11e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.12/dist-packages (0.48.1)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.11.0)\n",
            "Requirement already satisfied: torch<3,>=2.3 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.8.0+cu126)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from accelerate) (6.0.3)\n",
            "Requirement already satisfied: huggingface_hub>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (0.35.3)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from accelerate) (0.6.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (2025.3.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (2.32.5)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (1.1.10)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<3,>=2.3->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<3,>=2.3->bitsandbytes) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2025.10.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, BitsAndBytesConfig\n",
        "\n",
        "# 1.4-bit ì–‘ìí™” ì„¤ì •\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\"\n",
        ")\n",
        "\n",
        "# ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
        "model_id = \"microsoft/Phi-3-mini-4k-instruct\"\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=\"cuda\",\n",
        "    quantization_config=quantization_config,\n",
        "    trust_remote_code=True,\n",
        "    torch_dtype=torch.float16,\n",
        ")\n",
        "\n",
        "# torch.compile()ë¡œ ëª¨ë¸ ìµœì í™” (PyTorch 2.0+)\n",
        "try:\n",
        "    model = torch.compile(model, mode=\"reduce-overhead\")\n",
        "    print(\"âœ… torch.compile() ìµœì í™” ì ìš©ë¨\")\n",
        "except:\n",
        "    print(\"âš ï¸ torch.compile() ì‚¬ìš© ë¶ˆê°€ (PyTorch 2.0+ í•„ìš”)\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "# íŒŒì´í”„ë¼ì¸ ìƒì„±\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    return_full_text=False,\n",
        "    max_new_tokens=300,  # 500 -> 300ìœ¼ë¡œ ì¤„ì„\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DVuTdw7qcFFT",
        "outputId": "06e9d5c8-ffb7-43df-f33f-a8c912133de6"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… torch.compile() ìµœì í™” ì ìš©ë¨\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda\n",
            "The model 'OptimizedModule' is not supported for text-generation. Supported models are ['AriaTextForCausalLM', 'BambaForCausalLM', 'BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'Cohere2ForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'DiffLlamaForCausalLM', 'ElectraForCausalLM', 'Emu3ForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'Olmo2ForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM', 'ZambaForCausalLM', 'Phi3ForCausalLM'].\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"ëŒ€í™”í˜• ì±—ë´‡ì„ ì‹œì‘í•©ë‹ˆë‹¤. ì¢…ë£Œí•˜ë ¤ë©´ 'q'ë¥¼ ì…ë ¥í•˜ì„¸ìš”.\\n\")\n",
        "\n",
        "# ëŒ€í™” íˆìŠ¤í† ë¦¬ (ìµœëŒ€ 10ê°œ ë©”ì‹œì§€ë¡œ ì œí•œ)\n",
        "messages = []\n",
        "MAX_HISTORY = 10      # 3. ëŒ€í™” ê¸¸ì´ ì œí•œ\n",
        "\n",
        "# ë©”ì¸ ë£¨í”„\n",
        "while True:\n",
        "    user_input = input(\"âœ… You: \").strip()\n",
        "\n",
        "    if user_input.lower() == 'q':\n",
        "        print(\"\\nğŸ”š ì±—ë´‡ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.\")\n",
        "        break\n",
        "\n",
        "    if not user_input:\n",
        "        continue\n",
        "\n",
        "    # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€\n",
        "    messages.append({\"role\": \"user\", \"content\": user_input})\n",
        "\n",
        "    # íˆìŠ¤í† ë¦¬ ì œí•œ (ìµœì‹  10ê°œë§Œ ìœ ì§€)\n",
        "    if len(messages) > MAX_HISTORY:\n",
        "        messages = messages[-MAX_HISTORY:]\n",
        "\n",
        "    # ì‘ë‹µ ìƒì„±\n",
        "    output = pipe(\n",
        "        messages,\n",
        "        do_sample=True,\n",
        "        temperature=0.7,\n",
        "        top_p=0.9,\n",
        "        repetition_penalty=1.1\n",
        "    )\n",
        "    assistant_response = output[0][\"generated_text\"]\n",
        "\n",
        "    # ì–´ì‹œìŠ¤í„´íŠ¸ ë©”ì‹œì§€ ì¶”ê°€\n",
        "    messages.append({\"role\": \"assistant\", \"content\": assistant_response})\n",
        "\n",
        "    print(f\"ğŸ¤– Bot: {assistant_response}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "44feT-USkq31",
        "outputId": "e38b9e8c-0091-46b8-83b5-00607a7c2db1"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ëŒ€í™”í˜• ì±—ë´‡ì„ ì‹œì‘í•©ë‹ˆë‹¤. ì¢…ë£Œí•˜ë ¤ë©´ 'q'ë¥¼ ì…ë ¥í•˜ì„¸ìš”.\n",
            "\n",
            "âœ… You: ì œì£¼ë„ì— ìˆëŠ” ë§›ì§‘ ì¶”ì²œ\n",
            "ğŸ¤– Bot:  ì œì£¼ë„ì˜ ë„¤ ê°€ì§€ ì¸ê¸° ì¶•êµ¬ê°€ ìˆëŠ” ë§›ì§‘ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n",
            "\n",
            "1. **ì•„ì´ìŠ¤í¬ë¦¼ ë¦¬ìŠ¤íŠ¸** - ì œì£¼ì‹œì˜ ë§›ì§‘ ì¤‘ í•˜ë‚˜, ì•„ì´ìŠ¤í¬ë¦¼ ë¦¬ìŠ¤íŠ¸ëŠ” ê³ ê¸°ë¥¼ ìµíˆë©´ì„œ ë§Œë“¤ì–´ ì£¼ëŠ” ì¼ì¢…ì˜ ì‹ì‚¬ ë§¤ì¥ì…ë‹ˆë‹¤. ê·¸ë“¤ì€ í–¥ìˆ˜ë¡œ ì£¼íƒ ë‚´ë¶€ì— ì‚°ìˆ˜ ë¨¹ì‚¬ë¥¼ ì‹œí‚¤ê³ , ì˜·ì„ ì…í•˜ë©´ í•­ìƒ ì¢‹ì€ ìŒì‹ìœ¼ë¡œ ì „í†µì ì¸ í•œêµ­ì‹ ìŒì‹ìœ¼ë¡œ ì¡°ë¡±í•´ì•¼ í•©ë‹ˆë‹¤.\n",
            "\n",
            "2. **ë°”ë‹¤íŒ€** - ì´ ë§›ì§‘ì€ ì œì£¼ì‹œì˜ ì¸ë¼ì¸ ë©”ë‰´ì…‹ì„ ì—¿ë³´ëŠ” ë°ì— ê²½í…ì ì…\n",
            "\n",
            "âœ… You: ë¬´ì§€ê°œì˜ 7ê°€ì§€ ìƒ‰ì€?\n",
            "ğŸ¤– Bot:  ë¬´ì§€ê°œì˜ 7ê°€ì§€ ìƒ‰ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n",
            "\n",
            "1. ì²­ë‹­ (ì´ˆë¡ìƒ‰)\n",
            "\n",
            "2. ì½”ë¿” (ë…¸ë€ìƒ‰)\n",
            "\n",
            "3. ì†”ì (ë¹¨ê°•ìƒ‰)\n",
            "\n",
            "4. ì£½ì—¿ (íšŒìƒ‰)\n",
            "\n",
            "5. ë§ˆìŒ (ë‚¨ìƒ‰)\n",
            "\n",
            "6. ëŒ (ì²­ìƒ‰)\n",
            "\n",
            "7. ì‡ ì†Ÿì´ (ë¹¨ê°„ìƒ‰)\n",
            "\n",
            "\n",
            "ë¬´ì§€ê°œì˜ ìƒì„ ì€ ë§¤ë…„ ì—¬ë¦„ ì¤‘ì— ì²« ë‚ ë¶€í„° ëª©ìš• ê³¡ìœ¼ë¡œ ë¶„ë¥˜ë©ë‹ˆë‹¤. ì´ë“¤ì€ ëŒ€ë¦´ëª¨í†µ ë“±ì— ë§¤ìš° ìµœì ì˜ ìˆ˜ìš”ê°€ ìˆì–´ ì´ì „ ëª©ìš• ê³¡ì—ì„œëŠ” ë” ë§ì€ ìƒì„ ì„ ìˆ˜ì—¬\n",
            "\n",
            "âœ… You: q\n",
            "\n",
            "ğŸ”š ì±—ë´‡ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **ë­ì²´ì¸ìœ¼ë¡œ ì–‘ìí™”ëœ ëª¨ë¸ ë¡œë“œí•˜ê¸°**"
      ],
      "metadata": {
        "id": "N5FufrvYTpDf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **ì–‘ìí™”ëœ ëª¨ë¸** : **Phi-3-mini-4k-instruct-fp16.gguf**\n",
        "- **GGUF**ë€? : (GPT-Generated Unified Format)\n",
        "    - Phi-3, Llama-3, Mistral, Qwen, Gemma ê°™ì€ ëª¨ë¸ì„ LangChainì´ë‚˜ Ollama, llama.cpp, LM Studio, GPT4All ë“±ì—ì„œ ì“¸ ë•Œ ìì£¼ ì‚¬ìš©ë˜ëŠ” í¬ë§·\n",
        "        - ex:  **LM Studio**  https://lmstudio.ai/download\n",
        "        - **LM Studio**ëŠ” ë¡œì»¬ LLM ì‹¤í–‰ìš© GUI/ì—”ì§„ì´ë©°,\n",
        "        - **GGUF**ëŠ” ë¹ ë¥¸ ë¡œë”©/ì¶”ë¡ ì„ ìœ„í•œ ë°”ì´ë„ˆë¦¬ í¬ë§·\n",
        "    - ê°œë°œì/ë°°ê²½** : \tllama.cpp í”„ë¡œì íŠ¸(Georgi Gerganov) íŒ€ì—ì„œ ê¸°ì¡´ GGML â†’ GGUFë¡œ í™•ì¥í•œ ìƒˆë¡œìš´ í¬ë§·\n",
        "    - ë“±ì¥ ë°°ê²½ :\të‹¤ì–‘í•œ LLM(ì˜ˆ: Llama2, Phi3, Mistral)ì„ CPUÂ·GPUÂ·ëª¨ë°”ì¼ì—ì„œë„ **ë¹ ë¥´ê²Œ ì¶”ë¡ í•˜ê¸° ìœ„í•´ ê°€ë³ê²Œ ì €ì¥í•˜ê³  ë¶ˆëŸ¬ì˜¬ ìˆ˜ ìˆëŠ” ê³µí†µ í¬ë§·ì´ í•„ìš”**í–ˆê¸° ë•Œë¬¸\n",
        "    - **ì£¼ìš” ëª©ì **\t: **ëª¨ë¸ ê°€ì¤‘ì¹˜(weight)ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ì €ì¥**í•˜ê³ , **ë¡œì»¬ ë””ë°”ì´ìŠ¤ì—ì„œ ë¹ ë¥´ê²Œ ë¶ˆëŸ¬ì™€ ì¶”ë¡ **í•  ìˆ˜ ìˆë„ë¡ í•¨\n",
        "    - **Huggingface GGUF ê´€ë ¨** : https://huggingface.co/docs/hub/gguf\n",
        "        - **gguf ëª¨ë¸ ê²€ìƒ‰** : https://huggingface.co/models?library=gguf\n",
        "        - **gguf ëª¨ë¸ ë³€í™˜** : https://huggingface.co/spaces/ggml-org/gguf-my-repo\n",
        "        - í—ˆê¹…í˜ì´ìŠ¤ì—ì„œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë§í¬ ì°¾ëŠ” ë°©ë²•\n",
        "            - ëª¨ë¸ ê²€ìƒ‰ > ëª¨ë¸ í˜ì´ì§€ì—ì„œ File íƒ­ ì„ íƒ > í•´ë‹¹ ëª¨ë¸ í´ë¦­ > Copy download link\n"
      ],
      "metadata": {
        "id": "lbrudSwylipt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4EYKJi4bCAYf",
        "outputId": "1d470d60-134d-4beb-f37c-4ce54ddae281"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-10-22 02:18:05--  https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/Phi-3-mini-4k-instruct-fp16.gguf\n",
            "Resolving huggingface.co (huggingface.co)... 13.35.202.121, 13.35.202.97, 13.35.202.34, ...\n",
            "Connecting to huggingface.co (huggingface.co)|13.35.202.121|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cas-bridge.xethub.hf.co/xet-bridge-us/662698108f7573e6a6478546/a9cdcf6e9514941ea9e596583b3d3c44dd99359fb7dd57f322bb84a0adc12ad4?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20251022%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20251022T021806Z&X-Amz-Expires=3600&X-Amz-Signature=1487c1afc95cb5fa8bf036438e7cb23f56913ce24ea76574de1bdfe662253cf0&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27Phi-3-mini-4k-instruct-fp16.gguf%3B+filename%3D%22Phi-3-mini-4k-instruct-fp16.gguf%22%3B&x-id=GetObject&Expires=1761103086&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc2MTEwMzA4Nn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82NjI2OTgxMDhmNzU3M2U2YTY0Nzg1NDYvYTljZGNmNmU5NTE0OTQxZWE5ZTU5NjU4M2IzZDNjNDRkZDk5MzU5ZmI3ZGQ1N2YzMjJiYjg0YTBhZGMxMmFkNCoifV19&Signature=LeuaLlyfkmXgCvsStdxJe3dVICzfljBjK23hDzyUx0ZosTdvBMdlHNXccxqob1q3qMjQtEj%7EPxU-PgsyS11zfBsSLcphsi-WopdT3yZftzbXgIvM1xPHTF8CfqpD24AqmtQ%7EjRuN5lxMNZl8fg2Qqc5bGC03YI-exOk8z-QYJr8hRkvnkK2dN6BtBcUF2CE-Z9%7Eu07xl21QdeciiO-Xtm0Hf2DlPnjeyfCKtintykwGAPdLnwwsSShJSSlAB5nw0XUrEW6pvPyr3D4seF8c874t4f%7EzasuNx8%7E4zgRUmLX-gctWfiraWdV0Ll02zz3A7Td9tFTRb77he62GrjhXN1A__&Key-Pair-Id=K2L8F4GPSG1IFC [following]\n",
            "--2025-10-22 02:18:06--  https://cas-bridge.xethub.hf.co/xet-bridge-us/662698108f7573e6a6478546/a9cdcf6e9514941ea9e596583b3d3c44dd99359fb7dd57f322bb84a0adc12ad4?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20251022%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20251022T021806Z&X-Amz-Expires=3600&X-Amz-Signature=1487c1afc95cb5fa8bf036438e7cb23f56913ce24ea76574de1bdfe662253cf0&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27Phi-3-mini-4k-instruct-fp16.gguf%3B+filename%3D%22Phi-3-mini-4k-instruct-fp16.gguf%22%3B&x-id=GetObject&Expires=1761103086&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc2MTEwMzA4Nn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82NjI2OTgxMDhmNzU3M2U2YTY0Nzg1NDYvYTljZGNmNmU5NTE0OTQxZWE5ZTU5NjU4M2IzZDNjNDRkZDk5MzU5ZmI3ZGQ1N2YzMjJiYjg0YTBhZGMxMmFkNCoifV19&Signature=LeuaLlyfkmXgCvsStdxJe3dVICzfljBjK23hDzyUx0ZosTdvBMdlHNXccxqob1q3qMjQtEj%7EPxU-PgsyS11zfBsSLcphsi-WopdT3yZftzbXgIvM1xPHTF8CfqpD24AqmtQ%7EjRuN5lxMNZl8fg2Qqc5bGC03YI-exOk8z-QYJr8hRkvnkK2dN6BtBcUF2CE-Z9%7Eu07xl21QdeciiO-Xtm0Hf2DlPnjeyfCKtintykwGAPdLnwwsSShJSSlAB5nw0XUrEW6pvPyr3D4seF8c874t4f%7EzasuNx8%7E4zgRUmLX-gctWfiraWdV0Ll02zz3A7Td9tFTRb77he62GrjhXN1A__&Key-Pair-Id=K2L8F4GPSG1IFC\n",
            "Resolving cas-bridge.xethub.hf.co (cas-bridge.xethub.hf.co)... 18.155.68.46, 18.155.68.69, 18.155.68.14, ...\n",
            "Connecting to cas-bridge.xethub.hf.co (cas-bridge.xethub.hf.co)|18.155.68.46|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7643295904 (7.1G)\n",
            "Saving to: â€˜Phi-3-mini-4k-instruct-fp16.ggufâ€™\n",
            "\n",
            "Phi-3-mini-4k-instr 100%[===================>]   7.12G   458MB/s    in 19s     \n",
            "\n",
            "2025-10-22 02:18:25 (379 MB/s) - â€˜Phi-3-mini-4k-instruct-fp16.ggufâ€™ saved [7643295904/7643295904]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/Phi-3-mini-4k-instruct-fp16.gguf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langchain langchain-core langchain-openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tmlBE6Rs9I-q",
        "outputId": "0173cbea-73c4-40c7-8bef-8f3c197b2d36"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (1.0.2)\n",
            "Requirement already satisfied: langchain-core in /usr/local/lib/python3.12/dist-packages (1.0.0)\n",
            "Requirement already satisfied: langchain-openai in /usr/local/lib/python3.12/dist-packages (1.0.1)\n",
            "Requirement already satisfied: langgraph<1.1.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langchain) (1.0.1)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.11.10)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (0.4.37)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (25.0)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (6.0.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (8.5.0)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (4.15.0)\n",
            "Requirement already satisfied: openai<3.0.0,>=1.109.1 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (1.109.1)\n",
            "Requirement already satisfied: tiktoken<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (0.12.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core) (3.0.0)\n",
            "Requirement already satisfied: langgraph-checkpoint<4.0.0,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.0->langchain) (3.0.0)\n",
            "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.0->langchain) (1.0.1)\n",
            "Requirement already satisfied: langgraph-sdk<0.3.0,>=0.2.2 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.0->langchain) (0.2.9)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.0->langchain) (3.6.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (3.11.3)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (1.0.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (2.32.5)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (0.25.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (0.11.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (4.67.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.2)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1.0.0,>=0.7.0->langchain-openai) (2024.11.6)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai<3.0.0,>=1.109.1->langchain-openai) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core) (0.16.0)\n",
            "Requirement already satisfied: ormsgpack>=1.10.0 in /usr/local/lib/python3.12/dist-packages (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.0->langchain) (1.11.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core) (2.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ë¼ì´ë¸ŒëŸ¬ë¦¬ ë²„ì „ í™•ì¸\n",
        "%pip list | grep -E 'langchain'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eSRUi0tJ952a",
        "outputId": "fec6d657-c889-47bb-829a-54949a2faac3"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "langchain                                1.0.2\n",
            "langchain-classic                        1.0.0\n",
            "langchain-community                      0.4\n",
            "langchain-core                           1.0.0\n",
            "langchain-openai                         1.0.1\n",
            "langchain-text-splitters                 1.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **ì˜ˆì œ: ì–‘ìí™”ëœ ëª¨ë¸ ë¡œë“œí•˜ê¸°**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MnGSxOySuPlD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LQcht_ZFijW7",
        "outputId": "2ff6b599-ef7c-4c79-be14-1b66d2b325ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_context: n_batch is less than GGML_KQ_MASK_PAD - increasing to 64\n"
          ]
        }
      ],
      "source": [
        "# âœ… ì˜¬ë°”ë¥¸ ë°©ì‹ (langchain v1.0.0+)\n",
        "from langchain_community.llms import LlamaCpp\n",
        "# c++ë¡œ ëœ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ íŒŒì´ì¬ì—ì„œ ê°€ì ¸ì™€ì„œ ì“¸ìˆ˜ìˆê²Œ í•¨\n",
        "\n",
        "# ì—¬ëŸ¬ë¶„ì˜ ì»´í“¨í„°ì— ë‹¤ìš´ë¡œë“œí•œ ëª¨ë¸ì˜ ê²½ë¡œë¥¼ ì…ë ¥í•˜ì„¸ìš”!\n",
        "llm = LlamaCpp(\n",
        "    model_path=\"./Phi-3-mini-4k-instruct-fp16.gguf\",  # 16ë¹„íŠ¸ ì–‘ìí™”ëœ ëª¨ë¸\n",
        "    n_gpu_layers=-1,\n",
        "    max_tokens=500,\n",
        "    n_ctx=4096,\n",
        "    seed=42,\n",
        "    verbose=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "3SNhQF9WthzV",
        "outputId": "68342936-2dc6-47fa-c10d-1263bdad02e8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "''"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "llm.invoke(\"Hi! My name is Maarten. What is 1 + 1?\")\n",
        "\n",
        "# ì‹¤í–‰í•˜ë©´ ì•„ë¬´ê²ƒë„ ì¶œë ¥ë˜ì§€ ì•ŠëŠ”ë‹¤.\n",
        "# ì–‘ìí™”ëœ ëª¨ë¸ì€ ê·¸ëƒ¥ ì‚¬ìš©í•  ìˆ˜ ì—†ê³ \n",
        "# íŠ¹ë³„í•œ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ë°©ë²•ì¸ 'ì²´ì¸'ì„ ì‚¬ìš©í•´ì•¼ í•œë‹¤."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **[ì‹¤ìŠµ] LM Studio ì‚¬ìš©í•˜ê¸°**\n",
        "\n",
        "- **LM Studio** : PC ì—ì„œ LLM ëª¨ë¸ ì‚¬ìš©/í™•ì¸í•˜ê¸° ìœ„í•œ íˆ´\n",
        "LM Studioë¥¼ PCì— ì„¤ì¹˜í•˜ê³   LM Studioì—ì„œ ëª¨ë¸ ì‚¬ìš©í•´ë³´ê¸°\n",
        "**êµµì€ í…ìŠ¤íŠ¸**\n",
        "1. LM Studio ì„¤ì¹˜\n",
        "2. LM Studioì—ì„œ ëª¨ë¸ ì‚¬ìš©\n",
        "    - ëª¨ë¸ ê²€ìƒ‰: LM Studio ì‹¤í–‰ â†’ Models íƒ­ â†’ ê²€ìƒ‰ì°½ì— phi-3 mini gguf ì…ë ¥\n",
        "    - ëª¨ë¸ ë‹¤ìš´ë¡œë“œ\n",
        "    - ë‹¤ìš´ë¡œë“œê°€ ëë‚˜ë©´ Chat íƒ­ì—ì„œ ë°”ë¡œ ì‚¬ìš©"
      ],
      "metadata": {
        "id": "cZb5-8CvwKDp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "5Ej9imzLNA1D"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wwx2AIuGfCoP"
      },
      "source": [
        "# **ì²´ì¸(Chain)**\n",
        "\n",
        "- **ì²´ì¸**ì€ **ë­ì²´ì¸ì˜ ê°€ì¥ ê¸°ë³¸ì ì¸ í˜•íƒœ**, **ë‹¨ì¼ ì²´ì¸**\n",
        "- **ì²´ì¸ì„ ì‚¬ìš©í•´ LLMì˜ ê¸°ëŠ¥ì„ í™•ì¥í•˜ê±°ë‚˜ ì—°ê²°í•  ìˆ˜ ìˆë‹¤.**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **ë‹¨ì¼ ì²´ì¸** : í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿"
      ],
      "metadata": {
        "id": "i5k7ilDZwcsU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **ì˜ˆì œ : ë³€ìˆ˜ë¥¼ ê°€ì§„ í…œí”Œë¦¿ ì‚¬ìš©**"
      ],
      "metadata": {
        "id": "GLiBj4xu3lRU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "kF--Q5me_-X1"
      },
      "outputs": [],
      "source": [
        "# (LangChain v1.0.0+)\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "# \"input_prompt\" ë³€ìˆ˜ë¥¼ ê°€ì§„ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì„ ë§Œë“­ë‹ˆë‹¤.\n",
        "# |user|, |end| ë“± = gptê³„ì—´ì—ì„œ ì‚¬ìš©ë˜ëŠ” íŠ¹ìˆ˜ í† í°ë“¤\n",
        "template = \"\"\"<|user|>\n",
        "{input_prompt}<|end|>\n",
        "<|assistant|>\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    template=template,\n",
        "    input_variables=[\"input_prompt\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "ogWsGeg6hElt"
      },
      "outputs": [],
      "source": [
        "# ì²« ë²ˆì§¸ ì²´ì¸ : í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ê³¼ LLMì„ ì—°ê²°\n",
        "basic_chain = prompt | llm      # __or__() ë©”ì„œë“œ ë¡œ | ì—°ì‚°ìë¥¼ ì˜¤ë²„ë¡œë”©í•˜ì—¬ ì‚¬ìš©í•¨"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "KINQxKAINXgG",
        "outputId": "2991f6eb-3ff6-4ec4-8ebd-2fdd94018de4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Hello Maarten! The answer to 1 + 1 is 2.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "# ì²´ì¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
        "basic_chain.invoke(\n",
        "    {\n",
        "        \"input_prompt\": \"Hi! My name is Maarten. What is 1 + 1?\",\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ì´ë¦„ ìƒì„±ì„ ìœ„í•œ ì²´ì¸\n",
        "template = \"Create a funny name for a business that sells {product}.\"\n",
        "name_template = PromptTemplate(\n",
        "    template=template,\n",
        "    input_variables=[\"product\"],\n",
        ")\n",
        "\n",
        "# name_template ì„ ì‚¬ìš©í•´ì„œ ì²´ì¸ ì‚¬ìš©í•˜ê¸°\n",
        "name_chain = name_template | llm\n",
        "\n",
        "# name_chain ì‹¤í–‰í•˜ê¸°\n",
        "# name_chain.invoke({\"product\": \"colorful socks\"})\n",
        "name_chain.invoke({\"product\": \"Colorful polka dot socks\"})\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "VWHh83P8zDWQ",
        "outputId": "35556363-a652-4dc4-e82b-1fcedaa7a1c4"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n<|assistant|> Polka Dots & Puns: The Sock-tacular Emporium!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSMBMRxB8gFW"
      },
      "source": [
        "## **ì—¬ëŸ¬ í…œí”Œë¦¿ì„ ê°€ì§„ ì²´ì¸**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **ì˜ˆì œ : í”„ë¡¬í”„íŠ¸ë¥¼ ìª¼ê°œì„œ í•˜ìœ„ ì‘ì—…ì„ ìˆœì°¨ì ìœ¼ë¡œ ì‹¤í–‰**\n",
        "- ì´ì•¼ê¸°ë¥¼ ìƒì„±í•˜ëŠ” ê³¼ì •(ë‹¨ê³„ë³„ ì²˜ë¦¬)\n",
        "    - ì œëª©\n",
        "    - ì£¼ìš” ìºë¦­í„°ì— ëŒ€í•œ ì„¤ëª…\n",
        "    - ì´ì•¼ê¸° ìš”ì•½"
      ],
      "metadata": {
        "id": "pBRimaj_3Gql"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# llm ê°ì²´ëŠ” ì´ë¯¸ ì •ì˜ë˜ì–´ ìˆë‹¤ê³  ê°€ì •\n",
        "# ì˜ˆ: llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
        "# ë˜ëŠ”: llm = LlamaCpp(model_path=\"...\")\n",
        "\n",
        "# ì´ì•¼ê¸° ì œëª©ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿\n",
        "# ==========================================\n",
        "# 1ë‹¨ê³„: ì œëª© ìƒì„± ì²´ì¸\n",
        "# ==========================================\n",
        "template = \"\"\"<|user|>\n",
        "Create a title for a story about {summary}. Only return the title.<|end|>\n",
        "<|assistant|>\"\"\"\n",
        "\n",
        "title_prompt = PromptTemplate(\n",
        "    template=template,\n",
        "    input_variables=[\"summary\"]\n",
        ")\n",
        "\n",
        "# LCEL ì²´ì¸ êµ¬ì„± (íŒŒì´í”„ ì—°ì‚°ì)\n",
        "title_chain = title_prompt | llm | StrOutputParser()\n",
        "\n",
        "# ì‹¤í–‰ (invoke ë©”ì„œë“œ ì‚¬ìš©)\n",
        "result = title_chain.invoke({\"summary\": \"a brave knight fighting a dragon\"})\n",
        "print(f\"ìƒì„±ëœ ì œëª©: {result}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iJA4xfquAtle",
        "outputId": "1accd6ed-1844-4265-b61d-526d1826cc8a"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ìƒì„±ëœ ì œëª©:  \"Sir Valor's Flame: The Dragon's Tale\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "igFIyg73OtaL",
        "outputId": "cab40c90-b30c-4496-f167-a39b5982c78d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' \"Finding Light in the Shadow: A Motherless Journey\"'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "title_chain.invoke({\"summary\": \"a girl that lost her mother\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "zTtFEmANOhyE"
      },
      "outputs": [],
      "source": [
        "# ìš”ì•½ê³¼ ì œëª©ì„ ì‚¬ìš©í•˜ì—¬ ìºë¦­í„° ì„¤ëª…ì„ ìƒì„±í•˜ëŠ” ì²´ì¸ì„ ë§Œë“­ë‹ˆë‹¤.\n",
        "# ==========================================\n",
        "# 2ë‹¨ê³„: ìºë¦­í„° ìƒì„± ì²´ì¸\n",
        "# ==========================================\n",
        "template = \"\"\"<|user|>\n",
        "Describe the main character of a story about {summary} with the title {title}.\n",
        "Use only two sentences.<|end|>\n",
        "<|assistant|>\"\"\"\n",
        "\n",
        "character_prompt = PromptTemplate(\n",
        "    template=template, input_variables=[\"summary\", \"title\"]\n",
        ")\n",
        "\n",
        "# íŒŒì´í”„ ì—°ì‚°ìë¡œ ì²´ì¸ êµ¬ì„±\n",
        "character_chain = character_prompt | llm | StrOutputParser()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "Xjf-avW8NAqZ"
      },
      "outputs": [],
      "source": [
        "# ìš”ì•½, ì œëª©, ìºë¦­í„° ì„¤ëª…ì„ ì‚¬ìš©í•´ ì´ì•¼ê¸°ë¥¼ ìƒì„±í•˜ëŠ” ì²´ì¸ì„ ë§Œë“­ë‹ˆë‹¤.\n",
        "# ==========================================\n",
        "# 3ë‹¨ê³„: ì´ì•¼ê¸° ìƒì„± ì²´ì¸\n",
        "# ==========================================\n",
        "template = \"\"\"<|user|>\n",
        "Create a story about {summary} with the title {title}.\n",
        "The main charachter is: {character}.\n",
        "Only return the story and it cannot be longer than one paragraph<|end|>\n",
        "<|assistant|>\"\"\"\n",
        "\n",
        "story_prompt = PromptTemplate(\n",
        "    template=template,\n",
        "    input_variables=[\"summary\", \"title\", \"character\"]\n",
        ")\n",
        "\n",
        "# íŒŒì´í”„ ì—°ì‚°ìë¡œ ì²´ì¸ êµ¬ì„±\n",
        "story_chain = story_prompt | llm | StrOutputParser()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "epNudKyyPClO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd8658be-9422-4610-a8e2-0aa8b303a8c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ìš”ì•½: a girl that lost her mother\n",
            "ì œëª©:  \"Losing Her Light: A Mother's Legacy in Emily's Heart\"\n",
            "ìºë¦­í„° ì„¤ëª…:  Emily is a resilient and compassionate young girl, who struggles to come to terms with the loss of her mother. As she navigates through grief, she discovers strength in cherishing her mother's enduring love and wisdom that lives on within her heart.\n",
            "ìŠ¤í† ë¦¬:  Emily's heart ached as the emptiness of her mother's absence consumed her, yet within that void shone a steadfast light - an undying love and wisdom passed down through generations. Each day became a journey to reconnect with her mother's legacy, finding solace in cherished memories and embracing strength born from the compassionate lessons she imparted. In this heart-wrenching voyage of loss, Emily discovered that though her mother's physical presence was gone, her light continued to guide her, igniting a resilience within her young soul that promised enduring hope and unbreakable love in the face of grief.\n"
          ]
        }
      ],
      "source": [
        "# ì„¸ ê°œì˜ ìš”ì†Œë¥¼ ì—°ê²°í•˜ì—¬ ìµœì¢… ì²´ì¸ì„ ë§Œë“­ë‹ˆë‹¤.\n",
        "# ==========================================\n",
        "# ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ í•¨ìˆ˜\n",
        "# ==========================================\n",
        "def generate_complete_story(summary):\n",
        "    \"\"\"3ë‹¨ê³„ ì²´ì¸ì„ ìˆœì°¨ì ìœ¼ë¡œ ì‹¤í–‰\"\"\"\n",
        "\n",
        "    # 1ë‹¨ê³„: ì œëª© ìƒì„±\n",
        "    title = title_chain.invoke({\"summary\": summary})\n",
        "\n",
        "    # 2ë‹¨ê³„: ìºë¦­í„° ìƒì„± (summary + title ì‚¬ìš©)\n",
        "    character = character_chain.invoke({\n",
        "        \"summary\": summary,\n",
        "        \"title\": title\n",
        "    })\n",
        "\n",
        "    # 3ë‹¨ê³„: ì´ì•¼ê¸° ìƒì„± (summary + title + character ì‚¬ìš©)\n",
        "    story = story_chain.invoke({\n",
        "        \"summary\": summary,\n",
        "        \"title\": title,\n",
        "        \"character\": character\n",
        "    })\n",
        "\n",
        "    return {\n",
        "        \"summary\": summary,\n",
        "        \"title\": title,\n",
        "        \"character\": character,\n",
        "        \"story\": story\n",
        "    }\n",
        "\n",
        "# ì‹¤í–‰\n",
        "result = generate_complete_story(\"a girl that lost her mother\")\n",
        "\n",
        "# ê²°ê³¼ ì¶œë ¥\n",
        "print(f\"ìš”ì•½: {result['summary']}\")\n",
        "print(f\"ì œëª©: {result['title']}\")\n",
        "print(f\"ìºë¦­í„° ì„¤ëª…: {result['character']}\")\n",
        "print(f\"ìŠ¤í† ë¦¬: {result['story']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **[ì‹¤ìŠµ] í•œêµ­ì–´ ì§€ì› ê²½ëŸ‰í™” ëª¨ë¸ì„ ì´ìš©í•˜ì—¬ ì´ì•¼ê¸° ë§Œë“¤ê¸°**\n",
        "1. **í•œêµ­ì–´ ì§€ì› ê²½ëŸ‰í™” ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë°›ê¸°**\n",
        "    - í—ˆê¹…í˜ì´ìŠ¤ / LM studio ë“±\n",
        "    - ex: llama-3-Korean-Bllossom-3B, Konan-LLM-OND-gguf ë“± ê²½ëŸ‰í™” ëª¨ë¸ ì‚¬ìš©\n",
        "2. **3ë‹¨ê³„ ì²´ì¸ í…œí”Œë¦¿ì„ ë§Œë“¤ê³  Story ì¶œë ¥í•˜ê¸°**\n",
        "    - ì•ì—ì„œ ì‚¬ìš©í•œ ì—¬ëŸ¬ í…œí”Œë¦¿ì„ ê°€ì§„ ì²´ì¸ ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ 3ë‹¨ê³„ ì²´ì¸ í…œí”Œë¦¿ì„ ë§Œë“¤ê³  Story ì¶œë ¥í•˜ê¸°\n",
        "    - story = summary + title + character"
      ],
      "metadata": {
        "id": "R69wJNaZIHgG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://huggingface.co/Bllossom/llama-3.2-Korean-Bllossom-3B-gguf-Q4_K_M/resolve/main/llama-3.2-Korean-Bllossom-3B-gguf-Q4_K_M.gguf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pg8vuE9aNEpR",
        "outputId": "11fb8498-ac9a-4674-9c14-49fcfdaa4288"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-10-22 02:53:55--  https://huggingface.co/Bllossom/llama-3.2-Korean-Bllossom-3B-gguf-Q4_K_M/resolve/main/llama-3.2-Korean-Bllossom-3B-gguf-Q4_K_M.gguf\n",
            "Resolving huggingface.co (huggingface.co)... 13.35.202.97, 13.35.202.34, 13.35.202.40, ...\n",
            "Connecting to huggingface.co (huggingface.co)|13.35.202.97|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cas-bridge.xethub.hf.co/xet-bridge-us/670664a02d3883ec801acb9a/2eca4ef8aad90644a92e57d6138d0fee3083d5f1bf970faf11c226ea7146b75a?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20251022%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20251022T025356Z&X-Amz-Expires=3600&X-Amz-Signature=328b83bf3304d4736372a466577f491c3213dab5f9f93700fe820d50b9141da2&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27llama-3.2-Korean-Bllossom-3B-gguf-Q4_K_M.gguf%3B+filename%3D%22llama-3.2-Korean-Bllossom-3B-gguf-Q4_K_M.gguf%22%3B&x-id=GetObject&Expires=1761105236&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc2MTEwNTIzNn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82NzA2NjRhMDJkMzg4M2VjODAxYWNiOWEvMmVjYTRlZjhhYWQ5MDY0NGE5MmU1N2Q2MTM4ZDBmZWUzMDgzZDVmMWJmOTcwZmFmMTFjMjI2ZWE3MTQ2Yjc1YSoifV19&Signature=gY4%7E25qCN3Bdx4Gzq2PcsVRR1bXDhGTuLpJOp%7E0mL6-KtvSnFJ2sk2sfL6tnDD5ml4t%7EkpCNS8pVmeHq7utjjp01yAL8t%7E6NMrGcWQ2737-NkmY9SFaceK07ddgcCToZZyWRPOgOg0eqO4dFilFIBqNpFVW6T2zXidc4dPd5fL1cq7xsbRfVKJwbT-%7EqLnO%7EfnKDUxfHwS1ACuktYVSC0gQTbQitsDwPDWs59ja0czPA6AQIs2Y%7EjWpc1ptzptcnXoa916CO9wnOIAlH9OWg%7EPj1U-u9-Fl-kzYHur%7EMkyGjbfqZ-DU9npbhcw6GcsFi76OroyPTWZESzaWPLsaVfA__&Key-Pair-Id=K2L8F4GPSG1IFC [following]\n",
            "--2025-10-22 02:53:56--  https://cas-bridge.xethub.hf.co/xet-bridge-us/670664a02d3883ec801acb9a/2eca4ef8aad90644a92e57d6138d0fee3083d5f1bf970faf11c226ea7146b75a?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20251022%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20251022T025356Z&X-Amz-Expires=3600&X-Amz-Signature=328b83bf3304d4736372a466577f491c3213dab5f9f93700fe820d50b9141da2&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27llama-3.2-Korean-Bllossom-3B-gguf-Q4_K_M.gguf%3B+filename%3D%22llama-3.2-Korean-Bllossom-3B-gguf-Q4_K_M.gguf%22%3B&x-id=GetObject&Expires=1761105236&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc2MTEwNTIzNn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82NzA2NjRhMDJkMzg4M2VjODAxYWNiOWEvMmVjYTRlZjhhYWQ5MDY0NGE5MmU1N2Q2MTM4ZDBmZWUzMDgzZDVmMWJmOTcwZmFmMTFjMjI2ZWE3MTQ2Yjc1YSoifV19&Signature=gY4%7E25qCN3Bdx4Gzq2PcsVRR1bXDhGTuLpJOp%7E0mL6-KtvSnFJ2sk2sfL6tnDD5ml4t%7EkpCNS8pVmeHq7utjjp01yAL8t%7E6NMrGcWQ2737-NkmY9SFaceK07ddgcCToZZyWRPOgOg0eqO4dFilFIBqNpFVW6T2zXidc4dPd5fL1cq7xsbRfVKJwbT-%7EqLnO%7EfnKDUxfHwS1ACuktYVSC0gQTbQitsDwPDWs59ja0czPA6AQIs2Y%7EjWpc1ptzptcnXoa916CO9wnOIAlH9OWg%7EPj1U-u9-Fl-kzYHur%7EMkyGjbfqZ-DU9npbhcw6GcsFi76OroyPTWZESzaWPLsaVfA__&Key-Pair-Id=K2L8F4GPSG1IFC\n",
            "Resolving cas-bridge.xethub.hf.co (cas-bridge.xethub.hf.co)... 18.155.68.46, 18.155.68.69, 18.155.68.125, ...\n",
            "Connecting to cas-bridge.xethub.hf.co (cas-bridge.xethub.hf.co)|18.155.68.46|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2019377664 (1.9G)\n",
            "Saving to: â€˜llama-3.2-Korean-Bllossom-3B-gguf-Q4_K_M.ggufâ€™\n",
            "\n",
            "llama-3.2-Korean-Bl 100%[===================>]   1.88G   107MB/s    in 7.9s    \n",
            "\n",
            "2025-10-22 02:54:04 (242 MB/s) - â€˜llama-3.2-Korean-Bllossom-3B-gguf-Q4_K_M.ggufâ€™ saved [2019377664/2019377664]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget !wget https://huggingface.co/mykor/Konan-LLM-OND-gguf/resolve/main/Konan-LLM-OND-Q4_K_M.gguf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E5aQLfWxQMBa",
        "outputId": "8b9f598a-d746-4746-9d26-6458e2e404ff"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-10-22 02:54:04--  http://!wget/\n",
            "Resolving !wget (!wget)... failed: Name or service not known.\n",
            "wget: unable to resolve host address â€˜!wgetâ€™\n",
            "--2025-10-22 02:54:04--  https://huggingface.co/mykor/Konan-LLM-OND-gguf/resolve/main/Konan-LLM-OND-Q4_K_M.gguf\n",
            "Resolving huggingface.co (huggingface.co)... 13.35.202.97, 13.35.202.34, 13.35.202.121, ...\n",
            "Connecting to huggingface.co (huggingface.co)|13.35.202.97|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cas-bridge.xethub.hf.co/xet-bridge-us/68803db977925a8b83368521/ec9f7e7a45e6ae32b79e8928592791e61234cb8f26f1d08db9b88b4d5695db00?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20251022%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20251022T025404Z&X-Amz-Expires=3600&X-Amz-Signature=df20df24dda4155498cc83cb489f8da92d2ccc2d12bf140439f354ba2bf09c1a&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27Konan-LLM-OND-Q4_K_M.gguf%3B+filename%3D%22Konan-LLM-OND-Q4_K_M.gguf%22%3B&x-id=GetObject&Expires=1761105244&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc2MTEwNTI0NH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82ODgwM2RiOTc3OTI1YThiODMzNjg1MjEvZWM5ZjdlN2E0NWU2YWUzMmI3OWU4OTI4NTkyNzkxZTYxMjM0Y2I4ZjI2ZjFkMDhkYjliODhiNGQ1Njk1ZGIwMCoifV19&Signature=HIVtO2l3FgZCxXiaIa4YwOriqmZLmSlTQfPu5aPf9uAeoV1-Ja2yOKapbU5e8nm0rDOiUkbaQbQmJl3RltOsU2n-IHzkW-YXjxEOurUSBFRMlrSbTgxzbxrYBf10Da%7ES9G1MdKGLmVWQp9X8p13BXWWcDC1sGw%7EzeBEVC%7EY2mCfXpp5cyUrOMDrnsGn3hesIGo5A%7EeTLbhT2eWRYIThGFSqRcVg7U4cvuHgy8d8IRHa3IvQFtE6rjXt%7EADZxex0UvJ0ewcye3C1CrUn8XrG6cT54j8%7EQNois0wC6GARET%7ELtBaZ0InDK4p3lEoIvXpxpoPRQY0PSNLzd5vWkLolrbw__&Key-Pair-Id=K2L8F4GPSG1IFC [following]\n",
            "--2025-10-22 02:54:04--  https://cas-bridge.xethub.hf.co/xet-bridge-us/68803db977925a8b83368521/ec9f7e7a45e6ae32b79e8928592791e61234cb8f26f1d08db9b88b4d5695db00?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20251022%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20251022T025404Z&X-Amz-Expires=3600&X-Amz-Signature=df20df24dda4155498cc83cb489f8da92d2ccc2d12bf140439f354ba2bf09c1a&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27Konan-LLM-OND-Q4_K_M.gguf%3B+filename%3D%22Konan-LLM-OND-Q4_K_M.gguf%22%3B&x-id=GetObject&Expires=1761105244&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc2MTEwNTI0NH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82ODgwM2RiOTc3OTI1YThiODMzNjg1MjEvZWM5ZjdlN2E0NWU2YWUzMmI3OWU4OTI4NTkyNzkxZTYxMjM0Y2I4ZjI2ZjFkMDhkYjliODhiNGQ1Njk1ZGIwMCoifV19&Signature=HIVtO2l3FgZCxXiaIa4YwOriqmZLmSlTQfPu5aPf9uAeoV1-Ja2yOKapbU5e8nm0rDOiUkbaQbQmJl3RltOsU2n-IHzkW-YXjxEOurUSBFRMlrSbTgxzbxrYBf10Da%7ES9G1MdKGLmVWQp9X8p13BXWWcDC1sGw%7EzeBEVC%7EY2mCfXpp5cyUrOMDrnsGn3hesIGo5A%7EeTLbhT2eWRYIThGFSqRcVg7U4cvuHgy8d8IRHa3IvQFtE6rjXt%7EADZxex0UvJ0ewcye3C1CrUn8XrG6cT54j8%7EQNois0wC6GARET%7ELtBaZ0InDK4p3lEoIvXpxpoPRQY0PSNLzd5vWkLolrbw__&Key-Pair-Id=K2L8F4GPSG1IFC\n",
            "Resolving cas-bridge.xethub.hf.co (cas-bridge.xethub.hf.co)... 18.155.68.46, 18.155.68.69, 18.155.68.125, ...\n",
            "Connecting to cas-bridge.xethub.hf.co (cas-bridge.xethub.hf.co)|18.155.68.46|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2542139648 (2.4G)\n",
            "Saving to: â€˜Konan-LLM-OND-Q4_K_M.ggufâ€™\n",
            "\n",
            "Konan-LLM-OND-Q4_K_ 100%[===================>]   2.37G   324MB/s    in 8.4s    \n",
            "\n",
            "2025-10-22 02:54:12 (289 MB/s) - â€˜Konan-LLM-OND-Q4_K_M.ggufâ€™ saved [2542139648/2542139648]\n",
            "\n",
            "FINISHED --2025-10-22 02:54:12--\n",
            "Total wall clock time: 8.7s\n",
            "Downloaded: 1 files, 2.4G in 8.4s (289 MB/s)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.llms import LlamaCpp\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# ì—¬ëŸ¬ë¶„ì˜ ì»´í“¨í„°ì— ë‹¤ìš´ë¡œë“œí•œ ëª¨ë¸ì˜ ê²½ë¡œë¥¼ ì…ë ¥í•˜ì„¸ìš”!\n",
        "llm = LlamaCpp(\n",
        "    # model_path=\"./Phi-3-mini-4k-instruct-fp16.gguf\",  # 16ë¹„íŠ¸ ì–‘ìí™”ëœ ëª¨ë¸\n",
        "    # model_path = \"llama-3.2-Korean-Bllossom-3B-gguf-Q4_K_M.gguf\",\n",
        "    model_path = \"./Konan-LLM-OND-Q4_K_M.gguf\",\n",
        "    n_gpu_layers=-1,\n",
        "    max_tokens=500,\n",
        "    n_ctx=4096,\n",
        "    seed=42,\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "\n",
        "# ì´ì•¼ê¸° ì œëª©ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿\n",
        "# ==========================================\n",
        "# 1ë‹¨ê³„: ì œëª© ìƒì„± ì²´ì¸\n",
        "# ==========================================\n",
        "template = \"\"\"<|user|>\n",
        "{summary}ì— ëŒ€í•œ ê¸°ì‚¬ì˜ ì œëª©ì„ ë§Œë“­ë‹ˆë‹¤. ì œëª©ë§Œ ë°˜í™˜í•©ë‹ˆë‹¤.<|end|>\n",
        "<|assistant|>\"\"\"\n",
        "\n",
        "title_prompt = PromptTemplate(\n",
        "    template=template,\n",
        "    input_variables=[\"summary\"]\n",
        ")\n",
        "\n",
        "# LCEL ì²´ì¸ êµ¬ì„± (íŒŒì´í”„ ì—°ì‚°ì)\n",
        "title_chain = title_prompt | llm | StrOutputParser()\n",
        "\n",
        "\n",
        "# ==========================================\n",
        "# 2ë‹¨ê³„: ìºë¦­í„° ìƒì„± ì²´ì¸\n",
        "# ==========================================\n",
        "template = \"\"\"<|user|>\n",
        "{summary}ì— ëŒ€í•œ ì´ì•¼ê¸°ì˜ ì£¼ì¸ê³µì„ ì„¤ëª…í•˜ì„¸ìš”. ì œëª©ì€ {title}ì…ë‹ˆë‹¤.\n",
        "ë‘ ë¬¸ì¥ë§Œ ì‚¬ìš©í•˜ì„¸ìš”.<|end|>\n",
        "<|assistant|>\"\"\"\n",
        "\n",
        "character_prompt = PromptTemplate(\n",
        "    template=template, input_variables=[\"summary\", \"title\"]\n",
        ")\n",
        "\n",
        "# íŒŒì´í”„ ì—°ì‚°ìë¡œ ì²´ì¸ êµ¬ì„±\n",
        "character_chain = character_prompt | llm | StrOutputParser()\n",
        "\n",
        "# ==========================================\n",
        "# 3ë‹¨ê³„: ì´ì•¼ê¸° ìƒì„± ì²´ì¸\n",
        "# ==========================================\n",
        "template = \"\"\"<|user|>\n",
        "{summary}ì— ëŒ€í•œ ìŠ¤í† ë¦¬ë¥¼ {title}ì´ë¼ëŠ” ì œëª©ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”.\n",
        "ì£¼ì¸ê³µì€ {character}ì…ë‹ˆë‹¤.\n",
        "ìŠ¤í† ë¦¬ë§Œ ë°˜í™˜í•˜ë©°, í•œ ë‹¨ë½ì„ ë„˜ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.<|end|>\n",
        "<|assistant|>\"\"\"\n",
        "\n",
        "story_prompt = PromptTemplate(\n",
        "    template=template,\n",
        "    input_variables=[\"summary\", \"title\", \"character\"]\n",
        ")\n",
        "\n",
        "# íŒŒì´í”„ ì—°ì‚°ìë¡œ ì²´ì¸ êµ¬ì„±\n",
        "story_chain = story_prompt | llm | StrOutputParser()\n",
        "\n",
        "\n",
        "# ==========================================\n",
        "# ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ í•¨ìˆ˜\n",
        "# ==========================================\n",
        "def generate_complete_story(summary):\n",
        "    \"\"\"3ë‹¨ê³„ ì²´ì¸ì„ ìˆœì°¨ì ìœ¼ë¡œ ì‹¤í–‰\"\"\"\n",
        "\n",
        "    # 1ë‹¨ê³„: ì œëª© ìƒì„±\n",
        "    title = title_chain.invoke({\"summary\": summary})\n",
        "\n",
        "    # 2ë‹¨ê³„: ìºë¦­í„° ìƒì„± (summary + title ì‚¬ìš©)\n",
        "    character = character_chain.invoke({\n",
        "        \"summary\": summary,\n",
        "        \"title\": title\n",
        "    })\n",
        "\n",
        "    # 3ë‹¨ê³„: ì´ì•¼ê¸° ìƒì„± (summary + title + character ì‚¬ìš©)\n",
        "    story = story_chain.invoke({\n",
        "        \"summary\": summary,\n",
        "        \"title\": title,\n",
        "        \"character\": character\n",
        "    })\n",
        "\n",
        "    return {\n",
        "        \"summary\": summary,\n",
        "        \"title\": title,\n",
        "        \"character\": character,\n",
        "        \"story\": story\n",
        "    }\n",
        "\n",
        "# ì‹¤í–‰\n",
        "result = generate_complete_story(\"ì–´ë¨¸ë‹ˆë¥¼ ìƒì€ ì†Œë…€\")\n",
        "\n",
        "# ê²°ê³¼ ì¶œë ¥\n",
        "print(f\"âœ… ìš”ì•½: {result['summary']}\")\n",
        "print(f\"âœ… ì œëª©: {result['title']}\")\n",
        "print(f\"âœ… ìºë¦­í„° ì„¤ëª…: {result['character']}\")\n",
        "print(f\"âœ… ìŠ¤í† ë¦¬: {result['story']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lNVj5JcTIHrL",
        "outputId": "f38fcb2a-e0d6-4e7a-dc59-b75d9e2991db"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_context: n_batch is less than GGML_KQ_MASK_PAD - increasing to 64\n",
            "llama_context: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… ìš”ì•½: ì–´ë¨¸ë‹ˆë¥¼ ìƒì€ ì†Œë…€\n",
            "âœ… ì œëª©:  \"ì–´ë¨¸ë‹ˆì˜ ì‚¬ë‘ì„ í’ˆê³ , ì†Œë…€ëŠ” ë‹¤ì‹œ ì¼ì–´ì„°ë‹¤.\"\n",
            "âœ… ìºë¦­í„° ì„¤ëª…:  ì†Œë…€ëŠ” ì–´ë¨¸ë‹ˆë¥¼ ìƒì€ ê¹Šì€ ìƒì²˜ ì†ì—ì„œë„ ê°•ì¸í•˜ê²Œ ì„±ì¥í–ˆë‹¤. ê·¸ë…€ëŠ” ì–´ë¨¸ë‹ˆì˜ ì‚¬ë‘ê³¼ ê°€ë¥´ì¹¨ì„ ê°€ìŠ´ì— í’ˆê³ , ë‹¤ì‹œ ì¼ì–´ë‚˜ ì„¸ìƒì„ í–¥í•´ ìì‹ ì˜ ê¸¸ì„ ê±¸ì–´ë‚˜ê°”ë‹¤.\n",
            "âœ… ìŠ¤í† ë¦¬:  ì–´ë¨¸ë‹ˆë¥¼ ìƒì€ ê¹Šì€ ìƒì²˜ ì†ì—ì„œë„ ì†Œë…€ëŠ” ê°•ì¸í•˜ê²Œ ì„±ì¥í–ˆë‹¤. ê·¸ë…€ëŠ” ì–´ë¨¸ë‹ˆì˜ ì‚¬ë‘ê³¼ ê°€ë¥´ì¹¨ì„ ê°€ìŠ´ì— í’ˆê³ , ë‹¤ì‹œ ì¼ì–´ë‚˜ ì„¸ìƒì„ í–¥í•´ ìì‹ ì˜ ê¸¸ì„ ê±¸ì–´ë‚˜ê°”ë‹¤.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "5gTcrpdUUu6J"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7UQ-DZ71P-D-"
      },
      "source": [
        "# **ë©”ëª¨ë¦¬**\n",
        "ëŒ€í™”ë¥¼ ê¸°ì–µí•˜ë„ë¡ LLM ë•ê¸°\n",
        "\n",
        "- **LLMì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ë©´ëŒ€í™”ì˜ ë‚´ìš©ì„ ê¸°ì–µí•˜ì§€ ëª»í•œë‹¤.** --> **ë©”ëª¨ë¦¬ê°€ ì—†ë‹¤.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "-15Eoey5EJUO",
        "outputId": "46678d08-a76b-48f5-b7bd-70b7d03fca33"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Hello Maarten! The answer to 1 + 1 is 2.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "# LLMì—ê²Œ ì´ë¦„ì„ ì•Œë ¤ ì¤ë‹ˆë‹¤.\n",
        "basic_chain.invoke({\"input_prompt\": \"Hi! My name is Maarten. What is 1 + 1?\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "N42wQRl-Lykt",
        "outputId": "df62e835-ba82-428e-d08d-7255ed9688d1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" I'm sorry, but as a digital assistant, I don't have the ability to know personal information about individuals unless it has been shared with me in the course of our conversation. To ensure your privacy and data protection, please do not share sensitive personal details.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "# LLMì—ê²Œ ì´ë¦„ì„ ë¬»ìŠµë‹ˆë‹¤.\n",
        "basic_chain.invoke({\"input_prompt\": \"What is my name?\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- <mark>**LLMì´ ëŒ€í™”ë¥¼ ê¸°ì–µí•˜ë„ë¡ ë•ëŠ” ë°©ë²•**</mark>\n",
        "    - **ëŒ€í™” ë²„í¼**(conversation buffer)\n",
        "    - **ëŒ€í™” ìš”ì•½**(conversation summary)"
      ],
      "metadata": {
        "id": "xNJuWEVRV7zD"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PfqATEZjMgET"
      },
      "source": [
        "### **ëŒ€í™” ë²„í¼**\n",
        "\n",
        "- **ê°€ì¥ ê°„ë‹¨í•œ LLM ë©”ëª¨ë¦¬ í˜•íƒœ**\n",
        "- **ê³¼ê±°ì˜ ëŒ€í™”ë¥¼ ê·¸ëŒ€ë¡œ ì „ë‹¬** : ëŒ€í™” ì´ë ¥ì„ ëª¨ë‘ ë³µì‚¬í•˜ì—¬ í”„ë¡¬í”„íŠ¸ì— ì¶”ê°€\n",
        "- llangchain_community : **ChatMessageHistory**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "Zoo0PA1fUs70"
      },
      "outputs": [],
      "source": [
        "# ëŒ€í™” ê¸°ë¡ì„ ë‹´ì„ ìˆ˜ ìˆë„ë¡ í”„ë¡¬í”„íŠ¸ë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.\n",
        "template = \"\"\"<|user|>Current conversation:{chat_history}\n",
        "\n",
        "{input_prompt}<|end|>\n",
        "<|assistant|>\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    template=template,\n",
        "    input_variables=[\"input_prompt\", \"chat_history\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.llms import LlamaCpp\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_community.chat_message_histories import ChatMessageHistory\n",
        "\n",
        "# ==========================================\n",
        "# ì „ì—­ ì„¤ì •\n",
        "# ==========================================\n",
        "\n",
        "# LlamaCpp ëª¨ë¸ ì´ˆê¸°í™”\n",
        "llm = LlamaCpp(\n",
        "    model_path=\"Konan-LLM-OND-Q4_K_M.gguf\",\n",
        "    n_gpu_layers=-1,\n",
        "    max_tokens=500,\n",
        "    n_ctx=4096,\n",
        "    seed=42,\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "# ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿\n",
        "template = \"\"\"<|user|>ì´ì „ ëŒ€í™”:\n",
        "{chat_history}\n",
        "\n",
        "í˜„ì¬ ì§ˆë¬¸: {input_prompt}<|end|>\n",
        "<|assistant|>\"\"\"\n",
        "\n",
        "# í”„ë¡¬í”„íŠ¸ì™€ ì²´ì¸ êµ¬ì„±\n",
        "prompt = PromptTemplate(\n",
        "    template=template,\n",
        "    input_variables=[\"input_prompt\", \"chat_history\"]\n",
        ")\n",
        "\n",
        "chain = prompt | llm | StrOutputParser()\n",
        "\n",
        "# ì„¸ì…˜ ì €ì¥ì†Œ\n",
        "sessions = {}\n",
        "\n",
        "# ==========================================\n",
        "# í•µì‹¬ í•¨ìˆ˜ë“¤\n",
        "# ==========================================\n",
        "\n",
        "def get_history(session_id: str = \"default\") -> ChatMessageHistory:\n",
        "    \"\"\"ì„¸ì…˜ íˆìŠ¤í† ë¦¬ ê°€ì ¸ì˜¤ê¸°\"\"\"\n",
        "    if session_id not in sessions:\n",
        "        sessions[session_id] = ChatMessageHistory()\n",
        "    return sessions[session_id]\n",
        "\n",
        "def format_history(messages) -> str:\n",
        "    \"\"\"ë©”ì‹œì§€ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜\"\"\"\n",
        "    if not messages:\n",
        "        return \"ëŒ€í™” ê¸°ë¡ ì—†ìŒ\"\n",
        "    return \"\\n\".join(\n",
        "        f\"{'ì‚¬ìš©ì' if msg.type == 'human' else 'AI'}: {msg.content}\"\n",
        "        for msg in messages\n",
        "    )\n",
        "\n",
        "def chat(user_input: str, session_id: str = \"default\") -> str:\n",
        "    \"\"\"ëŒ€í™” ì‹¤í–‰\"\"\"\n",
        "    history = get_history(session_id)\n",
        "\n",
        "    response = chain.invoke({\n",
        "        \"input_prompt\": user_input,\n",
        "        \"chat_history\": format_history(history.messages)\n",
        "    })\n",
        "\n",
        "    history.add_user_message(user_input)\n",
        "    history.add_ai_message(response)\n",
        "\n",
        "    return response\n",
        "\n",
        "def get_chat_history(session_id: str = \"default\") -> list:\n",
        "    \"\"\"íŠ¹ì • ì„¸ì…˜ì˜ ëŒ€í™” ê¸°ë¡ ë°˜í™˜\"\"\"\n",
        "    history = get_history(session_id)\n",
        "    return [\n",
        "        {\"role\": msg.type, \"content\": msg.content}\n",
        "        for msg in history.messages\n",
        "    ]\n",
        "\n",
        "def clear_history(session_id: str = \"default\"):\n",
        "    \"\"\"íŠ¹ì • ì„¸ì…˜ì˜ ëŒ€í™” ê¸°ë¡ ì´ˆê¸°í™”\"\"\"\n",
        "    if session_id in sessions:\n",
        "        sessions[session_id].clear()\n",
        "\n",
        "def get_all_sessions() -> list:\n",
        "    \"\"\"ëª¨ë“  ì„¸ì…˜ ID ë°˜í™˜\"\"\"\n",
        "    return list(sessions.keys())\n",
        "\n",
        "# ==========================================\n",
        "# ì‚¬ìš© ì˜ˆì‹œ\n",
        "# ==========================================\n",
        "\n",
        "# ëŒ€í™” ì‹¤í–‰\n",
        "response1 = chat(\"ì•ˆë…•í•˜ì„¸ìš”!\", session_id=\"user_001\")\n",
        "response2 = chat(\"íŒŒì´ì¬ì´ë€?\", session_id=\"user_001\")\n",
        "response3 = chat(\"ì œê°€ ì²˜ìŒì— ë­ë¼ê³  í–ˆì£ ?\", session_id=\"user_001\")\n",
        "\n",
        "# ëŒ€í™” ê¸°ë¡ í™•ì¸\n",
        "history = get_chat_history(session_id=\"user_001\")\n",
        "\n",
        "print(f\"âœ… ì‘ë‹µ 1: {response1}\")\n",
        "print(f\"âœ… ì‘ë‹µ 2: {response2}\")\n",
        "print(f\"âœ… ì‘ë‹µ 3: {response3}\")\n",
        "print(f\"\\nâœ… ëŒ€í™” ê¸°ë¡: {len(history)}ê°œ ë©”ì‹œì§€\")\n",
        "print(f\"âœ… ëŒ€í™” ê¸°ë¡: {history}\")\n",
        "\n",
        "# ëŒ€í™” ê¸°ë¡ ì‚­ì œ\n",
        "clear_history(session_id=\"user_001\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jbqStW_seFyO",
        "outputId": "f0334e00-ee60-4967-fc1d-077e4e8095e7"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_context: n_batch is less than GGML_KQ_MASK_PAD - increasing to 64\n",
            "llama_context: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… ì‘ë‹µ 1: ì•ˆë…•í•˜ì„¸ìš”! ì–´ë–»ê²Œ ë„ì™€ë“œë¦´ê¹Œìš”?\n",
            "âœ… ì‘ë‹µ 2: íŒŒì´ì¬(Python)ì€ 1990ë…„ëŒ€ ì´ˆì— ê·€ë„ ë°˜ ë¡œì„¬(Guido van Rossum)ì´ ê°œë°œí•œ ê³ ê¸‰ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì…ë‹ˆë‹¤.\n",
            "\n",
            "### íŒŒì´ì¬ì˜ ì£¼ìš” íŠ¹ì§•:\n",
            "1. **ì‰¬ìš´ ë¬¸ë²•**: íŒŒì´ì¬ì€ ì½ê¸° ì‰½ê³  ê°„ê²°í•œ ë¬¸ë²•ì„ ê°€ì§€ê³  ìˆì–´, ì´ˆë³´ìë¶€í„° ì „ë¬¸ê°€ê¹Œì§€ í­ë„“ê²Œ ì‚¬ìš©ë©ë‹ˆë‹¤.\n",
            "\n",
            "2. **ì¸í„°í”„ë¦¬í„° ì–¸ì–´**: íŒŒì´ì¬ì€ ì»´íŒŒì¼ ì—†ì´ ë°”ë¡œ ì‹¤í–‰í•  ìˆ˜ ìˆëŠ” ì¸í„°í”„ë¦¬í„° ì–¸ì–´ì…ë‹ˆë‹¤.\n",
            "\n",
            "3. **ë‹¤ì–‘í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ì™€ í”„ë ˆì„ì›Œí¬**: íŒŒì´ì¬ì€ ë°ì´í„° ë¶„ì„, ë¨¸ì‹ ëŸ¬ë‹, ì›¹ ê°œë°œ ë“± ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ í’ë¶€í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ì™€ í”„ë ˆì„ì›Œí¬ë¥¼ ì œê³µí•©ë‹ˆë‹¤.\n",
            "\n",
            "4. **í”Œë«í¼ ë…ë¦½ì **: íŒŒì´ì¬ì€ ìœˆë„ìš°, ë§¥OS, ë¦¬ëˆ…ìŠ¤ ë“± ë‹¤ì–‘í•œ ìš´ì˜ì²´ì œì—ì„œ ë™ì¼í•˜ê²Œ ì‹¤í–‰ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
            "\n",
            "### íŒŒì´ì¬ì˜ ì£¼ìš” í™œìš© ë¶„ì•¼:\n",
            "- **ì›¹ ê°œë°œ**: Django, Flask ë“±ì˜ í”„ë ˆì„ì›Œí¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ê°œë°œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
            "- **ë°ì´í„° ë¶„ì„ ë° ì‹œê°í™”**: Pandas, NumPy, Matplotlib, Seaborn ë“±ì˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ê³  ì‹œê°í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
            "- **ë¨¸ì‹ ëŸ¬ë‹ ë° ì¸ê³µì§€ëŠ¥**: Scikit-learn, TensorFlow, PyTorch ë“±ì˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ì™€ í”„ë ˆì„ì›Œí¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì„ ê°œë°œí•˜ê³  í›ˆë ¨ì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
            "\n",
            "### ê²°ë¡ :\n",
            "íŒŒì´ì¬ì€ ê·¸ ê°„ê²°í•œ ë¬¸ë²•ê³¼ í’ë¶€í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë•ë¶„ì— ì´ˆë³´ìë¶€í„° ì „ë¬¸ê°€ê¹Œì§€ í­ë„“ê²Œ ì‚¬ë‘ë°›ëŠ” í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì…ë‹ˆë‹¤.\n",
            "âœ… ì‘ë‹µ 3: \n",
            "\n",
            "âœ… ëŒ€í™” ê¸°ë¡: 6ê°œ ë©”ì‹œì§€\n",
            "âœ… ëŒ€í™” ê¸°ë¡: [{'role': 'human', 'content': 'ì•ˆë…•í•˜ì„¸ìš”!'}, {'role': 'ai', 'content': 'ì•ˆë…•í•˜ì„¸ìš”! ì–´ë–»ê²Œ ë„ì™€ë“œë¦´ê¹Œìš”?'}, {'role': 'human', 'content': 'íŒŒì´ì¬ì´ë€?'}, {'role': 'ai', 'content': 'íŒŒì´ì¬(Python)ì€ 1990ë…„ëŒ€ ì´ˆì— ê·€ë„ ë°˜ ë¡œì„¬(Guido van Rossum)ì´ ê°œë°œí•œ ê³ ê¸‰ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì…ë‹ˆë‹¤.\\n\\n### íŒŒì´ì¬ì˜ ì£¼ìš” íŠ¹ì§•:\\n1. **ì‰¬ìš´ ë¬¸ë²•**: íŒŒì´ì¬ì€ ì½ê¸° ì‰½ê³  ê°„ê²°í•œ ë¬¸ë²•ì„ ê°€ì§€ê³  ìˆì–´, ì´ˆë³´ìë¶€í„° ì „ë¬¸ê°€ê¹Œì§€ í­ë„“ê²Œ ì‚¬ìš©ë©ë‹ˆë‹¤.\\n\\n2. **ì¸í„°í”„ë¦¬í„° ì–¸ì–´**: íŒŒì´ì¬ì€ ì»´íŒŒì¼ ì—†ì´ ë°”ë¡œ ì‹¤í–‰í•  ìˆ˜ ìˆëŠ” ì¸í„°í”„ë¦¬í„° ì–¸ì–´ì…ë‹ˆë‹¤.\\n\\n3. **ë‹¤ì–‘í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ì™€ í”„ë ˆì„ì›Œí¬**: íŒŒì´ì¬ì€ ë°ì´í„° ë¶„ì„, ë¨¸ì‹ ëŸ¬ë‹, ì›¹ ê°œë°œ ë“± ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ í’ë¶€í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ì™€ í”„ë ˆì„ì›Œí¬ë¥¼ ì œê³µí•©ë‹ˆë‹¤.\\n\\n4. **í”Œë«í¼ ë…ë¦½ì **: íŒŒì´ì¬ì€ ìœˆë„ìš°, ë§¥OS, ë¦¬ëˆ…ìŠ¤ ë“± ë‹¤ì–‘í•œ ìš´ì˜ì²´ì œì—ì„œ ë™ì¼í•˜ê²Œ ì‹¤í–‰ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\\n\\n### íŒŒì´ì¬ì˜ ì£¼ìš” í™œìš© ë¶„ì•¼:\\n- **ì›¹ ê°œë°œ**: Django, Flask ë“±ì˜ í”„ë ˆì„ì›Œí¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ê°œë°œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\\n- **ë°ì´í„° ë¶„ì„ ë° ì‹œê°í™”**: Pandas, NumPy, Matplotlib, Seaborn ë“±ì˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ê³  ì‹œê°í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\\n- **ë¨¸ì‹ ëŸ¬ë‹ ë° ì¸ê³µì§€ëŠ¥**: Scikit-learn, TensorFlow, PyTorch ë“±ì˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ì™€ í”„ë ˆì„ì›Œí¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì„ ê°œë°œí•˜ê³  í›ˆë ¨ì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\\n\\n### ê²°ë¡ :\\níŒŒì´ì¬ì€ ê·¸ ê°„ê²°í•œ ë¬¸ë²•ê³¼ í’ë¶€í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë•ë¶„ì— ì´ˆë³´ìë¶€í„° ì „ë¬¸ê°€ê¹Œì§€ í­ë„“ê²Œ ì‚¬ë‘ë°›ëŠ” í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì…ë‹ˆë‹¤.'}, {'role': 'human', 'content': 'ì œê°€ ì²˜ìŒì— ë­ë¼ê³  í–ˆì£ ?'}, {'role': 'ai', 'content': ''}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sw3ELCg6Rpsk"
      },
      "source": [
        "### **ìœˆë„ ëŒ€í™” ë²„í¼**\n",
        "\n",
        "- ëŒ€í™”ê°€ ëŠ˜ì–´ë‚¨ì— ë”°ë¼ ì…ë ¥ **í”„ë¡¬í”„íŠ¸ì˜ í¬ê¸°ë„ ì»¤ì ¸ ìµœëŒ€ í† í° ê°œìˆ˜ë¥¼ ì´ˆê³¼í•  ìˆ˜ ìˆë‹¤**.\n",
        "- ë¬¸ë§¥ ìœˆë„ í¬ê¸°ë¥¼ ìµœì†Œí™”í•˜ëŠ”  ë°©ë²•ì€ ì „ì²´ ì±„íŒ… ê¸°ë¡ì„ ì‚¬ìš©í•˜ì§€ ì•Šê³  **ë§ˆì§€ë§‰ kê°œì˜ ëŒ€í™”ë§Œ ì‚¬ìš©**í•˜ëŠ” ê²ƒì´ë‹¤.\n",
        "- llangchain_community : **ChatMessageHistory**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G0DRT7kjRtiC",
        "outputId": "3b900fd2-f5c6-4b01-e70f-1de878424059"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_context: n_batch is less than GGML_KQ_MASK_PAD - increasing to 64\n",
            "llama_context: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… ì‘ë‹µ 1: 1 + 1 = 2.\n",
            "âœ… ì‘ë‹µ 2: 3 + 3 = 6.\n",
            "\n",
            "âœ… ëŒ€í™” ê¸°ë¡: 4ê°œ ë©”ì‹œì§€\n",
            "âœ… ëŒ€í™” ê¸°ë¡: [HumanMessage(content='Hi! My name is Maarten and I am 33 years old. What is 1 + 1?', additional_kwargs={}, response_metadata={}), AIMessage(content='1 + 1 = 2.', additional_kwargs={}, response_metadata={}), HumanMessage(content='What is 3 + 3?', additional_kwargs={}, response_metadata={}), AIMessage(content='3 + 3 = 6.', additional_kwargs={}, response_metadata={})]\n"
          ]
        }
      ],
      "source": [
        "from langchain_community.llms import LlamaCpp\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_community.chat_message_histories import ChatMessageHistory\n",
        "\n",
        "# ëª¨ë¸ ì´ˆê¸°í™”\n",
        "llm = LlamaCpp(\n",
        "    model_path=\"Konan-LLM-OND-Q4_K_M.gguf\",\n",
        "    n_gpu_layers=-1,\n",
        "    max_tokens=500,\n",
        "    n_ctx=4096,\n",
        "    seed=42,\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "\n",
        "# ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿\n",
        "template = \"\"\"<|user|>ì´ì „ ëŒ€í™”:\n",
        "{chat_history}\n",
        "\n",
        "í˜„ì¬ ì§ˆë¬¸: {input_prompt}<|end|>\n",
        "<|assistant|>\"\"\"\n",
        "\n",
        "# í”„ë¡¬í”„íŠ¸ì™€ ì²´ì¸ êµ¬ì„±\n",
        "prompt = PromptTemplate(\n",
        "    template=template,\n",
        "    input_variables=[\"input_prompt\", \"chat_history\"]\n",
        ")\n",
        "\n",
        "# ì²´ì¸ êµ¬ì„±\n",
        "chain = prompt | llm | StrOutputParser()\n",
        "# chain = (\n",
        "#    PromptTemplate.from_template(\n",
        "#        \"<|user|>ì´ì „ ëŒ€í™”:\\n{chat_history}\\n\\ní˜„ì¬ ì§ˆë¬¸: {input_prompt}<|end|>\\n<|assistant|>\"\n",
        "#    )\n",
        "#    | llm\n",
        "#    | StrOutputParser()\n",
        "#)\n",
        "\n",
        "# ì„¸ì…˜ ì €ì¥ì†Œ (k ê°’ í¬í•¨)\n",
        "sessions = {}\n",
        "\n",
        "def chat(user_input: str, session_id: str = \"default\", k: int = 2) -> str:\n",
        "    \"\"\"ìœˆë„ìš° ë©”ëª¨ë¦¬ ëŒ€í™” í•¨ìˆ˜\"\"\"\n",
        "    if session_id not in sessions:\n",
        "        sessions[session_id] = {'history': ChatMessageHistory(), 'k': k}\n",
        "\n",
        "    session = sessions[session_id]\n",
        "    history = session['history']\n",
        "    window_size = session['k']\n",
        "\n",
        "    # ìµœê·¼ kí„´ë§Œ ì‚¬ìš©\n",
        "    recent = history.messages[-(window_size * 2):] if history.messages else []\n",
        "    chat_history = \"\\n\".join(\n",
        "        f\"{'ì‚¬ìš©ì' if m.type == 'human' else 'AI'}: {m.content}\"\n",
        "        for m in recent\n",
        "    ) or \"ëŒ€í™” ê¸°ë¡ ì—†ìŒ\"\n",
        "\n",
        "    response = chain.invoke({\n",
        "        \"input_prompt\": user_input,\n",
        "        \"chat_history\": chat_history\n",
        "    })\n",
        "\n",
        "    history.add_user_message(user_input)\n",
        "    history.add_ai_message(response)\n",
        "\n",
        "    return response\n",
        "\n",
        "# ì‚¬ìš©\n",
        "response1 = chat(\"Hi! My name is Maarten and I am 33 years old. What is 1 + 1?\", k=2)\n",
        "response2 = chat(\"What is 3 + 3?\")\n",
        "history = sessions['default']['history'].messages\n",
        "\n",
        "print(f\"âœ… ì‘ë‹µ 1: {response1}\")\n",
        "print(f\"âœ… ì‘ë‹µ 2: {response2}\")\n",
        "print(f\"\\nâœ… ëŒ€í™” ê¸°ë¡: {len(history)}ê°œ ë©”ì‹œì§€\")\n",
        "print(f\"âœ… ëŒ€í™” ê¸°ë¡: {history}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nvSLfKWpR5h5",
        "outputId": "c3482a82-9c8a-4081-f829-34acf6255b7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… ì‘ë‹µ : Your name is Maarten.\n"
          ]
        }
      ],
      "source": [
        "# ì´ë¦„ì„ ê¸°ì–µí•˜ëŠ”ê³  ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.\n",
        "response = chat(\"What is my name?\")\n",
        "print(f\"âœ… ì‘ë‹µ : {response}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YW7qEyctcqeJ",
        "outputId": "79f29d5a-8810-4e50-8f8b-c2319b4a4ee4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… ì‘ë‹µ :  It seems like there's no way for me to know your actual age. I only have the information we've exchanged so far, which does not include your age.\n",
            "\n",
            "If you have any other questions or need further assistance, feel free to ask!\n"
          ]
        }
      ],
      "source": [
        "# ë‚˜ì´ë¥¼ ê¸°ì–µí•˜ëŠ”ê³  ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.\n",
        "response = chat(\"What is my age?\")\n",
        "print(f\"âœ… ì‘ë‹µ : {response}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSb5OnANMhu2"
      },
      "source": [
        "### **ëŒ€í™” ìš”ì•½**\n",
        "\n",
        "- ë¬¸ë§¥ ìœˆë„ìš°ê°€ í° LLMì„ ì‚¬ìš©í•˜ì—¬ ëŒ€í™”ë¥¼ ê¸°ì–µí•˜ëŠ” ê¸°ëŠ¥ì„ ì–´ëŠì •ë„ í•´ê²°í•  ìˆ˜ ìˆì§€ë§Œ ì´ ë°©ë²•ì€ í† í°ì„ ìƒì„±í•˜ê¸° ì „ì— ëŒ€í™” ê¸°ë¡ì— ë‹´ê¸´ í† í°ì„ ì²˜ë¦¬í•´ì•¼ í•˜ë¯€ë¡œ ê³„ì‚° ì‹œê°„ì´ ëŠ˜ì–´ë‚œë‹¤.\n",
        "- **ëŒ€í™” ê¸°ë¡ì„ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©**í•˜ì—¬ **ë‹¤ë¥¸ LLMì—ê²Œ ê°„ê²°í•œ ìš”ì•½ì„ ìƒì„±í•˜ë¼ê³  ìš”ì²­**í•œë‹¤.\n",
        "- **LLMì—ê²Œ ì§ˆë¬¸í•  ë•Œë§ˆë‹¤ ë‘ ë²ˆì˜ ìš”ì²­(í˜¸ì¶œ)ì´ ë°œìƒ**í•œë‹¤.\n",
        "    - **ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸**\n",
        "    - **ìš”ì•½ í”„ë¡¬í”„íŠ¸**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "lWHZlJUbwpqE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d663bb44-035c-40d3-e4ab-6f0fcd18fb4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_context: n_batch is less than GGML_KQ_MASK_PAD - increasing to 64\n",
            "llama_context: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "ğŸ“ Summary Memory ëª¨ë“œ\n",
            "======================================================================\n",
            "âœ… ì‘ë‹µ 1:  Hi, Maarten! The answer to 1 + 1 is **2**. ğŸ˜Š\n",
            "âœ… ì‘ë‹µ 2:  Your name is Maarten.\n",
            "âœ… ìš”ì•½: \n",
            "âœ… ëŒ€í™” ê¸°ë¡: 4ê°œ ë©”ì‹œì§€\n"
          ]
        }
      ],
      "source": [
        "from langchain_community.llms import LlamaCpp\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_community.chat_message_histories import ChatMessageHistory\n",
        "\n",
        "# ëª¨ë¸ ì´ˆê¸°í™”\n",
        "llm = LlamaCpp(\n",
        "    model_path=\"Konan-LLM-OND-Q4_K_M.gguf\",\n",
        "    n_gpu_layers=-1,\n",
        "    max_tokens=500,\n",
        "    n_ctx=4096,\n",
        "    seed=42,\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿\n",
        "template = \"\"\"<|user|>ì´ì „ ëŒ€í™” ìš”ì•½:\n",
        "{summary}\n",
        "\n",
        "ìµœê·¼ ëŒ€í™”:\n",
        "{chat_history}\n",
        "\n",
        "í˜„ì¬ ì§ˆë¬¸: {input_prompt}<|end|>\n",
        "<|assistant|>\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    template=template,\n",
        "    input_variables=[\"input_prompt\", \"summary\", \"chat_history\"]\n",
        ")\n",
        "\n",
        "# ìš”ì•½ í…œí”Œë¦¿\n",
        "summary_prompt_template = \"\"\"<|user|>Summarize the conversations and update with the new lines.\n",
        "\n",
        "Current summary:\n",
        "{summary}\n",
        "\n",
        "new lines of conversation:\n",
        "{new_lines}\n",
        "\n",
        "New summary:<|end|>\n",
        "<|assistant|>\"\"\"\n",
        "\n",
        "summary_prompt = PromptTemplate(\n",
        "    input_variables=[\"new_lines\", \"summary\"],\n",
        "    template=summary_prompt_template\n",
        ")\n",
        "\n",
        "# ì²´ì¸ êµ¬ì„±\n",
        "chain = prompt | llm | StrOutputParser()\n",
        "summary_chain = summary_prompt | llm | StrOutputParser()\n",
        "\n",
        "# ì„¸ì…˜ ì €ì¥ì†Œ\n",
        "sessions = {}\n",
        "\n",
        "def chat(user_input: str, session_id: str = \"default\", k: int = None) -> str:\n",
        "    \"\"\"\n",
        "    ëŒ€í™” í•¨ìˆ˜\n",
        "    - k=None: Summary Memory ì‚¬ìš© (max_history=4)\n",
        "    - k=ìˆ«ì: Window Memory ì‚¬ìš©\n",
        "    \"\"\"\n",
        "    if session_id not in sessions:\n",
        "        sessions[session_id] = {\n",
        "            'history': ChatMessageHistory(),\n",
        "            'summary': \"\",\n",
        "            'k': k\n",
        "        }\n",
        "\n",
        "    session = sessions[session_id]\n",
        "    history = session['history']\n",
        "    window_size = k if k is not None else session['k']\n",
        "\n",
        "    # Window Memory ëª¨ë“œ\n",
        "    if window_size is not None:\n",
        "        recent = history.messages[-(window_size * 2):] if history.messages else []\n",
        "        chat_history = \"\\n\".join(\n",
        "            f\"{'ì‚¬ìš©ì' if m.type == 'human' else 'AI'}: {m.content}\"\n",
        "            for m in recent\n",
        "        ) or \"ëŒ€í™” ê¸°ë¡ ì—†ìŒ\"\n",
        "        summary = \"ì—†ìŒ\"\n",
        "\n",
        "    # Summary Memory ëª¨ë“œ\n",
        "    else:\n",
        "        max_history = 4\n",
        "\n",
        "        # ë©”ì‹œì§€ê°€ ë§ìœ¼ë©´ ìš”ì•½\n",
        "        if len(history.messages) > max_history:\n",
        "            messages_to_summarize = history.messages[:-max_history]\n",
        "            new_lines = \"\\n\".join(\n",
        "                f\"{'ì‚¬ìš©ì' if m.type == 'human' else 'AI'}: {m.content}\"\n",
        "                for m in messages_to_summarize\n",
        "            )\n",
        "\n",
        "            session['summary'] = summary_chain.invoke({\n",
        "                \"summary\": session['summary'] if session['summary'] else \"ì—†ìŒ\",\n",
        "                \"new_lines\": new_lines\n",
        "            })\n",
        "\n",
        "            recent = history.messages[-max_history:]\n",
        "            history.clear()\n",
        "            for msg in recent:\n",
        "                if msg.type == \"human\":\n",
        "                    history.add_user_message(msg.content)\n",
        "                else:\n",
        "                    history.add_ai_message(msg.content)\n",
        "\n",
        "        chat_history = \"\\n\".join(\n",
        "            f\"{'ì‚¬ìš©ì' if m.type == 'human' else 'AI'}: {m.content}\"\n",
        "            for m in history.messages\n",
        "        ) or \"ëŒ€í™” ê¸°ë¡ ì—†ìŒ\"\n",
        "\n",
        "        summary = session['summary'] if session['summary'] else \"ì—†ìŒ\"\n",
        "\n",
        "    # ì‘ë‹µ ìƒì„±\n",
        "    response = chain.invoke({\n",
        "        \"input_prompt\": user_input,\n",
        "        \"summary\": summary,\n",
        "        \"chat_history\": chat_history\n",
        "    })\n",
        "\n",
        "    history.add_user_message(user_input)\n",
        "    history.add_ai_message(response)\n",
        "\n",
        "    return response\n",
        "\n",
        "# ==========================================\n",
        "# ì‚¬ìš© ì˜ˆì‹œ\n",
        "# ==========================================\n",
        "\n",
        "# Summary Memory ëª¨ë“œ (k=None)\n",
        "print(\"=\"*70)\n",
        "print(\"ğŸ“ Summary Memory ëª¨ë“œ\")\n",
        "print(\"=\"*70)\n",
        "response1 = chat(\"Hi! My name is Maarten. What is 1 + 1?\", session_id=\"user1\")\n",
        "response2 = chat(\"What is my name?\", session_id=\"user1\")\n",
        "\n",
        "print(f\"âœ… ì‘ë‹µ 1: {response1}\")\n",
        "print(f\"âœ… ì‘ë‹µ 2: {response2}\")\n",
        "print(f\"âœ… ìš”ì•½: {sessions['user1']['summary']}\")\n",
        "print(f\"âœ… ëŒ€í™” ê¸°ë¡: {len(sessions['user1']['history'].messages)}ê°œ ë©”ì‹œì§€\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ì§€ê¸ˆê¹Œì§€ ë‚´ìš©ì´ ìš”ì•½ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.\n",
        "response = chat(\"What was the first question I asked?\", session_id=\"user1\")\n",
        "print(f\"âœ… ì‘ë‹µ : {response}\")\n",
        "print(f\"âœ… ìš”ì•½ : {sessions['user1']['summary']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jK2epxcem6mt",
        "outputId": "99660661-e8d2-4c53-dd62-82d939c86c4f"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… ì‘ë‹µ : \n",
            "âœ… ìš”ì•½ : \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ì—¬ëŸ¬ ëŒ€í™”ë¥¼ ì§„í–‰í•˜ì—¬ ìš”ì•½ íŠ¸ë¦¬ê±°\n",
        "conversations = [\n",
        "    \"Hi! My name is Maarten.\",\n",
        "    \"I am 33 years old.\",\n",
        "    \"I live in Amsterdam.\",\n",
        "    \"What is 1 + 1?\",\n",
        "    \"What is my name?\",  # ì´ ì‹œì ì—ì„œ ìš”ì•½ í™•ì¸ ê°€ëŠ¥\n",
        "]\n",
        "\n",
        "for i, conv in enumerate(conversations, 1):\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"ëŒ€í™” {i}: {conv}\")\n",
        "    print(f\"{'='*70}\")\n",
        "\n",
        "    response = chat(conv, session_id=\"user_001\")\n",
        "    print(f\"ğŸ¤– ì‘ë‹µ: {response[:150]}...\")\n",
        "\n",
        "    # í˜„ì¬ ì„¸ì…˜ ìƒíƒœ\n",
        "    session = sessions['user_001']\n",
        "    print(f\"\\nğŸ“Š í˜„ì¬ ìƒíƒœ:\")\n",
        "    print(f\"   - í˜„ì¬ ë©”ì‹œì§€ ìˆ˜: {len(session['history'].messages)}ê°œ\")\n",
        "    print(f\"   - ìš”ì•½ ìƒíƒœ: {'ìˆìŒ' if session['summary'] else 'ì—†ìŒ'}\")\n",
        "    if session['summary']:\n",
        "        print(f\"   - ìš”ì•½ ë‚´ìš©: {session['summary'][:100]}...\")\n",
        "\n",
        "# ==========================================\n",
        "# ìµœì¢… í™•ì¸\n",
        "# ==========================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"ğŸ“„ ìµœì¢… ì„¸ì…˜ ìƒíƒœ\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "session = sessions['user_001']\n",
        "history = session['history'].messages\n",
        "summary = session['summary']\n",
        "\n",
        "print(f\"\\nâœ… ëŒ€í™” ê¸°ë¡: {len(history)}ê°œ ë©”ì‹œì§€\")\n",
        "for i, msg in enumerate(history, 1):\n",
        "    print(f\"   [{i}] {msg.type}: {msg.content[:60]}...\")\n",
        "\n",
        "print(f\"\\nâœ… ìš”ì•½:\")\n",
        "print(f\"   {summary if summary else '(ìš”ì•½ ì—†ìŒ)'}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IZV5s5T_n8Hp",
        "outputId": "dad53ecc-3feb-407a-f688-52911db66e0c"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "ëŒ€í™” 1: Hi! My name is Maarten.\n",
            "======================================================================\n",
            "ğŸ¤– ì‘ë‹µ: Hello, Maarten! It's nice to meet you. How can I assist you today?...\n",
            "\n",
            "ğŸ“Š í˜„ì¬ ìƒíƒœ:\n",
            "   - í˜„ì¬ ë©”ì‹œì§€ ìˆ˜: 2ê°œ\n",
            "   - ìš”ì•½ ìƒíƒœ: ì—†ìŒ\n",
            "\n",
            "======================================================================\n",
            "ëŒ€í™” 2: I am 33 years old.\n",
            "======================================================================\n",
            "ğŸ¤– ì‘ë‹µ: I'm glad to hear that you've already taken some time for yourself. At 33, you're at a stage in life where personal growth and development are particul...\n",
            "\n",
            "ğŸ“Š í˜„ì¬ ìƒíƒœ:\n",
            "   - í˜„ì¬ ë©”ì‹œì§€ ìˆ˜: 4ê°œ\n",
            "   - ìš”ì•½ ìƒíƒœ: ì—†ìŒ\n",
            "\n",
            "======================================================================\n",
            "ëŒ€í™” 3: I live in Amsterdam.\n",
            "======================================================================\n",
            "ğŸ¤– ì‘ë‹µ:  It sounds like you're in a vibrant and culturally rich city. Amsterdam is known for its iconic landmarks, such as the Rijksmuseum or the Van Gogh Mus...\n",
            "\n",
            "ğŸ“Š í˜„ì¬ ìƒíƒœ:\n",
            "   - í˜„ì¬ ë©”ì‹œì§€ ìˆ˜: 6ê°œ\n",
            "   - ìš”ì•½ ìƒíƒœ: ì—†ìŒ\n",
            "\n",
            "======================================================================\n",
            "ëŒ€í™” 4: What is 1 + 1?\n",
            "======================================================================\n",
            "ğŸ¤– ì‘ë‹µ: 1 + 1 is equal to 2....\n",
            "\n",
            "ğŸ“Š í˜„ì¬ ìƒíƒœ:\n",
            "   - í˜„ì¬ ë©”ì‹œì§€ ìˆ˜: 6ê°œ\n",
            "   - ìš”ì•½ ìƒíƒœ: ì—†ìŒ\n",
            "\n",
            "======================================================================\n",
            "ëŒ€í™” 5: What is my name?\n",
            "======================================================================\n",
            "ğŸ¤– ì‘ë‹µ:  I don't have access to your personal information. However, I can say that you need to check the name field in your account settings or profile inform...\n",
            "\n",
            "ğŸ“Š í˜„ì¬ ìƒíƒœ:\n",
            "   - í˜„ì¬ ë©”ì‹œì§€ ìˆ˜: 6ê°œ\n",
            "   - ìš”ì•½ ìƒíƒœ: ì—†ìŒ\n",
            "\n",
            "======================================================================\n",
            "ğŸ“„ ìµœì¢… ì„¸ì…˜ ìƒíƒœ\n",
            "======================================================================\n",
            "\n",
            "âœ… ëŒ€í™” ê¸°ë¡: 6ê°œ ë©”ì‹œì§€\n",
            "   [1] human: I live in Amsterdam....\n",
            "   [2] ai:  It sounds like you're in a vibrant and culturally rich city...\n",
            "   [3] human: What is 1 + 1?...\n",
            "   [4] ai: 1 + 1 is equal to 2....\n",
            "   [5] human: What is my name?...\n",
            "   [6] ai:  I don't have access to your personal information. However, ...\n",
            "\n",
            "âœ… ìš”ì•½:\n",
            "   (ìš”ì•½ ì—†ìŒ)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "ndsVUZZCojv8"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BG5sJa1qvS4N"
      },
      "source": [
        "# **ì—ì´ì „íŠ¸**\n",
        "LLM ì‹œìŠ¤í…œ êµ¬ì¶•í•˜ê¸°"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **ì—ì´ì „íŠ¸**(**Agent**) : ì–¸ì–´ ëª¨ë¸ì„ ì‚¬ìš©í•´ ì–´ë–¤ í–‰ë™ì„ ì–´ë–¤ ìˆœì„œë¡œ ìˆ˜í–‰íì§€ ê²°ì •í•˜ëŠ” ì‹œìŠ¤í…œ\n",
        "- **ì—ì´ì „íŠ¸ ì¶”ê°€ í•µì‹¬ êµ¬ì„± ìš”ì†Œ**\n",
        "    - ì—ì´ì „íŠ¸ê°€ ìŠ¤ìŠ¤ë¡œ ìˆ˜í–‰í•  ìˆ˜ ì—†ëŠ” ì‘ì—…ì„ ìœ„í•´ ì‚¬ìš©í•  ë„êµ¬(tool)\n",
        "    - ìˆ˜í–‰í•  í–‰ë™ ë˜ëŠ” ì‚¬ìš©í•  ë„êµ¬ë¥¼ ê³„íší•˜ëŠ” ì—ì´ì „íŠ¸ ìœ í˜•(agent type)\n",
        "\n",
        "- ì—ì´ì „íŠ¸ëŠ” ëª©í‘œë¥¼ ë‹¬ì„±í•˜ê¸° ìœ„í•œ ë¡œë“œë§µì„ ë§Œë“¤ê³  ìŠ¤ìŠ¤ë¡œ ìˆ˜ì •í•˜ëŠ” ë“±ì˜ ê³ ì°¨ì›ì  í–‰ë™ì„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "- ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•´ ì‹¤ì„¸ê³„ì™€ ìƒí˜¸ì‘ìš©ì„ í•  ìˆ˜ ìˆë‹¤. (--> ë‹¤ì–‘í•œ ì‘ì—…ì„ í•  ìˆ˜ ìˆë‹¤.)"
      ],
      "metadata": {
        "id": "BaHDNJyRotpP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **ReAct**: ì¶”ë¡ ê³¼ í–‰ë™ì„ ì—°ê²°í•˜ëŠ” ê°•ë ¥í•œ í”„ë ˆì„ì›Œí¬\n",
        "    - Reasoning(ì¶”ë¡ ) + Acting(í–‰ë™)\n",
        "- **ReActì˜ ì‚¬ì´í´ ë‹¨ê³„**\n",
        "    - ì‚¬ê³  : ìƒê°í•˜ê³  ì¶”ë¡ í•˜ëŠ” ê³¼ì • (Thought), í•´ì•¼í•  ì¼\n",
        "    - í–‰ë™ : ì‹¤ì œ í–‰ë™ì„ ì·¨í•˜ëŠ” ê³¼ì • (Action), í•  ì¼\n",
        "    - ê´€ì¸¡ : í•´ë™ì˜ ê²°ê³¼\n",
        "    - ì¶”ë¡ ê³¼ í–‰ë™ì„ ë²ˆê°ˆì•„ê°€ë©° ìˆ˜í–‰í•˜ì—¬ ë¬¸ì œ í•´ê²°\n",
        "\n"
      ],
      "metadata": {
        "id": "VO-rZUGHqI7_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **ë­ì²´ì¸ì˜ ReACT**"
      ],
      "metadata": {
        "id": "r8XYxpIvrSJw"
      }
    }
  ]
}