{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanghyun-ai/ktcloud_genai/blob/main/%EC%8B%A4%EC%8A%B5%EC%BD%94%EB%93%9C/111_LLM_%ED%86%A0%ED%81%B0%EA%B3%BC%EC%9E%84%EB%B2%A0%EB%94%A9_new.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Token & Embedding 자세히 살펴보기**"
      ],
      "metadata": {
        "id": "_intVJo1FXQF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "- 💡 **NOTE**\n",
        "    - 이 노트북의 코드를 실행하려면 GPU를 사용하는 것이 좋습니다. 구글 코랩에서는 **런타임 > 런타임 유형 변경 > 하드웨어 가속기 > T4 GPU**를 선택하세요.\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "81_Ybs4LI7IX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Tokenization**"
      ],
      "metadata": {
        "id": "X8ya0DICf_mC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4Z1qf7ir_O8k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "846acc96-81c6-4465-aa7f-c2e70350cb2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers==4.48.3\n",
            "  Downloading transformers-4.48.3-py3-none-any.whl.metadata (44 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/44.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers==4.48.3) (3.19.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.48.3) (0.35.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.48.3) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.48.3) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.48.3) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.48.3) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers==4.48.3) (2.32.4)\n",
            "Collecting tokenizers<0.22,>=0.21 (from transformers==4.48.3)\n",
            "  Downloading tokenizers-0.21.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.48.3) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers==4.48.3) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers==4.48.3) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers==4.48.3) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers==4.48.3) (1.1.10)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.48.3) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.48.3) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.48.3) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.48.3) (2025.8.3)\n",
            "Downloading transformers-4.48.3-py3-none-any.whl (9.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m62.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.21.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m36.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tokenizers, transformers\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.22.0\n",
            "    Uninstalling tokenizers-0.22.0:\n",
            "      Successfully uninstalled tokenizers-0.22.0\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.56.1\n",
            "    Uninstalling transformers-4.56.1:\n",
            "      Successfully uninstalled transformers-4.56.1\n",
            "Successfully installed tokenizers-0.21.4 transformers-4.48.3\n"
          ]
        }
      ],
      "source": [
        "# Phi-3 모델과 호환성 때문에 transformers 4.48.3 버전을 사용합니다.\n",
        "!pip install transformers==4.48.3"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 깃허브에서 위젯 상태 오류를 피하기 위해 진행 표시줄을 나타내지 않도록 설정합니다.\n",
        "from transformers.utils import logging\n",
        "\n",
        "logging.disable_progress_bar()"
      ],
      "metadata": {
        "id": "BKozl7tQzNjm"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQHfpqT_t9-K"
      },
      "source": [
        "## LLM 토큰화 확인\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. 모델 로드하고 토크나이저 지정하기**"
      ],
      "metadata": {
        "id": "xg49D27lZWwD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "jjU8NBHnwA4j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49b96929-8540-4be7-b516-695ac85a2736"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
            "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
            "You are not authenticated with the Hugging Face Hub in this notebook.\n",
            "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
            "  warnings.warn(\n",
            "A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-3-mini-4k-instruct:\n",
            "- configuration_phi3.py\n",
            ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
            "A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-3-mini-4k-instruct:\n",
            "- modeling_phi3.py\n",
            ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
            "WARNING:transformers_modules.microsoft.Phi-3-mini-4k-instruct.0a67737cc96d2554230f90338b163bc6380a2a85.modeling_phi3:`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
            "WARNING:transformers_modules.microsoft.Phi-3-mini-4k-instruct.0a67737cc96d2554230f90338b163bc6380a2a85.modeling_phi3:Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# 모델과 토크나이저를 로드합니다.\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"microsoft/Phi-3-mini-4k-instruct\",\n",
        "    device_map=\"cuda\",\n",
        "    torch_dtype=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.입력 프롬프트를 토큰으로 나누기**"
      ],
      "metadata": {
        "id": "wdSVV0tZZ2GE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "_iVl5yePuq3B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24c51032-a058-45cc-fe36-0e04389a4872"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n",
            "`get_max_cache()` is deprecated for all Cache classes. Use `get_max_cache_shape()` instead. Calling `get_max_cache()` will raise error from v4.48\n",
            "WARNING:transformers_modules.microsoft.Phi-3-mini-4k-instruct.0a67737cc96d2554230f90338b163bc6380a2a85.modeling_phi3:You are not running the flash-attention implementation, expect numerical differences.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Write an email apologizing to Sarah for the tragic gardening mishap.\n",
            "Explain how it happened.<|assistant|> Subject: Sincere Apologies for the Gardening Mishap\n",
            "\n",
            "Dear Sarah\n"
          ]
        }
      ],
      "source": [
        "prompt = '''Write an email apologizing to Sarah for the tragic gardening mishap.\n",
        "Explain how it happened.<|assistant|>'''\n",
        "\n",
        "# 입력 프롬프트를 토큰으로 나눕니다.\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
        "\n",
        "# 텍스트를 생성합니다.\n",
        "generation_output = model.generate(\n",
        "  input_ids=input_ids,\n",
        "  max_new_tokens=20   # 토큰을 20개 생성\n",
        ")\n",
        "\n",
        "# 출력을 프린트합니다.\n",
        "print(tokenizer.decode(generation_output[0]))  # decode 메서드: 토큰ID를 실제 텍스트로 변환"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "JmzgbbdKuvHt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19a2d2c0-ded1-4865-976e-587460b33dd5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[14350,   385,  4876, 27746,  5281,   304, 19235,   363,   278, 25305,\n",
            "           293, 16423,   292,   286,   728,   481, 29889,    13,  9544,  7420,\n",
            "           920,   372,  9559, 29889, 32001]], device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "print(input_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "W4vsjbxwu1K1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17b1fad7-830a-49ec-e383-40fd3f57c5b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Write\n",
            "an\n",
            "email\n",
            "apolog\n",
            "izing\n",
            "to\n",
            "Sarah\n",
            "for\n",
            "the\n",
            "trag\n",
            "ic\n",
            "garden\n",
            "ing\n",
            "m\n",
            "ish\n",
            "ap\n",
            ".\n",
            "\n",
            "\n",
            "Exp\n",
            "lain\n",
            "how\n",
            "it\n",
            "happened\n",
            ".\n",
            "<|assistant|>\n"
          ]
        }
      ],
      "source": [
        "for id in input_ids[0]:\n",
        "   print(tokenizer.decode(id))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "A9wRZ3J3u4z1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4667aa1-1738-42de-af2c-2da6215bb7d9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[14350,   385,  4876, 27746,  5281,   304, 19235,   363,   278, 25305,\n",
              "           293, 16423,   292,   286,   728,   481, 29889,    13,  9544,  7420,\n",
              "           920,   372,  9559, 29889, 32001,  3323,   622, 29901,   317,  3742,\n",
              "           406,  6225, 11763,   363,   278, 19906,   292,   341,   728,   481,\n",
              "            13,    13, 29928,   799, 19235]], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "generation_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "7QlHLof3u8A3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf12b6ab-e795-481d-a6d7-2c8c7298a055"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sub\n",
            "ject\n",
            "Subject\n",
            ":\n"
          ]
        }
      ],
      "source": [
        "print(tokenizer.decode(3323))\n",
        "print(tokenizer.decode(622))\n",
        "print(tokenizer.decode([3323, 622]))\n",
        "print(tokenizer.decode(29901))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **토크나이저가 텍스트를 분할하는 방법**\n",
        "\n",
        "1. 모델 설계시 모델 작성자가 토큰화 방법을 선택\n",
        "    - GTP 모델 : BPE(byte pair encoding)\n",
        "    - BERT 모델 : WordPiece\n",
        "2. 토큰화 방법을 선택한 후에 어휘사전 크기와 특수 토큰 같은 토크나이저 설계상의 여러 가지 선택을 해야함\n",
        "3. 토크나이저는 특정 데이터셋에서 훈련하여 해당 데이터셋을 표현하는 최상의 어휘사전을 구축해야 함"
      ],
      "metadata": {
        "id": "fV2mGa9VcpGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **토큰 종류**\n",
        "\n",
        "- **단어 토큰**\n",
        "    - word2vec와 같은 초기 토큰화에 사용됨 --> 현재는 덜 사용됨\n",
        "    - 지금도 추천 시스템과 같은 곳에서 사용\n",
        "    - (단점: 훈련된 후에 데이터셋에 새롭게 추가된 단어는 처리할 수 없다.-->되도록 많은 어휘사전을 만들어야 한다.)\n",
        "\n",
        "- **부분단어 토큰** (완전단어+부분완전단어 포함)\n",
        "    - 새로운 단어를 (어휘사전에 포함되어 있을 가능성이 높은) 더 작은 단위로 나눈다.\n",
        "    - **평균적으로 토큰당 세 개의 문자로 구성됨**\n",
        "- **문자 토큰**\n",
        "    - 대체할 원시 문자가 있기 때문에 새로운 단어를 잘 처리할 수 있다.\n",
        "    - 토큰화는 쉽지만 모델링은 어렵다.--> 문자를 조합하는 정보까지 모델링해야함)\n",
        "- **바이트 토큰**\n",
        "    - 유니코드 문자를 표현하는 바이트로 토큰을 분할하는 방법\n",
        "    - '토큰화-프리 인코딩' 라고 부름\n",
        "    - 다국어 환경에서 경쟁력이 있다고 봄"
      ],
      "metadata": {
        "id": "79cY6Iu9d8FE"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9nRducW48bd"
      },
      "source": [
        "## **훈련된 LLM 토큰나이저 비교하기**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **중점 비교 요소**\n",
        "    - **토큰화 방법** :  BEP, SentencePiece, WordPiece\n",
        "    - **토크나이저 파라미터**\n",
        "        - 어휘사전 크기 : 토크나이저가 어휘사전에 얼마나 많은 토큰을 포함할 건가?\n",
        "        - 특수 토큰 : 모델이 추적해야할 특수 토큰은 무엇인가?\n",
        "        - 대소문자 : 영어와 같은 대소문자를 어떻게 다뤄야 할까?"
      ],
      "metadata": {
        "id": "nS-tC9BJ5dcY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "7W0xFIVo5A0S"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "colors_list = [\n",
        "    '102;194;165', '252;141;98', '141;160;203',\n",
        "    '231;138;195', '166;216;84', '255;217;47'\n",
        "]\n",
        "\n",
        "def show_tokens(sentence, tokenizer_name):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
        "    token_ids = tokenizer(sentence).input_ids\n",
        "    for idx, t in enumerate(token_ids):\n",
        "        # 텍스트를 인코딩한 후 다시 디코딩했을 때 원본 텍스트와 동일해지려면\n",
        "        # clean_up_tokenization_spaces를 False로 지정해야 합니다.\n",
        "        # 현재 이 매개변수의 기본값은 None(True에 해당)이며\n",
        "        # transformers 4.45에서 True로 바뀔 예정입니다.\n",
        "        # https://github.com/huggingface/transformers/issues/31884\n",
        "        print(\n",
        "            f'\\x1b[0;30;48;2;{colors_list[idx % len(colors_list)]}m' +\n",
        "            tokenizer.decode(t, clean_up_tokenization_spaces=False) +\n",
        "            '\\x1b[0m',\n",
        "            end=' '\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Gcc3JjwX5DK-"
      },
      "outputs": [],
      "source": [
        "text = \"\"\"\n",
        "English and CAPITALIZATION\n",
        "🎵 鸟\n",
        "show_tokens False None elif == >= else: two tabs:\"\t\t\" four spaces:\"    \"\n",
        "12.0*50=600\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **BERT 베이스 모델(uncased)(2018)**\n",
        "    - 토큰화 방법 : WordPiece\n",
        "    - 어휘사전 크기 : 30, 522\n",
        "    - 특수 토큰:\n",
        "        - [UNK] unk_token : 토크나이저가 인코딩 방법을 모르는 토큰\n",
        "        - [SEP] sep_toketn :  특정 작업에서 두 개의 텍스트를 구분하기 위한 토큰(cross-endcoder)\n",
        "        - [PAD] pad_token :  모델 입력에서 사용되지 않는 위치를 채우기 위한 패딩 토큰 --> 모델은 특정 길이(문맥 크기)의 입력을 기대하기 때문\n",
        "        - [CLS] cls_token :  분류 작업을 위한 특수 토큰\n",
        "        - [MASK] mask_token : 훈련 과정 동안 일부 토큰을 감추기 위해 사용되는 마스킹 토큰\n",
        "    - 특징 : 줄 바꿈 인코딩 정보를 알지 못함"
      ],
      "metadata": {
        "id": "CGIMgVxflv7S"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "fCDGSXP75Hv-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbf7387a-307c-4ffc-ebe3-b408812fffc6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0;30;48;2;102;194;165m[CLS]\u001b[0m \u001b[0;30;48;2;252;141;98menglish\u001b[0m \u001b[0;30;48;2;141;160;203mand\u001b[0m \u001b[0;30;48;2;231;138;195mcapital\u001b[0m \u001b[0;30;48;2;166;216;84m##ization\u001b[0m \u001b[0;30;48;2;255;217;47m[UNK]\u001b[0m \u001b[0;30;48;2;102;194;165m[UNK]\u001b[0m \u001b[0;30;48;2;252;141;98mshow\u001b[0m \u001b[0;30;48;2;141;160;203m_\u001b[0m \u001b[0;30;48;2;231;138;195mtoken\u001b[0m \u001b[0;30;48;2;166;216;84m##s\u001b[0m \u001b[0;30;48;2;255;217;47mfalse\u001b[0m \u001b[0;30;48;2;102;194;165mnone\u001b[0m \u001b[0;30;48;2;252;141;98meli\u001b[0m \u001b[0;30;48;2;141;160;203m##f\u001b[0m \u001b[0;30;48;2;231;138;195m=\u001b[0m \u001b[0;30;48;2;166;216;84m=\u001b[0m \u001b[0;30;48;2;255;217;47m>\u001b[0m \u001b[0;30;48;2;102;194;165m=\u001b[0m \u001b[0;30;48;2;252;141;98melse\u001b[0m \u001b[0;30;48;2;141;160;203m:\u001b[0m \u001b[0;30;48;2;231;138;195mtwo\u001b[0m \u001b[0;30;48;2;166;216;84mtab\u001b[0m \u001b[0;30;48;2;255;217;47m##s\u001b[0m \u001b[0;30;48;2;102;194;165m:\u001b[0m \u001b[0;30;48;2;252;141;98m\"\u001b[0m \u001b[0;30;48;2;141;160;203m\"\u001b[0m \u001b[0;30;48;2;231;138;195mfour\u001b[0m \u001b[0;30;48;2;166;216;84mspaces\u001b[0m \u001b[0;30;48;2;255;217;47m:\u001b[0m \u001b[0;30;48;2;102;194;165m\"\u001b[0m \u001b[0;30;48;2;252;141;98m\"\u001b[0m \u001b[0;30;48;2;141;160;203m12\u001b[0m \u001b[0;30;48;2;231;138;195m.\u001b[0m \u001b[0;30;48;2;166;216;84m0\u001b[0m \u001b[0;30;48;2;255;217;47m*\u001b[0m \u001b[0;30;48;2;102;194;165m50\u001b[0m \u001b[0;30;48;2;252;141;98m=\u001b[0m \u001b[0;30;48;2;141;160;203m600\u001b[0m \u001b[0;30;48;2;231;138;195m[SEP]\u001b[0m "
          ]
        }
      ],
      "source": [
        "show_tokens(text, \"bert-base-uncased\")  # Uncased : 대소문자를 구분하지 않음"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **BERT 베이스 모델(cased)(2018)**\n",
        "    - 토큰화 방법 : WordPiece\n",
        "    - 어휘사전 크기 : 28,996\n",
        "    - 특수 토큰: uncased 버전과 동일"
      ],
      "metadata": {
        "id": "Aks_7WuCnqKS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "0Ay_NX3K5HyP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9150f3f3-9eed-4467-99b4-c1749e30bb2f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0;30;48;2;102;194;165m[CLS]\u001b[0m \u001b[0;30;48;2;252;141;98mEnglish\u001b[0m \u001b[0;30;48;2;141;160;203mand\u001b[0m \u001b[0;30;48;2;231;138;195mCA\u001b[0m \u001b[0;30;48;2;166;216;84m##PI\u001b[0m \u001b[0;30;48;2;255;217;47m##TA\u001b[0m \u001b[0;30;48;2;102;194;165m##L\u001b[0m \u001b[0;30;48;2;252;141;98m##I\u001b[0m \u001b[0;30;48;2;141;160;203m##Z\u001b[0m \u001b[0;30;48;2;231;138;195m##AT\u001b[0m \u001b[0;30;48;2;166;216;84m##ION\u001b[0m \u001b[0;30;48;2;255;217;47m[UNK]\u001b[0m \u001b[0;30;48;2;102;194;165m[UNK]\u001b[0m \u001b[0;30;48;2;252;141;98mshow\u001b[0m \u001b[0;30;48;2;141;160;203m_\u001b[0m \u001b[0;30;48;2;231;138;195mtoken\u001b[0m \u001b[0;30;48;2;166;216;84m##s\u001b[0m \u001b[0;30;48;2;255;217;47mF\u001b[0m \u001b[0;30;48;2;102;194;165m##als\u001b[0m \u001b[0;30;48;2;252;141;98m##e\u001b[0m \u001b[0;30;48;2;141;160;203mNone\u001b[0m \u001b[0;30;48;2;231;138;195mel\u001b[0m \u001b[0;30;48;2;166;216;84m##if\u001b[0m \u001b[0;30;48;2;255;217;47m=\u001b[0m \u001b[0;30;48;2;102;194;165m=\u001b[0m \u001b[0;30;48;2;252;141;98m>\u001b[0m \u001b[0;30;48;2;141;160;203m=\u001b[0m \u001b[0;30;48;2;231;138;195melse\u001b[0m \u001b[0;30;48;2;166;216;84m:\u001b[0m \u001b[0;30;48;2;255;217;47mtwo\u001b[0m \u001b[0;30;48;2;102;194;165mta\u001b[0m \u001b[0;30;48;2;252;141;98m##bs\u001b[0m \u001b[0;30;48;2;141;160;203m:\u001b[0m \u001b[0;30;48;2;231;138;195m\"\u001b[0m \u001b[0;30;48;2;166;216;84m\"\u001b[0m \u001b[0;30;48;2;255;217;47mfour\u001b[0m \u001b[0;30;48;2;102;194;165mspaces\u001b[0m \u001b[0;30;48;2;252;141;98m:\u001b[0m \u001b[0;30;48;2;141;160;203m\"\u001b[0m \u001b[0;30;48;2;231;138;195m\"\u001b[0m \u001b[0;30;48;2;166;216;84m12\u001b[0m \u001b[0;30;48;2;255;217;47m.\u001b[0m \u001b[0;30;48;2;102;194;165m0\u001b[0m \u001b[0;30;48;2;252;141;98m*\u001b[0m \u001b[0;30;48;2;141;160;203m50\u001b[0m \u001b[0;30;48;2;231;138;195m=\u001b[0m \u001b[0;30;48;2;166;216;84m600\u001b[0m \u001b[0;30;48;2;255;217;47m[SEP]\u001b[0m "
          ]
        }
      ],
      "source": [
        "show_tokens(text, \"bert-base-cased\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **GPT-2(2019)**\n",
        "    - 토큰화 방법 : BPE\n",
        "    - 어휘사전 크기 : 50,257\n",
        "    - 특수 토큰:\n",
        "        - <|endoftext|>\n",
        "    - 특징: 줄바꿈이 토크나이저 내에서 표현됨"
      ],
      "metadata": {
        "id": "Gm9BFfkgoQL1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "K_k5QduY5H0u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "742fc74e-77af-4a6e-d209-0af8c1d7441a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0;30;48;2;102;194;165m\n",
            "\u001b[0m \u001b[0;30;48;2;252;141;98mEnglish\u001b[0m \u001b[0;30;48;2;141;160;203m and\u001b[0m \u001b[0;30;48;2;231;138;195m CAP\u001b[0m \u001b[0;30;48;2;166;216;84mITAL\u001b[0m \u001b[0;30;48;2;255;217;47mIZ\u001b[0m \u001b[0;30;48;2;102;194;165mATION\u001b[0m \u001b[0;30;48;2;252;141;98m\n",
            "\u001b[0m \u001b[0;30;48;2;141;160;203m�\u001b[0m \u001b[0;30;48;2;231;138;195m�\u001b[0m \u001b[0;30;48;2;166;216;84m�\u001b[0m \u001b[0;30;48;2;255;217;47m �\u001b[0m \u001b[0;30;48;2;102;194;165m�\u001b[0m \u001b[0;30;48;2;252;141;98m�\u001b[0m \u001b[0;30;48;2;141;160;203m\n",
            "\u001b[0m \u001b[0;30;48;2;231;138;195mshow\u001b[0m \u001b[0;30;48;2;166;216;84m_\u001b[0m \u001b[0;30;48;2;255;217;47mt\u001b[0m \u001b[0;30;48;2;102;194;165mok\u001b[0m \u001b[0;30;48;2;252;141;98mens\u001b[0m \u001b[0;30;48;2;141;160;203m False\u001b[0m \u001b[0;30;48;2;231;138;195m None\u001b[0m \u001b[0;30;48;2;166;216;84m el\u001b[0m \u001b[0;30;48;2;255;217;47mif\u001b[0m \u001b[0;30;48;2;102;194;165m ==\u001b[0m \u001b[0;30;48;2;252;141;98m >=\u001b[0m \u001b[0;30;48;2;141;160;203m else\u001b[0m \u001b[0;30;48;2;231;138;195m:\u001b[0m \u001b[0;30;48;2;166;216;84m two\u001b[0m \u001b[0;30;48;2;255;217;47m tabs\u001b[0m \u001b[0;30;48;2;102;194;165m:\"\u001b[0m \u001b[0;30;48;2;252;141;98m\t\u001b[0m \u001b[0;30;48;2;141;160;203m\t\u001b[0m \u001b[0;30;48;2;231;138;195m\"\u001b[0m \u001b[0;30;48;2;166;216;84m four\u001b[0m \u001b[0;30;48;2;255;217;47m spaces\u001b[0m \u001b[0;30;48;2;102;194;165m:\"\u001b[0m \u001b[0;30;48;2;252;141;98m \u001b[0m \u001b[0;30;48;2;141;160;203m \u001b[0m \u001b[0;30;48;2;231;138;195m \u001b[0m \u001b[0;30;48;2;166;216;84m \"\u001b[0m \u001b[0;30;48;2;255;217;47m\n",
            "\u001b[0m \u001b[0;30;48;2;102;194;165m12\u001b[0m \u001b[0;30;48;2;252;141;98m.\u001b[0m \u001b[0;30;48;2;141;160;203m0\u001b[0m \u001b[0;30;48;2;231;138;195m*\u001b[0m \u001b[0;30;48;2;166;216;84m50\u001b[0m \u001b[0;30;48;2;255;217;47m=\u001b[0m \u001b[0;30;48;2;102;194;165m600\u001b[0m \u001b[0;30;48;2;252;141;98m\n",
            "\u001b[0m "
          ]
        }
      ],
      "source": [
        "show_tokens(text, \"gpt2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Flan-T5(2022)**\n",
        "    - 토큰화 방법 : SentencePiece\n",
        "    - 어휘사전 크기 : 32,100\n",
        "    - 특수 토큰:\n",
        "        - `<unk>` unk_token\n",
        "        - `<pad>` pad_token\n",
        "    - 특징 :\n",
        "        - 줄바꿈이나 공백 토큰이 없음 --> 모델이 코드를 다루기 어렵다.\n",
        "        - 이모자와 한자가 모두 <unk> 토큰으로 바꾸었음 --> 모델이 이런 토큰을 식별하지 못함"
      ],
      "metadata": {
        "id": "G0CUY7TYpNsh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "EJn5nf3c5H2_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0215c77-24e4-4b7b-ab72-5385f8acce48"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0;30;48;2;102;194;165mEnglish\u001b[0m \u001b[0;30;48;2;252;141;98mand\u001b[0m \u001b[0;30;48;2;141;160;203mCA\u001b[0m \u001b[0;30;48;2;231;138;195mPI\u001b[0m \u001b[0;30;48;2;166;216;84mTAL\u001b[0m \u001b[0;30;48;2;255;217;47mIZ\u001b[0m \u001b[0;30;48;2;102;194;165mATION\u001b[0m \u001b[0;30;48;2;252;141;98m\u001b[0m \u001b[0;30;48;2;141;160;203m<unk>\u001b[0m \u001b[0;30;48;2;231;138;195m\u001b[0m \u001b[0;30;48;2;166;216;84m<unk>\u001b[0m \u001b[0;30;48;2;255;217;47mshow\u001b[0m \u001b[0;30;48;2;102;194;165m_\u001b[0m \u001b[0;30;48;2;252;141;98mto\u001b[0m \u001b[0;30;48;2;141;160;203mken\u001b[0m \u001b[0;30;48;2;231;138;195ms\u001b[0m \u001b[0;30;48;2;166;216;84mFal\u001b[0m \u001b[0;30;48;2;255;217;47ms\u001b[0m \u001b[0;30;48;2;102;194;165me\u001b[0m \u001b[0;30;48;2;252;141;98mNone\u001b[0m \u001b[0;30;48;2;141;160;203m\u001b[0m \u001b[0;30;48;2;231;138;195me\u001b[0m \u001b[0;30;48;2;166;216;84ml\u001b[0m \u001b[0;30;48;2;255;217;47mif\u001b[0m \u001b[0;30;48;2;102;194;165m=\u001b[0m \u001b[0;30;48;2;252;141;98m=\u001b[0m \u001b[0;30;48;2;141;160;203m>\u001b[0m \u001b[0;30;48;2;231;138;195m=\u001b[0m \u001b[0;30;48;2;166;216;84melse\u001b[0m \u001b[0;30;48;2;255;217;47m:\u001b[0m \u001b[0;30;48;2;102;194;165mtwo\u001b[0m \u001b[0;30;48;2;252;141;98mtab\u001b[0m \u001b[0;30;48;2;141;160;203ms\u001b[0m \u001b[0;30;48;2;231;138;195m:\u001b[0m \u001b[0;30;48;2;166;216;84m\"\u001b[0m \u001b[0;30;48;2;255;217;47m\"\u001b[0m \u001b[0;30;48;2;102;194;165mfour\u001b[0m \u001b[0;30;48;2;252;141;98mspaces\u001b[0m \u001b[0;30;48;2;141;160;203m:\u001b[0m \u001b[0;30;48;2;231;138;195m\"\u001b[0m \u001b[0;30;48;2;166;216;84m\"\u001b[0m \u001b[0;30;48;2;255;217;47m12.\u001b[0m \u001b[0;30;48;2;102;194;165m0\u001b[0m \u001b[0;30;48;2;252;141;98m*\u001b[0m \u001b[0;30;48;2;141;160;203m50\u001b[0m \u001b[0;30;48;2;231;138;195m=\u001b[0m \u001b[0;30;48;2;166;216;84m600\u001b[0m \u001b[0;30;48;2;255;217;47m\u001b[0m \u001b[0;30;48;2;102;194;165m</s>\u001b[0m "
          ]
        }
      ],
      "source": [
        "show_tokens(text, \"google/flan-t5-small\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **GPT-4(2023)**\n",
        "    - 토큰화 방법 : BPE\n",
        "    - 어휘사전 크기 : 100,000 이상\n",
        "    - 특수 토큰:\n",
        "        - `<endoftext>`\n",
        "        - 중간 토큰을 채우도록 훈련됨. 세 개의 특수 토큰을 사용해 앞, 뒤에 나오는 텍스트를 고려해 LLM이 완성된 문장을 생성함\n",
        "            - <|fim_prefix|>\n",
        "            - <|fim_middle|>\n",
        "            - <|fim_suffix|>\n",
        "    - 특징 :\n",
        "        - GPT-2 토크나이저와 비슷하게 동작\n",
        "        - GPT-4는 4개의 공백을 하나의 토큰으로 표현\n",
        "        - elif를 하나의 토큰으로 표현 --> 자연어외에 코드에 초점을 맞추고 있음\n",
        "        - 더 적은 토큰을 사용해 대부분의 단어를 표현함(CAPITALIZATION 두 개의 토큰)"
      ],
      "metadata": {
        "id": "bxZ45wZoqYBo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "1ymhAsTg5H5e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a43c3fe-8b5b-4c63-89f7-f8ec80762a38"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0;30;48;2;102;194;165m\n",
            "\u001b[0m \u001b[0;30;48;2;252;141;98mEnglish\u001b[0m \u001b[0;30;48;2;141;160;203m and\u001b[0m \u001b[0;30;48;2;231;138;195m CAPITAL\u001b[0m \u001b[0;30;48;2;166;216;84mIZATION\u001b[0m \u001b[0;30;48;2;255;217;47m\n",
            "\u001b[0m \u001b[0;30;48;2;102;194;165m�\u001b[0m \u001b[0;30;48;2;252;141;98m�\u001b[0m \u001b[0;30;48;2;141;160;203m�\u001b[0m \u001b[0;30;48;2;231;138;195m �\u001b[0m \u001b[0;30;48;2;166;216;84m�\u001b[0m \u001b[0;30;48;2;255;217;47m�\u001b[0m \u001b[0;30;48;2;102;194;165m\n",
            "\u001b[0m \u001b[0;30;48;2;252;141;98mshow\u001b[0m \u001b[0;30;48;2;141;160;203m_tokens\u001b[0m \u001b[0;30;48;2;231;138;195m False\u001b[0m \u001b[0;30;48;2;166;216;84m None\u001b[0m \u001b[0;30;48;2;255;217;47m elif\u001b[0m \u001b[0;30;48;2;102;194;165m ==\u001b[0m \u001b[0;30;48;2;252;141;98m >=\u001b[0m \u001b[0;30;48;2;141;160;203m else\u001b[0m \u001b[0;30;48;2;231;138;195m:\u001b[0m \u001b[0;30;48;2;166;216;84m two\u001b[0m \u001b[0;30;48;2;255;217;47m tabs\u001b[0m \u001b[0;30;48;2;102;194;165m:\"\u001b[0m \u001b[0;30;48;2;252;141;98m\t\u001b[0m \u001b[0;30;48;2;141;160;203m\t\u001b[0m \u001b[0;30;48;2;231;138;195m\"\u001b[0m \u001b[0;30;48;2;166;216;84m four\u001b[0m \u001b[0;30;48;2;255;217;47m spaces\u001b[0m \u001b[0;30;48;2;102;194;165m:\"\u001b[0m \u001b[0;30;48;2;252;141;98m   \u001b[0m \u001b[0;30;48;2;141;160;203m \"\n",
            "\u001b[0m \u001b[0;30;48;2;231;138;195m12\u001b[0m \u001b[0;30;48;2;166;216;84m.\u001b[0m \u001b[0;30;48;2;255;217;47m0\u001b[0m \u001b[0;30;48;2;102;194;165m*\u001b[0m \u001b[0;30;48;2;252;141;98m50\u001b[0m \u001b[0;30;48;2;141;160;203m=\u001b[0m \u001b[0;30;48;2;231;138;195m600\u001b[0m \u001b[0;30;48;2;166;216;84m\n",
            "\u001b[0m "
          ]
        }
      ],
      "source": [
        "# 공식 토크나이저는 `tiktoken`이지만 허깅 페이스 플랫폼에 동일한 토크나이저가 있습니다.\n",
        "show_tokens(text, \"Xenova/gpt-4\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **StarCoder2(2024)**\n",
        "    - 코드 생성에 초점을 맞춘 150개의 파라미터를 가진 디코더 모델\n",
        "    - 토큰화 방법 : BPE\n",
        "    - 어휘사전 크기 : 49,152\n",
        "    - 특수 토큰:\n",
        "        - `<endoftext>`\n",
        "        - 중간 채우기를 위한 토큰: `<fim_prefix>`, `<fim_middle>`, `<fim_suffix>`, `<fim_pad>`\n",
        "        - `<filename>`, `<reponame>`, `<gh_stars>`\n",
        "    - 특징 :\n",
        "        - 코드를 표현할 때 문맥 관리가 중요(예를 들어 파일에서 다른 파일에 정의된 함수를 호출하는 경우)\n",
        "        - 모델은 같은 저장소의 다른 파일에 있는 코드를 식별하고 다른 저장소에 있는 코드와 구분할 수 있어야함"
      ],
      "metadata": {
        "id": "iqgzgVfSsW7q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "3_vAyeTy5H7_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c3b279b-11c6-4ecd-e0d3-c658d3d62ef8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0;30;48;2;102;194;165m\n",
            "\u001b[0m \u001b[0;30;48;2;252;141;98mEnglish\u001b[0m \u001b[0;30;48;2;141;160;203m and\u001b[0m \u001b[0;30;48;2;231;138;195m CAPITAL\u001b[0m \u001b[0;30;48;2;166;216;84mIZATION\u001b[0m \u001b[0;30;48;2;255;217;47m\n",
            "\u001b[0m \u001b[0;30;48;2;102;194;165m�\u001b[0m \u001b[0;30;48;2;252;141;98m�\u001b[0m \u001b[0;30;48;2;141;160;203m�\u001b[0m \u001b[0;30;48;2;231;138;195m \u001b[0m \u001b[0;30;48;2;166;216;84m�\u001b[0m \u001b[0;30;48;2;255;217;47m�\u001b[0m \u001b[0;30;48;2;102;194;165m\n",
            "\u001b[0m \u001b[0;30;48;2;252;141;98mshow\u001b[0m \u001b[0;30;48;2;141;160;203m_\u001b[0m \u001b[0;30;48;2;231;138;195mtokens\u001b[0m \u001b[0;30;48;2;166;216;84m False\u001b[0m \u001b[0;30;48;2;255;217;47m None\u001b[0m \u001b[0;30;48;2;102;194;165m elif\u001b[0m \u001b[0;30;48;2;252;141;98m ==\u001b[0m \u001b[0;30;48;2;141;160;203m >=\u001b[0m \u001b[0;30;48;2;231;138;195m else\u001b[0m \u001b[0;30;48;2;166;216;84m:\u001b[0m \u001b[0;30;48;2;255;217;47m two\u001b[0m \u001b[0;30;48;2;102;194;165m tabs\u001b[0m \u001b[0;30;48;2;252;141;98m:\"\u001b[0m \u001b[0;30;48;2;141;160;203m\t\u001b[0m \u001b[0;30;48;2;231;138;195m\t\u001b[0m \u001b[0;30;48;2;166;216;84m\"\u001b[0m \u001b[0;30;48;2;255;217;47m four\u001b[0m \u001b[0;30;48;2;102;194;165m spaces\u001b[0m \u001b[0;30;48;2;252;141;98m:\"\u001b[0m \u001b[0;30;48;2;141;160;203m   \u001b[0m \u001b[0;30;48;2;231;138;195m \"\u001b[0m \u001b[0;30;48;2;166;216;84m\n",
            "\u001b[0m \u001b[0;30;48;2;255;217;47m1\u001b[0m \u001b[0;30;48;2;102;194;165m2\u001b[0m \u001b[0;30;48;2;252;141;98m.\u001b[0m \u001b[0;30;48;2;141;160;203m0\u001b[0m \u001b[0;30;48;2;231;138;195m*\u001b[0m \u001b[0;30;48;2;166;216;84m5\u001b[0m \u001b[0;30;48;2;255;217;47m0\u001b[0m \u001b[0;30;48;2;102;194;165m=\u001b[0m \u001b[0;30;48;2;252;141;98m6\u001b[0m \u001b[0;30;48;2;141;160;203m0\u001b[0m \u001b[0;30;48;2;231;138;195m0\u001b[0m \u001b[0;30;48;2;166;216;84m\n",
            "\u001b[0m "
          ]
        }
      ],
      "source": [
        "show_tokens(text, \"bigcode/starcoder2-15b\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Galatica**\n",
        "\n",
        "    - 과학 지식에 초점을 맞추어 많은 과학 논문, 참고 자료, 지식 데이터에서 훈련됨.\n",
        "    - 토큰화에 더 주의를 기울여 데이터셋에 있는 뉘앙스에 민감함 --> 인요, 추론, 수학, 펩타이드 서열, DNA 서열을 위한 특수 토큰이 있음\n",
        "    -\n",
        "    - 토큰화 방법 : BPE\n",
        "    - 어휘사전 크기 : 50,000\n",
        "    - 특수 토큰:\n",
        "        - `<s>`\n",
        "        - `<pad>`\n",
        "        - `</s>`,\n",
        "        - `<unk>`\n",
        "        - 참조/인용은 [START_REF]와 [END_REF]로 감쌈\n",
        "        - 단계별 추론: <work>는 모델이 CoT(chain-of-thought)추론에 사용하는 토큰\n",
        "    - 특징 :\n",
        "        - 코드를 염두에 둠, StarCoder2와 비슷하게 동작\n",
        "        - 탭도 하나의 토큰으로 인코딩"
      ],
      "metadata": {
        "id": "XlLf_HfNuW0T"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "KeWcUdxY6I3u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8b19a74-380e-4d42-f7c0-3376f85c807f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0;30;48;2;102;194;165m\n",
            "\u001b[0m \u001b[0;30;48;2;252;141;98mEnglish\u001b[0m \u001b[0;30;48;2;141;160;203m and\u001b[0m \u001b[0;30;48;2;231;138;195m CAP\u001b[0m \u001b[0;30;48;2;166;216;84mITAL\u001b[0m \u001b[0;30;48;2;255;217;47mIZATION\u001b[0m \u001b[0;30;48;2;102;194;165m\n",
            "\u001b[0m \u001b[0;30;48;2;252;141;98m�\u001b[0m \u001b[0;30;48;2;141;160;203m�\u001b[0m \u001b[0;30;48;2;231;138;195m�\u001b[0m \u001b[0;30;48;2;166;216;84m�\u001b[0m \u001b[0;30;48;2;255;217;47m �\u001b[0m \u001b[0;30;48;2;102;194;165m�\u001b[0m \u001b[0;30;48;2;252;141;98m�\u001b[0m \u001b[0;30;48;2;141;160;203m\n",
            "\u001b[0m \u001b[0;30;48;2;231;138;195mshow\u001b[0m \u001b[0;30;48;2;166;216;84m_\u001b[0m \u001b[0;30;48;2;255;217;47mtokens\u001b[0m \u001b[0;30;48;2;102;194;165m False\u001b[0m \u001b[0;30;48;2;252;141;98m None\u001b[0m \u001b[0;30;48;2;141;160;203m elif\u001b[0m \u001b[0;30;48;2;231;138;195m \u001b[0m \u001b[0;30;48;2;166;216;84m==\u001b[0m \u001b[0;30;48;2;255;217;47m \u001b[0m \u001b[0;30;48;2;102;194;165m>\u001b[0m \u001b[0;30;48;2;252;141;98m=\u001b[0m \u001b[0;30;48;2;141;160;203m else\u001b[0m \u001b[0;30;48;2;231;138;195m:\u001b[0m \u001b[0;30;48;2;166;216;84m two\u001b[0m \u001b[0;30;48;2;255;217;47m t\u001b[0m \u001b[0;30;48;2;102;194;165mabs\u001b[0m \u001b[0;30;48;2;252;141;98m:\u001b[0m \u001b[0;30;48;2;141;160;203m\"\u001b[0m \u001b[0;30;48;2;231;138;195m\t\t\u001b[0m \u001b[0;30;48;2;166;216;84m\"\u001b[0m \u001b[0;30;48;2;255;217;47m four\u001b[0m \u001b[0;30;48;2;102;194;165m spaces\u001b[0m \u001b[0;30;48;2;252;141;98m:\u001b[0m \u001b[0;30;48;2;141;160;203m\"\u001b[0m \u001b[0;30;48;2;231;138;195m    \u001b[0m \u001b[0;30;48;2;166;216;84m\"\u001b[0m \u001b[0;30;48;2;255;217;47m\n",
            "\u001b[0m \u001b[0;30;48;2;102;194;165m1\u001b[0m \u001b[0;30;48;2;252;141;98m2\u001b[0m \u001b[0;30;48;2;141;160;203m.\u001b[0m \u001b[0;30;48;2;231;138;195m0\u001b[0m \u001b[0;30;48;2;166;216;84m*\u001b[0m \u001b[0;30;48;2;255;217;47m5\u001b[0m \u001b[0;30;48;2;102;194;165m0\u001b[0m \u001b[0;30;48;2;252;141;98m=\u001b[0m \u001b[0;30;48;2;141;160;203m6\u001b[0m \u001b[0;30;48;2;231;138;195m0\u001b[0m \u001b[0;30;48;2;166;216;84m0\u001b[0m \u001b[0;30;48;2;255;217;47m\n",
            "\u001b[0m "
          ]
        }
      ],
      "source": [
        "show_tokens(text, \"facebook/galactica-1.3b\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Phi-3(Llama 2)**\n",
        "    - 여러 개의 특수 토큰을 추가한 Llama 2 토크나이저 재사용\n",
        "    -\n",
        "    - 토큰화 방법 : BPE\n",
        "    - 어휘사전 크기 : 32,000\n",
        "    - 특수 토큰:\n",
        "        - `<|endoftext|>`\n",
        "        - 채팅토큰\n",
        "            - `<|user|>`,\n",
        "            - `<|assistant|>`\n",
        "            - `<|system|>`\n",
        "    - 특징 :\n",
        "        - 채팅에 초점을 맞춤"
      ],
      "metadata": {
        "id": "f-Ua2aSSwb7F"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "__QNj2Cohzz2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "083292a9-ad63-466b-9560-da5f9b8e2c40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0;30;48;2;102;194;165m\u001b[0m \u001b[0;30;48;2;252;141;98m\n",
            "\u001b[0m \u001b[0;30;48;2;141;160;203mEnglish\u001b[0m \u001b[0;30;48;2;231;138;195mand\u001b[0m \u001b[0;30;48;2;166;216;84mC\u001b[0m \u001b[0;30;48;2;255;217;47mAP\u001b[0m \u001b[0;30;48;2;102;194;165mIT\u001b[0m \u001b[0;30;48;2;252;141;98mAL\u001b[0m \u001b[0;30;48;2;141;160;203mIZ\u001b[0m \u001b[0;30;48;2;231;138;195mATION\u001b[0m \u001b[0;30;48;2;166;216;84m\n",
            "\u001b[0m \u001b[0;30;48;2;255;217;47m�\u001b[0m \u001b[0;30;48;2;102;194;165m�\u001b[0m \u001b[0;30;48;2;252;141;98m�\u001b[0m \u001b[0;30;48;2;141;160;203m�\u001b[0m \u001b[0;30;48;2;231;138;195m\u001b[0m \u001b[0;30;48;2;166;216;84m�\u001b[0m \u001b[0;30;48;2;255;217;47m�\u001b[0m \u001b[0;30;48;2;102;194;165m�\u001b[0m \u001b[0;30;48;2;252;141;98m\n",
            "\u001b[0m \u001b[0;30;48;2;141;160;203mshow\u001b[0m \u001b[0;30;48;2;231;138;195m_\u001b[0m \u001b[0;30;48;2;166;216;84mto\u001b[0m \u001b[0;30;48;2;255;217;47mkens\u001b[0m \u001b[0;30;48;2;102;194;165mFalse\u001b[0m \u001b[0;30;48;2;252;141;98mNone\u001b[0m \u001b[0;30;48;2;141;160;203melif\u001b[0m \u001b[0;30;48;2;231;138;195m==\u001b[0m \u001b[0;30;48;2;166;216;84m>=\u001b[0m \u001b[0;30;48;2;255;217;47melse\u001b[0m \u001b[0;30;48;2;102;194;165m:\u001b[0m \u001b[0;30;48;2;252;141;98mtwo\u001b[0m \u001b[0;30;48;2;141;160;203mtabs\u001b[0m \u001b[0;30;48;2;231;138;195m:\"\u001b[0m \u001b[0;30;48;2;166;216;84m\t\u001b[0m \u001b[0;30;48;2;255;217;47m\t\u001b[0m \u001b[0;30;48;2;102;194;165m\"\u001b[0m \u001b[0;30;48;2;252;141;98mfour\u001b[0m \u001b[0;30;48;2;141;160;203mspaces\u001b[0m \u001b[0;30;48;2;231;138;195m:\"\u001b[0m \u001b[0;30;48;2;166;216;84m  \u001b[0m \u001b[0;30;48;2;255;217;47m\"\u001b[0m \u001b[0;30;48;2;102;194;165m\n",
            "\u001b[0m \u001b[0;30;48;2;252;141;98m1\u001b[0m \u001b[0;30;48;2;141;160;203m2\u001b[0m \u001b[0;30;48;2;231;138;195m.\u001b[0m \u001b[0;30;48;2;166;216;84m0\u001b[0m \u001b[0;30;48;2;255;217;47m*\u001b[0m \u001b[0;30;48;2;102;194;165m5\u001b[0m \u001b[0;30;48;2;252;141;98m0\u001b[0m \u001b[0;30;48;2;141;160;203m=\u001b[0m \u001b[0;30;48;2;231;138;195m6\u001b[0m \u001b[0;30;48;2;166;216;84m0\u001b[0m \u001b[0;30;48;2;255;217;47m0\u001b[0m \u001b[0;30;48;2;102;194;165m\n",
            "\u001b[0m "
          ]
        }
      ],
      "source": [
        "show_tokens(text, \"microsoft/Phi-3-mini-4k-instruct\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "fyUC440f78ZS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **한국어 특화된 토크나이저**"
      ],
      "metadata": {
        "id": "cYujEh7i7Zk_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **한글의 특수성**:\n",
        "    - **교착어**: 조사가 붙어 단어 형태가 무한대로 변형 (\"학교\", \"학교가\", \"학교에서\", \"학교로부터\"...)\n",
        "    - **띄어쓰기 불규칙**: SNS, 댓글 등에서 띄어쓰기가 일관되지 않음\n",
        "    - **자모 조합**: 초성+중성+종성이 결합되어 하나의 음절 형성 (ex: ㄱ+ㅏ+ㅁ = 감)\n",
        "\n",
        "- 발전 과정:\n",
        "    - 2013-2016: KoNLPy, 형태소 분석기 (Mecab, Okt) 등장\n",
        "    - 2018: 다국어 BERT에 한국어 포함되었으나 성능 부족\n",
        "    - 2020: KoBERT (SKT) 출시 - 한국어 Wikipedia로 학습\n",
        "    - 2021: KoGPT, KoBART 등 한국어 특화 모델 등장\n",
        "    - 2022-2024: KoAlpaca, Polyglot-Ko, KULLM 등 다양한 한국어 LLM 개발\n",
        "\n",
        "- 주요 참고자료:\n",
        "    - SKT KoBERT: https://github.com/SKTBrain/KoBERT\n",
        "    - Kakao KoGPT: https://github.com/kakaobrain/kogpt"
      ],
      "metadata": {
        "id": "YmHB-GYj7fVy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **[미션]** 한국어 LLM 찾아 토큰화 해보기.\n",
        "- 허깅페이스에서 사전학습된 한국어 LLM을 여러 개(2개 이상) 찾고 위와 같이 토큰화 & 비교해보세요."
      ],
      "metadata": {
        "id": "vuXMBfD9F2vc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ko_text = \"\"\"\n",
        "    안녕하세요! 저는 인공지능을 공부하고 있습니다.,\n",
        "    한글은 세종대왕이 창제하신 문자입니다. 😊,\n",
        "    카카오와 네이버는 한국의 대표적인 IT 기업입니다.,\n",
        "    자연어처리(NLP)는 컴퓨터가 인간의 언어를 이해하도록 하는 기술이다.,\n",
        "\"\"\"\n",
        "\n",
        "# 예제 : SKT의 KoBERT - 비교적 안정적\n",
        "show_tokens(ko_text, \"skt/kobert-base-v1\")"
      ],
      "metadata": {
        "id": "c1mnnX3D_BWA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a3261dc-9e7e-4340-b510-d2930178e5ab"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0;30;48;2;102;194;165m\u001b[0m \u001b[0;30;48;2;252;141;98mᄋ\u001b[0m \u001b[0;30;48;2;141;160;203m[UNK]\u001b[0m \u001b[0;30;48;2;231;138;195mᄒ\u001b[0m \u001b[0;30;48;2;166;216;84m[UNK]\u001b[0m \u001b[0;30;48;2;255;217;47mᄋ\u001b[0m \u001b[0;30;48;2;102;194;165m[UNK]\u001b[0m \u001b[0;30;48;2;252;141;98m!\u001b[0m \u001b[0;30;48;2;141;160;203m\u001b[0m \u001b[0;30;48;2;231;138;195m[UNK]\u001b[0m \u001b[0;30;48;2;166;216;84m\u001b[0m \u001b[0;30;48;2;255;217;47mᄋ\u001b[0m \u001b[0;30;48;2;102;194;165mᅵ\u001b[0m \u001b[0;30;48;2;252;141;98m[UNK]\u001b[0m \u001b[0;30;48;2;141;160;203mᄀ\u001b[0m \u001b[0;30;48;2;231;138;195m[UNK]\u001b[0m \u001b[0;30;48;2;166;216;84mᅵ\u001b[0m \u001b[0;30;48;2;255;217;47m[UNK]\u001b[0m \u001b[0;30;48;2;102;194;165mᄋ\u001b[0m \u001b[0;30;48;2;252;141;98m[UNK]\u001b[0m \u001b[0;30;48;2;141;160;203m\u001b[0m \u001b[0;30;48;2;231;138;195mᄀ\u001b[0m \u001b[0;30;48;2;166;216;84m[UNK]\u001b[0m \u001b[0;30;48;2;255;217;47mᄒ\u001b[0m \u001b[0;30;48;2;102;194;165m[UNK]\u001b[0m \u001b[0;30;48;2;252;141;98mᄀ\u001b[0m \u001b[0;30;48;2;141;160;203m[UNK]\u001b[0m \u001b[0;30;48;2;231;138;195m\u001b[0m \u001b[0;30;48;2;166;216;84mᄋ\u001b[0m \u001b[0;30;48;2;255;217;47mᅵ\u001b[0m \u001b[0;30;48;2;102;194;165m[UNK]\u001b[0m \u001b[0;30;48;2;252;141;98mᅵ\u001b[0m \u001b[0;30;48;2;141;160;203m[UNK]\u001b[0m \u001b[0;30;48;2;231;138;195m.\u001b[0m \u001b[0;30;48;2;166;216;84m,\u001b[0m \u001b[0;30;48;2;255;217;47m\u001b[0m \u001b[0;30;48;2;102;194;165mᄒ\u001b[0m \u001b[0;30;48;2;252;141;98m[UNK]\u001b[0m \u001b[0;30;48;2;141;160;203mᄀ\u001b[0m \u001b[0;30;48;2;231;138;195m[UNK]\u001b[0m \u001b[0;30;48;2;166;216;84mᄋ\u001b[0m \u001b[0;30;48;2;255;217;47m[UNK]\u001b[0m \u001b[0;30;48;2;102;194;165m\u001b[0m \u001b[0;30;48;2;252;141;98m[UNK]\u001b[0m \u001b[0;30;48;2;141;160;203mᄋ\u001b[0m \u001b[0;30;48;2;231;138;195m[UNK]\u001b[0m \u001b[0;30;48;2;166;216;84mᄋ\u001b[0m \u001b[0;30;48;2;255;217;47mᅵ\u001b[0m \u001b[0;30;48;2;102;194;165m\u001b[0m \u001b[0;30;48;2;252;141;98m[UNK]\u001b[0m \u001b[0;30;48;2;141;160;203mᄒ\u001b[0m \u001b[0;30;48;2;231;138;195m[UNK]\u001b[0m \u001b[0;30;48;2;166;216;84mᅵ\u001b[0m \u001b[0;30;48;2;255;217;47m[UNK]\u001b[0m \u001b[0;30;48;2;102;194;165m\u001b[0m \u001b[0;30;48;2;252;141;98m[UNK]\u001b[0m \u001b[0;30;48;2;141;160;203mᄋ\u001b[0m \u001b[0;30;48;2;231;138;195mᅵ\u001b[0m \u001b[0;30;48;2;166;216;84m[UNK]\u001b[0m \u001b[0;30;48;2;255;217;47mᅵ\u001b[0m \u001b[0;30;48;2;102;194;165m[UNK]\u001b[0m \u001b[0;30;48;2;252;141;98m.\u001b[0m \u001b[0;30;48;2;141;160;203m\u001b[0m \u001b[0;30;48;2;231;138;195m[UNK]\u001b[0m \u001b[0;30;48;2;166;216;84m,\u001b[0m \u001b[0;30;48;2;255;217;47m\u001b[0m \u001b[0;30;48;2;102;194;165mᄏ\u001b[0m \u001b[0;30;48;2;252;141;98m[UNK]\u001b[0m \u001b[0;30;48;2;141;160;203mᄏ\u001b[0m \u001b[0;30;48;2;231;138;195m[UNK]\u001b[0m \u001b[0;30;48;2;166;216;84mᄋ\u001b[0m \u001b[0;30;48;2;255;217;47m[UNK]\u001b[0m \u001b[0;30;48;2;102;194;165mᄋ\u001b[0m \u001b[0;30;48;2;252;141;98m[UNK]\u001b[0m \u001b[0;30;48;2;141;160;203m\u001b[0m \u001b[0;30;48;2;231;138;195m[UNK]\u001b[0m \u001b[0;30;48;2;166;216;84mᄋ\u001b[0m \u001b[0;30;48;2;255;217;47mᅵ\u001b[0m \u001b[0;30;48;2;102;194;165m[UNK]\u001b[0m \u001b[0;30;48;2;252;141;98m\u001b[0m \u001b[0;30;48;2;141;160;203mᄒ\u001b[0m \u001b[0;30;48;2;231;138;195m[UNK]\u001b[0m \u001b[0;30;48;2;166;216;84mᄀ\u001b[0m \u001b[0;30;48;2;255;217;47m[UNK]\u001b[0m \u001b[0;30;48;2;102;194;165mᄋ\u001b[0m \u001b[0;30;48;2;252;141;98m[UNK]\u001b[0m \u001b[0;30;48;2;141;160;203m\u001b[0m \u001b[0;30;48;2;231;138;195m[UNK]\u001b[0m \u001b[0;30;48;2;166;216;84mᄋ\u001b[0m \u001b[0;30;48;2;255;217;47mᅵ\u001b[0m \u001b[0;30;48;2;102;194;165m[UNK]\u001b[0m \u001b[0;30;48;2;252;141;98mIT\u001b[0m \u001b[0;30;48;2;141;160;203m\u001b[0m \u001b[0;30;48;2;231;138;195mᄀ\u001b[0m \u001b[0;30;48;2;166;216;84mᅵ\u001b[0m \u001b[0;30;48;2;255;217;47mᄋ\u001b[0m \u001b[0;30;48;2;102;194;165m[UNK]\u001b[0m \u001b[0;30;48;2;252;141;98mᄋ\u001b[0m \u001b[0;30;48;2;141;160;203mᅵ\u001b[0m \u001b[0;30;48;2;231;138;195m[UNK]\u001b[0m \u001b[0;30;48;2;166;216;84mᅵ\u001b[0m \u001b[0;30;48;2;255;217;47m[UNK]\u001b[0m \u001b[0;30;48;2;102;194;165m.\u001b[0m \u001b[0;30;48;2;252;141;98m,\u001b[0m \u001b[0;30;48;2;141;160;203m\u001b[0m \u001b[0;30;48;2;231;138;195m[UNK]\u001b[0m \u001b[0;30;48;2;166;216;84mᄋ\u001b[0m \u001b[0;30;48;2;255;217;47m[UNK]\u001b[0m \u001b[0;30;48;2;102;194;165mᄋ\u001b[0m \u001b[0;30;48;2;252;141;98m[UNK]\u001b[0m \u001b[0;30;48;2;141;160;203mᅵ\u001b[0m \u001b[0;30;48;2;231;138;195m(\u001b[0m \u001b[0;30;48;2;166;216;84mN\u001b[0m \u001b[0;30;48;2;255;217;47mL\u001b[0m \u001b[0;30;48;2;102;194;165mP\u001b[0m \u001b[0;30;48;2;252;141;98m)\u001b[0m \u001b[0;30;48;2;141;160;203m[UNK]\u001b[0m \u001b[0;30;48;2;231;138;195m\u001b[0m \u001b[0;30;48;2;166;216;84mᄏ\u001b[0m \u001b[0;30;48;2;255;217;47m[UNK]\u001b[0m \u001b[0;30;48;2;102;194;165mᄀ\u001b[0m \u001b[0;30;48;2;252;141;98m[UNK]\u001b[0m \u001b[0;30;48;2;141;160;203m\u001b[0m \u001b[0;30;48;2;231;138;195mᄋ\u001b[0m \u001b[0;30;48;2;166;216;84mᅵ\u001b[0m \u001b[0;30;48;2;255;217;47m[UNK]\u001b[0m \u001b[0;30;48;2;102;194;165mᄀ\u001b[0m \u001b[0;30;48;2;252;141;98m[UNK]\u001b[0m \u001b[0;30;48;2;141;160;203mᄋ\u001b[0m \u001b[0;30;48;2;231;138;195m[UNK]\u001b[0m \u001b[0;30;48;2;166;216;84m\u001b[0m \u001b[0;30;48;2;255;217;47mᄋ\u001b[0m \u001b[0;30;48;2;102;194;165m[UNK]\u001b[0m \u001b[0;30;48;2;252;141;98mᄋ\u001b[0m \u001b[0;30;48;2;141;160;203m[UNK]\u001b[0m \u001b[0;30;48;2;231;138;195m\u001b[0m \u001b[0;30;48;2;166;216;84mᄋ\u001b[0m \u001b[0;30;48;2;255;217;47mᅵ\u001b[0m \u001b[0;30;48;2;102;194;165mᄒ\u001b[0m \u001b[0;30;48;2;252;141;98m[UNK]\u001b[0m \u001b[0;30;48;2;141;160;203mᄒ\u001b[0m \u001b[0;30;48;2;231;138;195m[UNK]\u001b[0m \u001b[0;30;48;2;166;216;84m\u001b[0m \u001b[0;30;48;2;255;217;47mᄒ\u001b[0m \u001b[0;30;48;2;102;194;165m[UNK]\u001b[0m \u001b[0;30;48;2;252;141;98m\u001b[0m \u001b[0;30;48;2;141;160;203mᄀ\u001b[0m \u001b[0;30;48;2;231;138;195mᅵ\u001b[0m \u001b[0;30;48;2;166;216;84m[UNK]\u001b[0m \u001b[0;30;48;2;255;217;47mᄋ\u001b[0m \u001b[0;30;48;2;102;194;165mᅵ\u001b[0m \u001b[0;30;48;2;252;141;98m[UNK]\u001b[0m \u001b[0;30;48;2;141;160;203m.\u001b[0m \u001b[0;30;48;2;231;138;195m,\u001b[0m \u001b[0;30;48;2;166;216;84m\u001b[0m \u001b[0;30;48;2;255;217;47m[UNK]\u001b[0m \u001b[0;30;48;2;102;194;165m[UNK]\u001b[0m "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# EleutherAI의 Polyglot-Ko 시리즈\n",
        "show_tokens(ko_text, \"EleutherAI/polyglot-ko-1.3b\")  # Polyglot-Ko-1.3B\n",
        "show_tokens(ko_text, \"EleutherAI/polyglot-ko-5.8b\")  # Polyglot-Ko-5.8B"
      ],
      "metadata": {
        "id": "n_egY3Ex_tH9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60a9e4b4-37ba-4ffa-ca3c-ebba17ca5ed3"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0;30;48;2;102;194;165m\n",
            "\u001b[0m \u001b[0;30;48;2;252;141;98m \u001b[0m \u001b[0;30;48;2;141;160;203m \u001b[0m \u001b[0;30;48;2;231;138;195m \u001b[0m \u001b[0;30;48;2;166;216;84m 안녕\u001b[0m \u001b[0;30;48;2;255;217;47m하\u001b[0m \u001b[0;30;48;2;102;194;165m세요\u001b[0m \u001b[0;30;48;2;252;141;98m!\u001b[0m \u001b[0;30;48;2;141;160;203m 저\u001b[0m \u001b[0;30;48;2;231;138;195m는\u001b[0m \u001b[0;30;48;2;166;216;84m 인공지능\u001b[0m \u001b[0;30;48;2;255;217;47m을\u001b[0m \u001b[0;30;48;2;102;194;165m 공부\u001b[0m \u001b[0;30;48;2;252;141;98m하고\u001b[0m \u001b[0;30;48;2;141;160;203m 있\u001b[0m \u001b[0;30;48;2;231;138;195m습니다\u001b[0m \u001b[0;30;48;2;166;216;84m.\u001b[0m \u001b[0;30;48;2;255;217;47m,\u001b[0m \u001b[0;30;48;2;102;194;165m\n",
            "\u001b[0m \u001b[0;30;48;2;252;141;98m \u001b[0m \u001b[0;30;48;2;141;160;203m \u001b[0m \u001b[0;30;48;2;231;138;195m \u001b[0m \u001b[0;30;48;2;166;216;84m 한글\u001b[0m \u001b[0;30;48;2;255;217;47m은\u001b[0m \u001b[0;30;48;2;102;194;165m 세종\u001b[0m \u001b[0;30;48;2;252;141;98m대왕\u001b[0m \u001b[0;30;48;2;141;160;203m이\u001b[0m \u001b[0;30;48;2;231;138;195m 창\u001b[0m \u001b[0;30;48;2;166;216;84m제\u001b[0m \u001b[0;30;48;2;255;217;47m하\u001b[0m \u001b[0;30;48;2;102;194;165m신\u001b[0m \u001b[0;30;48;2;252;141;98m 문자\u001b[0m \u001b[0;30;48;2;141;160;203m입니다\u001b[0m \u001b[0;30;48;2;231;138;195m.\u001b[0m \u001b[0;30;48;2;166;216;84m \u001b[0m \u001b[0;30;48;2;255;217;47m�\u001b[0m \u001b[0;30;48;2;102;194;165m�\u001b[0m \u001b[0;30;48;2;252;141;98m�\u001b[0m \u001b[0;30;48;2;141;160;203m�\u001b[0m \u001b[0;30;48;2;231;138;195m,\u001b[0m \u001b[0;30;48;2;166;216;84m\n",
            "\u001b[0m \u001b[0;30;48;2;255;217;47m \u001b[0m \u001b[0;30;48;2;102;194;165m \u001b[0m \u001b[0;30;48;2;252;141;98m \u001b[0m \u001b[0;30;48;2;141;160;203m 카카오\u001b[0m \u001b[0;30;48;2;231;138;195m와\u001b[0m \u001b[0;30;48;2;166;216;84m 네이버\u001b[0m \u001b[0;30;48;2;255;217;47m는\u001b[0m \u001b[0;30;48;2;102;194;165m 한국\u001b[0m \u001b[0;30;48;2;252;141;98m의\u001b[0m \u001b[0;30;48;2;141;160;203m 대표\u001b[0m \u001b[0;30;48;2;231;138;195m적\u001b[0m \u001b[0;30;48;2;166;216;84m인\u001b[0m \u001b[0;30;48;2;255;217;47m IT\u001b[0m \u001b[0;30;48;2;102;194;165m 기업\u001b[0m \u001b[0;30;48;2;252;141;98m입니다\u001b[0m \u001b[0;30;48;2;141;160;203m.\u001b[0m \u001b[0;30;48;2;231;138;195m,\u001b[0m \u001b[0;30;48;2;166;216;84m\n",
            "\u001b[0m \u001b[0;30;48;2;255;217;47m \u001b[0m \u001b[0;30;48;2;102;194;165m \u001b[0m \u001b[0;30;48;2;252;141;98m \u001b[0m \u001b[0;30;48;2;141;160;203m 자연\u001b[0m \u001b[0;30;48;2;231;138;195m어\u001b[0m \u001b[0;30;48;2;166;216;84m처리\u001b[0m \u001b[0;30;48;2;255;217;47m(\u001b[0m \u001b[0;30;48;2;102;194;165mNL\u001b[0m \u001b[0;30;48;2;252;141;98mP\u001b[0m \u001b[0;30;48;2;141;160;203m)\u001b[0m \u001b[0;30;48;2;231;138;195m는\u001b[0m \u001b[0;30;48;2;166;216;84m 컴퓨터\u001b[0m \u001b[0;30;48;2;255;217;47m가\u001b[0m \u001b[0;30;48;2;102;194;165m 인간\u001b[0m \u001b[0;30;48;2;252;141;98m의\u001b[0m \u001b[0;30;48;2;141;160;203m 언어\u001b[0m \u001b[0;30;48;2;231;138;195m를\u001b[0m \u001b[0;30;48;2;166;216;84m 이해\u001b[0m \u001b[0;30;48;2;255;217;47m하\u001b[0m \u001b[0;30;48;2;102;194;165m도록\u001b[0m \u001b[0;30;48;2;252;141;98m 하\u001b[0m \u001b[0;30;48;2;141;160;203m는\u001b[0m \u001b[0;30;48;2;231;138;195m 기술\u001b[0m \u001b[0;30;48;2;166;216;84m이\u001b[0m \u001b[0;30;48;2;255;217;47m다\u001b[0m \u001b[0;30;48;2;102;194;165m.\u001b[0m \u001b[0;30;48;2;252;141;98m,\u001b[0m \u001b[0;30;48;2;141;160;203m\n",
            "\u001b[0m \u001b[0;30;48;2;102;194;165m\n",
            "\u001b[0m \u001b[0;30;48;2;252;141;98m \u001b[0m \u001b[0;30;48;2;141;160;203m \u001b[0m \u001b[0;30;48;2;231;138;195m \u001b[0m \u001b[0;30;48;2;166;216;84m 안녕\u001b[0m \u001b[0;30;48;2;255;217;47m하\u001b[0m \u001b[0;30;48;2;102;194;165m세요\u001b[0m \u001b[0;30;48;2;252;141;98m!\u001b[0m \u001b[0;30;48;2;141;160;203m 저\u001b[0m \u001b[0;30;48;2;231;138;195m는\u001b[0m \u001b[0;30;48;2;166;216;84m 인공지능\u001b[0m \u001b[0;30;48;2;255;217;47m을\u001b[0m \u001b[0;30;48;2;102;194;165m 공부\u001b[0m \u001b[0;30;48;2;252;141;98m하고\u001b[0m \u001b[0;30;48;2;141;160;203m 있\u001b[0m \u001b[0;30;48;2;231;138;195m습니다\u001b[0m \u001b[0;30;48;2;166;216;84m.\u001b[0m \u001b[0;30;48;2;255;217;47m,\u001b[0m \u001b[0;30;48;2;102;194;165m\n",
            "\u001b[0m \u001b[0;30;48;2;252;141;98m \u001b[0m \u001b[0;30;48;2;141;160;203m \u001b[0m \u001b[0;30;48;2;231;138;195m \u001b[0m \u001b[0;30;48;2;166;216;84m 한글\u001b[0m \u001b[0;30;48;2;255;217;47m은\u001b[0m \u001b[0;30;48;2;102;194;165m 세종\u001b[0m \u001b[0;30;48;2;252;141;98m대왕\u001b[0m \u001b[0;30;48;2;141;160;203m이\u001b[0m \u001b[0;30;48;2;231;138;195m 창\u001b[0m \u001b[0;30;48;2;166;216;84m제\u001b[0m \u001b[0;30;48;2;255;217;47m하\u001b[0m \u001b[0;30;48;2;102;194;165m신\u001b[0m \u001b[0;30;48;2;252;141;98m 문자\u001b[0m \u001b[0;30;48;2;141;160;203m입니다\u001b[0m \u001b[0;30;48;2;231;138;195m.\u001b[0m \u001b[0;30;48;2;166;216;84m \u001b[0m \u001b[0;30;48;2;255;217;47m�\u001b[0m \u001b[0;30;48;2;102;194;165m�\u001b[0m \u001b[0;30;48;2;252;141;98m�\u001b[0m \u001b[0;30;48;2;141;160;203m�\u001b[0m \u001b[0;30;48;2;231;138;195m,\u001b[0m \u001b[0;30;48;2;166;216;84m\n",
            "\u001b[0m \u001b[0;30;48;2;255;217;47m \u001b[0m \u001b[0;30;48;2;102;194;165m \u001b[0m \u001b[0;30;48;2;252;141;98m \u001b[0m \u001b[0;30;48;2;141;160;203m 카카오\u001b[0m \u001b[0;30;48;2;231;138;195m와\u001b[0m \u001b[0;30;48;2;166;216;84m 네이버\u001b[0m \u001b[0;30;48;2;255;217;47m는\u001b[0m \u001b[0;30;48;2;102;194;165m 한국\u001b[0m \u001b[0;30;48;2;252;141;98m의\u001b[0m \u001b[0;30;48;2;141;160;203m 대표\u001b[0m \u001b[0;30;48;2;231;138;195m적\u001b[0m \u001b[0;30;48;2;166;216;84m인\u001b[0m \u001b[0;30;48;2;255;217;47m IT\u001b[0m \u001b[0;30;48;2;102;194;165m 기업\u001b[0m \u001b[0;30;48;2;252;141;98m입니다\u001b[0m \u001b[0;30;48;2;141;160;203m.\u001b[0m \u001b[0;30;48;2;231;138;195m,\u001b[0m \u001b[0;30;48;2;166;216;84m\n",
            "\u001b[0m \u001b[0;30;48;2;255;217;47m \u001b[0m \u001b[0;30;48;2;102;194;165m \u001b[0m \u001b[0;30;48;2;252;141;98m \u001b[0m \u001b[0;30;48;2;141;160;203m 자연\u001b[0m \u001b[0;30;48;2;231;138;195m어\u001b[0m \u001b[0;30;48;2;166;216;84m처리\u001b[0m \u001b[0;30;48;2;255;217;47m(\u001b[0m \u001b[0;30;48;2;102;194;165mNL\u001b[0m \u001b[0;30;48;2;252;141;98mP\u001b[0m \u001b[0;30;48;2;141;160;203m)\u001b[0m \u001b[0;30;48;2;231;138;195m는\u001b[0m \u001b[0;30;48;2;166;216;84m 컴퓨터\u001b[0m \u001b[0;30;48;2;255;217;47m가\u001b[0m \u001b[0;30;48;2;102;194;165m 인간\u001b[0m \u001b[0;30;48;2;252;141;98m의\u001b[0m \u001b[0;30;48;2;141;160;203m 언어\u001b[0m \u001b[0;30;48;2;231;138;195m를\u001b[0m \u001b[0;30;48;2;166;216;84m 이해\u001b[0m \u001b[0;30;48;2;255;217;47m하\u001b[0m \u001b[0;30;48;2;102;194;165m도록\u001b[0m \u001b[0;30;48;2;252;141;98m 하\u001b[0m \u001b[0;30;48;2;141;160;203m는\u001b[0m \u001b[0;30;48;2;231;138;195m 기술\u001b[0m \u001b[0;30;48;2;166;216;84m이\u001b[0m \u001b[0;30;48;2;255;217;47m다\u001b[0m \u001b[0;30;48;2;102;194;165m.\u001b[0m \u001b[0;30;48;2;252;141;98m,\u001b[0m \u001b[0;30;48;2;141;160;203m\n",
            "\u001b[0m "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Beomi의 KcBERT - 댓글 데이터 특화\n",
        "show_tokens(ko_text, \"beomi/kcbert-base\")"
      ],
      "metadata": {
        "id": "ltpFu7zq_qpE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d02a911a-6ffe-4bed-c547-4b97255e839d"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0;30;48;2;102;194;165m[CLS]\u001b[0m \u001b[0;30;48;2;252;141;98m안녕\u001b[0m \u001b[0;30;48;2;141;160;203m##하세요\u001b[0m \u001b[0;30;48;2;231;138;195m!\u001b[0m \u001b[0;30;48;2;166;216;84m저는\u001b[0m \u001b[0;30;48;2;255;217;47m인공\u001b[0m \u001b[0;30;48;2;102;194;165m##지능\u001b[0m \u001b[0;30;48;2;252;141;98m##을\u001b[0m \u001b[0;30;48;2;141;160;203m공부하고\u001b[0m \u001b[0;30;48;2;231;138;195m있습니다\u001b[0m \u001b[0;30;48;2;166;216;84m.\u001b[0m \u001b[0;30;48;2;255;217;47m,\u001b[0m \u001b[0;30;48;2;102;194;165m한글\u001b[0m \u001b[0;30;48;2;252;141;98m##은\u001b[0m \u001b[0;30;48;2;141;160;203m세종대왕\u001b[0m \u001b[0;30;48;2;231;138;195m##이\u001b[0m \u001b[0;30;48;2;166;216;84m창\u001b[0m \u001b[0;30;48;2;255;217;47m##제\u001b[0m \u001b[0;30;48;2;102;194;165m##하신\u001b[0m \u001b[0;30;48;2;252;141;98m문자\u001b[0m \u001b[0;30;48;2;141;160;203m##입니다\u001b[0m \u001b[0;30;48;2;231;138;195m.\u001b[0m \u001b[0;30;48;2;166;216;84m😊\u001b[0m \u001b[0;30;48;2;255;217;47m,\u001b[0m \u001b[0;30;48;2;102;194;165m카카오\u001b[0m \u001b[0;30;48;2;252;141;98m##와\u001b[0m \u001b[0;30;48;2;141;160;203m네이버는\u001b[0m \u001b[0;30;48;2;231;138;195m한국의\u001b[0m \u001b[0;30;48;2;166;216;84m대표적인\u001b[0m \u001b[0;30;48;2;255;217;47mIT\u001b[0m \u001b[0;30;48;2;102;194;165m기업\u001b[0m \u001b[0;30;48;2;252;141;98m##입니다\u001b[0m \u001b[0;30;48;2;141;160;203m.\u001b[0m \u001b[0;30;48;2;231;138;195m,\u001b[0m \u001b[0;30;48;2;166;216;84m자연\u001b[0m \u001b[0;30;48;2;255;217;47m##어\u001b[0m \u001b[0;30;48;2;102;194;165m##처리\u001b[0m \u001b[0;30;48;2;252;141;98m(\u001b[0m \u001b[0;30;48;2;141;160;203mN\u001b[0m \u001b[0;30;48;2;231;138;195m##L\u001b[0m \u001b[0;30;48;2;166;216;84m##P\u001b[0m \u001b[0;30;48;2;255;217;47m)\u001b[0m \u001b[0;30;48;2;102;194;165m는\u001b[0m \u001b[0;30;48;2;252;141;98m컴퓨터\u001b[0m \u001b[0;30;48;2;141;160;203m##가\u001b[0m \u001b[0;30;48;2;231;138;195m인간의\u001b[0m \u001b[0;30;48;2;166;216;84m언어\u001b[0m \u001b[0;30;48;2;255;217;47m##를\u001b[0m \u001b[0;30;48;2;102;194;165m이해\u001b[0m \u001b[0;30;48;2;252;141;98m##하도록\u001b[0m \u001b[0;30;48;2;141;160;203m하는\u001b[0m \u001b[0;30;48;2;231;138;195m기술이\u001b[0m \u001b[0;30;48;2;166;216;84m##다\u001b[0m \u001b[0;30;48;2;255;217;47m.\u001b[0m \u001b[0;30;48;2;102;194;165m,\u001b[0m \u001b[0;30;48;2;252;141;98m[SEP]\u001b[0m "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Hugging Face에서 보안 이슈 때문에 trust_remote_code=True 추가함\n",
        "def show_tokens(sentence, tokenizer_name):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name,\n",
        "                                              trust_remote_code=True)\n",
        "    token_ids = tokenizer(sentence).input_ids\n",
        "    for idx, t in enumerate(token_ids):\n",
        "        print(\n",
        "            f'\\x1b[0;30;48;2;{colors_list[idx % len(colors_list)]}m' +\n",
        "            tokenizer.decode(t, clean_up_tokenization_spaces=False) +\n",
        "            '\\x1b[0m',\n",
        "            end=' '\n",
        "        )\n",
        "\n",
        "\n",
        "# monologg의 KoBERT (Transformers 호환)\n",
        "show_tokens(ko_text, \"monologg/kobert\")"
      ],
      "metadata": {
        "id": "tX42XpfS_tLW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "617e7c9f-0008-4a01-ab8b-737f6020e5be"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "A new version of the following files was downloaded from https://huggingface.co/monologg/kobert:\n",
            "- tokenization_kobert.py\n",
            ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0;30;48;2;102;194;165m[CLS]\u001b[0m \u001b[0;30;48;2;252;141;98m안\u001b[0m \u001b[0;30;48;2;141;160;203m녕\u001b[0m \u001b[0;30;48;2;231;138;195m하세요\u001b[0m \u001b[0;30;48;2;166;216;84m!\u001b[0m \u001b[0;30;48;2;255;217;47m저\u001b[0m \u001b[0;30;48;2;102;194;165m는\u001b[0m \u001b[0;30;48;2;252;141;98m인\u001b[0m \u001b[0;30;48;2;141;160;203m공\u001b[0m \u001b[0;30;48;2;231;138;195m지\u001b[0m \u001b[0;30;48;2;166;216;84m능\u001b[0m \u001b[0;30;48;2;255;217;47m을\u001b[0m \u001b[0;30;48;2;102;194;165m공부\u001b[0m \u001b[0;30;48;2;252;141;98m하고\u001b[0m \u001b[0;30;48;2;141;160;203m있습니다\u001b[0m \u001b[0;30;48;2;231;138;195m.\u001b[0m \u001b[0;30;48;2;166;216;84m,\u001b[0m \u001b[0;30;48;2;255;217;47m한\u001b[0m \u001b[0;30;48;2;102;194;165m글\u001b[0m \u001b[0;30;48;2;252;141;98m은\u001b[0m \u001b[0;30;48;2;141;160;203m세종\u001b[0m \u001b[0;30;48;2;231;138;195m대\u001b[0m \u001b[0;30;48;2;166;216;84m왕\u001b[0m \u001b[0;30;48;2;255;217;47m이\u001b[0m \u001b[0;30;48;2;102;194;165m창\u001b[0m \u001b[0;30;48;2;252;141;98m제\u001b[0m \u001b[0;30;48;2;141;160;203m하\u001b[0m \u001b[0;30;48;2;231;138;195m신\u001b[0m \u001b[0;30;48;2;166;216;84m문자\u001b[0m \u001b[0;30;48;2;255;217;47m입니다\u001b[0m \u001b[0;30;48;2;102;194;165m.\u001b[0m \u001b[0;30;48;2;252;141;98m\u001b[0m \u001b[0;30;48;2;141;160;203m[UNK]\u001b[0m \u001b[0;30;48;2;231;138;195m,\u001b[0m \u001b[0;30;48;2;166;216;84m카카오\u001b[0m \u001b[0;30;48;2;255;217;47m와\u001b[0m \u001b[0;30;48;2;102;194;165m네이버\u001b[0m \u001b[0;30;48;2;252;141;98m는\u001b[0m \u001b[0;30;48;2;141;160;203m한국의\u001b[0m \u001b[0;30;48;2;231;138;195m대표적인\u001b[0m \u001b[0;30;48;2;166;216;84mIT\u001b[0m \u001b[0;30;48;2;255;217;47m기업\u001b[0m \u001b[0;30;48;2;102;194;165m입니다\u001b[0m \u001b[0;30;48;2;252;141;98m.\u001b[0m \u001b[0;30;48;2;141;160;203m,\u001b[0m \u001b[0;30;48;2;231;138;195m자연\u001b[0m \u001b[0;30;48;2;166;216;84m어\u001b[0m \u001b[0;30;48;2;255;217;47m처리\u001b[0m \u001b[0;30;48;2;102;194;165m(\u001b[0m \u001b[0;30;48;2;252;141;98mN\u001b[0m \u001b[0;30;48;2;141;160;203mL\u001b[0m \u001b[0;30;48;2;231;138;195mP\u001b[0m \u001b[0;30;48;2;166;216;84m)\u001b[0m \u001b[0;30;48;2;255;217;47m는\u001b[0m \u001b[0;30;48;2;102;194;165m컴퓨터\u001b[0m \u001b[0;30;48;2;252;141;98m가\u001b[0m \u001b[0;30;48;2;141;160;203m인간\u001b[0m \u001b[0;30;48;2;231;138;195m의\u001b[0m \u001b[0;30;48;2;166;216;84m언\u001b[0m \u001b[0;30;48;2;255;217;47m어\u001b[0m \u001b[0;30;48;2;102;194;165m를\u001b[0m \u001b[0;30;48;2;252;141;98m이해\u001b[0m \u001b[0;30;48;2;141;160;203m하도록\u001b[0m \u001b[0;30;48;2;231;138;195m하는\u001b[0m \u001b[0;30;48;2;166;216;84m기술\u001b[0m \u001b[0;30;48;2;255;217;47m이다\u001b[0m \u001b[0;30;48;2;102;194;165m.\u001b[0m \u001b[0;30;48;2;252;141;98m,\u001b[0m \u001b[0;30;48;2;141;160;203m[SEP]\u001b[0m "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "wiL4pKvd8P_j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **토큰 임베딩(Token Embedding)**"
      ],
      "metadata": {
        "id": "7Cci9koNIGiZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **언어** --> **토큰의 시퀀스**\n",
        "- **충분히 좋은 모델**을 **충분히 큰 토큰 집합**에서 훈련한다면 **훈련 데이터셋에 있는 복잡한 패턴을 포착**하기 시작한다.\n",
        "- **Embedding** --> 수치표현, **언어에 있는 의미와 패턴을 포착하기 위한 수치 표현 공간**"
      ],
      "metadata": {
        "id": "1Y0VO0ciITwN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 토크나이저가 초기화되고 훈련되고 나면 이를 사용해 언어 모델을 훈련함\n",
        "    - 사전 훈련된 언어 모델이 해당 토크나이저와 연결되는 이유\n",
        "    - 모델을 재훈련하지 않고는 다른 토크나이저를 사용할 수 없음"
      ],
      "metadata": {
        "id": "ocQmgQX4LnOv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **[Quiz] \"충분히 좋은 모델\"은 어떤 것?**"
      ],
      "metadata": {
        "id": "JisemV0voDsz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "문맥을 구분할 수 있는 모델"
      ],
      "metadata": {
        "id": "zGnM2YqCoTkn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "임베딩할 때 유사한 텍스트를 잘 구분해 낼 것"
      ],
      "metadata": {
        "id": "X9b_YqCwoWYC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **[Quiz] “충분히 큰 토큰 집합”의 조건은?**"
      ],
      "metadata": {
        "id": "7IZWSZ6anuYX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 중요한 단어/형태소 등이 과도하게 <unk> 로 표시되지 않을 것\n",
        "- 너무 희귀하게 등장하는 단어까지 토큰에 포함하면 토큰 집합이 지나치게 커지므로 적당히 포함시킬 것\n"
      ],
      "metadata": {
        "id": "BlqtL0aooUJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "gD-bb8EMoBCE"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Tu7OY4HvBEm"
      },
      "source": [
        "## (BERT와 같은)**언어 모델로 문맥을 고려한 단어 임베딩 만들기**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "nsjz-VsYu9bB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35809b69-c1bc-412d-acd7-9663f7bc583f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/transformers/convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModel, AutoTokenizer\n",
        "\n",
        "# 토크나이저를 로드합니다.\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-base\")\n",
        "\n",
        "# 언어 모델을 로드합니다.\n",
        "model = AutoModel.from_pretrained(\"microsoft/deberta-v3-xsmall\")\n",
        "\n",
        "# 문장을 토큰으로 나눕니다.\n",
        "tokens = tokenizer('Hello world', return_tensors='pt')\n",
        "\n",
        "# 토큰을 처리합니다.\n",
        "output = model(**tokens)[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "lQly_KcbvDce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1408db92-5f9f-4faf-e881-f62bb2ef04db"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 4, 384])"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "output.shape\n",
        "\n",
        "# torch.Size([1, 4, 384])\n",
        "#     1 : batch Size (한번에 처리하는 샘플(문장)개수)\n",
        "#     4 : 토큰의 개수\n",
        "#     384: 토큰의 임베딩 차원\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aZsuPFbgg3sH",
        "outputId": "d37d8526-eb76-4819-a9ec-e150932b7093"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[   1, 5365,  447,    2]]), 'token_type_ids': tensor([[0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1]])}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "8GcRrpPV0kVj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a00e83a-daa0-4696-a91d-0c45d651e6a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CLS]\n",
            "Hello\n",
            "world\n",
            "[SEP]\n"
          ]
        }
      ],
      "source": [
        "for token in tokens['input_ids'][0]:\n",
        "    print(tokenizer.decode(token))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "e8oHVC7B0lkk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db585d19-a829-44ff-c4e1-c90166d7c066"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-3.3060, -0.0507, -0.1098,  ..., -0.1704, -0.1618,  0.6932],\n",
              "         [ 0.8918,  0.0740, -0.1583,  ...,  0.1869,  1.4760,  0.0751],\n",
              "         [ 0.0871,  0.6364, -0.3050,  ...,  0.4729, -0.1829,  1.0157],\n",
              "         [-3.1624, -0.1436, -0.0941,  ..., -0.0290, -0.1265,  0.7954]]],\n",
              "       grad_fn=<NativeLayerNormBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DdEDuLWa0r4L"
      },
      "source": [
        "## **텍스트 임베딩** (문장과 전체 문서)\n",
        "\n",
        "- 텍스트 임베딩이란 하나의 벡터로 토큰보다 긴 텍스트를 표현하는 것\n",
        "- **텍스트 임베딩 모델**은 텍스트 조각을 입력받아 텍스트를 표현하고 유용한 형태로 의미를 포착하는 하나의 벡터를 만드는 모델"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- sentence-transformers/all-mpnet-base-v2\n",
        "    - 텍스트를 의미적으로 유사한 고품질의 벡터(임베딩)로 변환하는 데 특화된 문장 임베딩 모델\n",
        "    - 문장 임베딩 분야에서 최고 수준의 성능을 보이는 모델 중 하나"
      ],
      "metadata": {
        "id": "x7hxP2gHcq4d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "TQHWioIc0pQ8"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# 모델을 로드합니다.\n",
        "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
        "\n",
        "# 텍스트를 텍스트 임베딩으로 변환합니다.\n",
        "vector = model.encode(\"Best movie ever!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "PDwfmBiC0uER",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "438f57f1-4e74-4bc8-c97f-f42c15429cef"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(768,)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "vector.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **예제 1: 영화 리뷰 감성 유사도 분석기**\n",
        "텍스트 임베딩을 이용해 리뷰들의 의미적 유사도를 계산하고, 비슷한 감성을 가진 리뷰 찾기"
      ],
      "metadata": {
        "id": "z0Y822vxdSG-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "# 1. 모델 로드\n",
        "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
        "\n",
        "# 2. 영화 리뷰 데이터 (학생들이 직접 작성한 리뷰로 대체 가능)\n",
        "reviews = [\n",
        "    \"이 영화는 정말 최고예요! 감동적이고 재미있었어요.\",\n",
        "    \"인생 영화입니다. 눈물이 멈추지 않았어요.\",\n",
        "    \"돈과 시간 낭비였어요. 지루하고 재미없었습니다.\",\n",
        "    \"최악의 영화. 절대 추천하지 않습니다.\",\n",
        "    \"배우들의 연기가 훌륭했고 스토리도 좋았어요.\",\n",
        "    \"영화관에서 잠들었어요. 너무 지루했습니다.\"\n",
        "]\n",
        "\n",
        "# 3. 텍스트를 임베딩 벡터로 변환\n",
        "embeddings = model.encode(reviews)\n",
        "\n",
        "print(f\"임베딩 벡터의 크기: {embeddings.shape}\")  # (6, 768) - 6개 문장, 768차원\n",
        "print(f\"첫 번째 리뷰의 벡터 일부: {embeddings[0][:5]}\")  # 첫 5개 값만 출력\n",
        "\n",
        "# 4. 코사인 유사도 계산\n",
        "similarity_matrix = cosine_similarity(embeddings)\n",
        "\n",
        "# 5. 결과 시각화\n",
        "print(\"\\n=== 리뷰 간 유사도 매트릭스 ===\")\n",
        "print(\"(1.0에 가까울수록 유사, -1.0에 가까울수록 반대)\")\n",
        "print()\n",
        "\n",
        "for i, review in enumerate(reviews):\n",
        "    print(f\"\\n리뷰 {i+1}: {review[:30]}...\")\n",
        "    # 자기 자신을 제외하고 가장 유사한 리뷰 찾기\n",
        "    similarities = similarity_matrix[i].copy()\n",
        "    similarities[i] = -1  # 자기 자신 제외\n",
        "    most_similar_idx = np.argmax(similarities)\n",
        "\n",
        "    print(f\"  → 가장 유사한 리뷰: 리뷰 {most_similar_idx+1}\")\n",
        "    print(f\"     '{reviews[most_similar_idx][:40]}...'\")\n",
        "    print(f\"  → 유사도 점수: {similarities[most_similar_idx]:.4f}\")\n",
        "\n",
        "# 6. 새로운 리뷰에 대해 가장 유사한 기존 리뷰 찾기\n",
        "new_review = \"정말 감동적인 영화였어요. 강력 추천합니다!\"\n",
        "new_embedding = model.encode([new_review])\n",
        "new_similarities = cosine_similarity(new_embedding, embeddings)[0]\n",
        "\n",
        "print(f\"\\n\\n=== 새로운 리뷰 분석 ===\")\n",
        "print(f\"새 리뷰: {new_review}\")\n",
        "most_similar = np.argmax(new_similarities)\n",
        "print(f\"가장 유사한 기존 리뷰 {most_similar+1}: {reviews[most_similar]}\")\n",
        "print(f\"유사도: {new_similarities[most_similar]:.4f}\")"
      ],
      "metadata": {
        "id": "2PaMnrTodSQF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7cee41a9-8a8c-46cb-f61b-3efca62b16c5"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "임베딩 벡터의 크기: (6, 768)\n",
            "첫 번째 리뷰의 벡터 일부: [ 0.02392089 -0.00769814 -0.00504546  0.02505269  0.0425802 ]\n",
            "\n",
            "=== 리뷰 간 유사도 매트릭스 ===\n",
            "(1.0에 가까울수록 유사, -1.0에 가까울수록 반대)\n",
            "\n",
            "\n",
            "리뷰 1: 이 영화는 정말 최고예요! 감동적이고 재미있었어요....\n",
            "  → 가장 유사한 리뷰: 리뷰 3\n",
            "     '돈과 시간 낭비였어요. 지루하고 재미없었습니다....'\n",
            "  → 유사도 점수: 0.8817\n",
            "\n",
            "리뷰 2: 인생 영화입니다. 눈물이 멈추지 않았어요....\n",
            "  → 가장 유사한 리뷰: 리뷰 4\n",
            "     '최악의 영화. 절대 추천하지 않습니다....'\n",
            "  → 유사도 점수: 0.9125\n",
            "\n",
            "리뷰 3: 돈과 시간 낭비였어요. 지루하고 재미없었습니다....\n",
            "  → 가장 유사한 리뷰: 리뷰 4\n",
            "     '최악의 영화. 절대 추천하지 않습니다....'\n",
            "  → 유사도 점수: 0.9079\n",
            "\n",
            "리뷰 4: 최악의 영화. 절대 추천하지 않습니다....\n",
            "  → 가장 유사한 리뷰: 리뷰 6\n",
            "     '영화관에서 잠들었어요. 너무 지루했습니다....'\n",
            "  → 유사도 점수: 0.9171\n",
            "\n",
            "리뷰 5: 배우들의 연기가 훌륭했고 스토리도 좋았어요....\n",
            "  → 가장 유사한 리뷰: 리뷰 3\n",
            "     '돈과 시간 낭비였어요. 지루하고 재미없었습니다....'\n",
            "  → 유사도 점수: 0.8902\n",
            "\n",
            "리뷰 6: 영화관에서 잠들었어요. 너무 지루했습니다....\n",
            "  → 가장 유사한 리뷰: 리뷰 4\n",
            "     '최악의 영화. 절대 추천하지 않습니다....'\n",
            "  → 유사도 점수: 0.9171\n",
            "\n",
            "\n",
            "=== 새로운 리뷰 분석 ===\n",
            "새 리뷰: 정말 감동적인 영화였어요. 강력 추천합니다!\n",
            "가장 유사한 기존 리뷰 1: 이 영화는 정말 최고예요! 감동적이고 재미있었어요.\n",
            "유사도: 0.9190\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **예제 2: 간단한 질문-답변 검색 시스템 (FAQ 봇)**\n",
        "사용자 질문과 가장 유사한 FAQ를 찾아 답변을 제공"
      ],
      "metadata": {
        "id": "UmKE3dsoeDP9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "# 1. 모델 로드\n",
        "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
        "\n",
        "# 2. FAQ 데이터베이스 (대학교 컴퓨터공학과 FAQ 예시)\n",
        "faq_database = {\n",
        "    \"questions\": [\n",
        "        \"졸업 요건이 어떻게 되나요?\",\n",
        "        \"전공 필수 과목은 무엇인가요?\",\n",
        "        \"복수전공 신청은 어떻게 하나요?\",\n",
        "        \"취업률이 어느 정도인가요?\",\n",
        "        \"학점 인정은 어떻게 받나요?\",\n",
        "        \"전과는 가능한가요?\",\n",
        "        \"인턴십 프로그램이 있나요?\",\n",
        "        \"기숙사 신청 방법을 알려주세요\",\n",
        "        \"장학금 종류가 궁금합니다\",\n",
        "        \"졸업 프로젝트는 필수인가요?\"\n",
        "    ],\n",
        "    \"answers\": [\n",
        "        \"졸업하려면 전공 60학점, 교양 30학점 등 총 140학점이 필요하며, 평점 2.0 이상을 유지해야 합니다.\",\n",
        "        \"자료구조, 알고리즘, 운영체제, 데이터베이스, 컴퓨터구조가 전공 필수 과목입니다.\",\n",
        "        \"복수전공은 2학년 2학기부터 신청 가능하며, 학사포털에서 온라인으로 신청하시면 됩니다.\",\n",
        "        \"최근 3년간 평균 취업률은 92%이며, 대기업 및 IT 기업 취업률이 높습니다.\",\n",
        "        \"타 대학 학점 인정은 학점교류 협정 대학에 한하며, 학과 사무실에 신청서를 제출하시면 됩니다.\",\n",
        "        \"전과는 1학년 말에 가능하며, 성적 및 TO에 따라 선발됩니다.\",\n",
        "        \"여름/겨울 방학 중 산학협력 인턴십 프로그램을 운영하고 있으며, 학점으로 인정됩니다.\",\n",
        "        \"기숙사는 매 학기 초 학생포털에서 신청하며, 거리와 성적 순으로 선발됩니다.\",\n",
        "        \"성적우수 장학금, 국가장학금, 근로장학금 등 다양한 장학 제도가 있습니다.\",\n",
        "        \"네, 졸업 프로젝트는 필수이며 4학년 1, 2학기에 걸쳐 진행됩니다.\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "# 3. FAQ 질문들을 임베딩으로 변환 (미리 계산해두면 효율적)\n",
        "print(\"\\nFAQ 데이터베이스를 임베딩으로 변환 중...\")\n",
        "faq_embeddings = model.encode(faq_database[\"questions\"])\n",
        "print(f\"총 {len(faq_database['questions'])}개의 FAQ가 준비되었습니다.\")\n",
        "print(\"-\" * 70,'\\n')\n",
        "\n",
        "\n",
        "# 4. 사용자 질문 처리 함수\n",
        "def find_answer(user_question, top_k=3):\n",
        "    \"\"\"\n",
        "    사용자 질문에 가장 유사한 FAQ를 찾아 답변을 반환\n",
        "\n",
        "    Parameters:\n",
        "        user_question: 사용자의 질문 (문자열)\n",
        "        top_k: 상위 몇 개의 유사한 질문을 보여줄지\n",
        "    \"\"\"\n",
        "    # 사용자 질문을 임베딩으로 변환\n",
        "    question_embedding = model.encode([user_question])\n",
        "\n",
        "    # 유사도 계산\n",
        "    similarities = cosine_similarity(question_embedding, faq_embeddings)[0]\n",
        "\n",
        "    # 상위 k개의 가장 유사한 질문 찾기\n",
        "    top_indices = np.argsort(similarities)[-top_k:][::-1]\n",
        "\n",
        "    print(f\"✅ 질문: {user_question}\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    for rank, idx in enumerate(top_indices, 1):\n",
        "        print(f\"\\n\\t[{rank}순위] 유사도: {similarities[idx]:.4f}\")\n",
        "        print(f\"\\t유사한 질문: {faq_database['questions'][idx]}\")\n",
        "        print(f\"\\t답변: {faq_database['answers'][idx]}\")\n",
        "        print(\"\\t\", \"-\" * 70)\n",
        "\n",
        "    # 가장 유사한 답변 반환\n",
        "    best_match_idx = top_indices[0]\n",
        "    return faq_database['answers'][best_match_idx], similarities[best_match_idx]\n",
        "\n",
        "\n",
        "# 5. 테스트 시나리오\n",
        "test_questions = [\n",
        "    \"졸업하려면 학점을 얼마나 들어야 해요?\",  # '졸업 요건'과 유사\n",
        "    \"인턴 할 수 있나요?\",  # '인턴십 프로그램'과 유사\n",
        "    \"다른 과로 옮길 수 있어요?\",  # '전과'와 유사\n",
        "    \"AI 수업을 듣고 싶어요\"  # FAQ에 없는 질문\n",
        "]\n",
        "\n",
        "\n",
        "for test_q in test_questions:\n",
        "    answer, similarity = find_answer(test_q, top_k=2)\n",
        "    print(f\"\\n\\t{'='*70}\")\n",
        "    print(f\"\\t★ 최종 답변 (신뢰도: {similarity:.4f})\")\n",
        "\n",
        "    if similarity < 0.5:  # 유사도가 낮으면 경고\n",
        "        print(\"⚠️ 유사도가 낮습니다. 관련된 질문이 FAQ에 없을 수 있습니다.\")\n",
        "\n",
        "    print(f\"\\t답변: {answer}\")\n",
        "    print(f\"\\t{'='*70}\\n\\n\")"
      ],
      "metadata": {
        "id": "ujNF0E7DeR7N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "afa204cd-a24b-4a19-f5d1-20850d8b4ce4"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "FAQ 데이터베이스를 임베딩으로 변환 중...\n",
            "총 10개의 FAQ가 준비되었습니다.\n",
            "---------------------------------------------------------------------- \n",
            "\n",
            "✅ 질문: 졸업하려면 학점을 얼마나 들어야 해요?\n",
            "======================================================================\n",
            "\n",
            "\t[1순위] 유사도: 0.8883\n",
            "\t유사한 질문: 복수전공 신청은 어떻게 하나요?\n",
            "\t답변: 복수전공은 2학년 2학기부터 신청 가능하며, 학사포털에서 온라인으로 신청하시면 됩니다.\n",
            "\t ----------------------------------------------------------------------\n",
            "\n",
            "\t[2순위] 유사도: 0.8542\n",
            "\t유사한 질문: 학점 인정은 어떻게 받나요?\n",
            "\t답변: 타 대학 학점 인정은 학점교류 협정 대학에 한하며, 학과 사무실에 신청서를 제출하시면 됩니다.\n",
            "\t ----------------------------------------------------------------------\n",
            "\n",
            "\t======================================================================\n",
            "\t★ 최종 답변 (신뢰도: 0.8883)\n",
            "\t답변: 복수전공은 2학년 2학기부터 신청 가능하며, 학사포털에서 온라인으로 신청하시면 됩니다.\n",
            "\t======================================================================\n",
            "\n",
            "\n",
            "✅ 질문: 인턴 할 수 있나요?\n",
            "======================================================================\n",
            "\n",
            "\t[1순위] 유사도: 0.8241\n",
            "\t유사한 질문: 복수전공 신청은 어떻게 하나요?\n",
            "\t답변: 복수전공은 2학년 2학기부터 신청 가능하며, 학사포털에서 온라인으로 신청하시면 됩니다.\n",
            "\t ----------------------------------------------------------------------\n",
            "\n",
            "\t[2순위] 유사도: 0.7904\n",
            "\t유사한 질문: 학점 인정은 어떻게 받나요?\n",
            "\t답변: 타 대학 학점 인정은 학점교류 협정 대학에 한하며, 학과 사무실에 신청서를 제출하시면 됩니다.\n",
            "\t ----------------------------------------------------------------------\n",
            "\n",
            "\t======================================================================\n",
            "\t★ 최종 답변 (신뢰도: 0.8241)\n",
            "\t답변: 복수전공은 2학년 2학기부터 신청 가능하며, 학사포털에서 온라인으로 신청하시면 됩니다.\n",
            "\t======================================================================\n",
            "\n",
            "\n",
            "✅ 질문: 다른 과로 옮길 수 있어요?\n",
            "======================================================================\n",
            "\n",
            "\t[1순위] 유사도: 0.7901\n",
            "\t유사한 질문: 복수전공 신청은 어떻게 하나요?\n",
            "\t답변: 복수전공은 2학년 2학기부터 신청 가능하며, 학사포털에서 온라인으로 신청하시면 됩니다.\n",
            "\t ----------------------------------------------------------------------\n",
            "\n",
            "\t[2순위] 유사도: 0.7496\n",
            "\t유사한 질문: 취업률이 어느 정도인가요?\n",
            "\t답변: 최근 3년간 평균 취업률은 92%이며, 대기업 및 IT 기업 취업률이 높습니다.\n",
            "\t ----------------------------------------------------------------------\n",
            "\n",
            "\t======================================================================\n",
            "\t★ 최종 답변 (신뢰도: 0.7901)\n",
            "\t답변: 복수전공은 2학년 2학기부터 신청 가능하며, 학사포털에서 온라인으로 신청하시면 됩니다.\n",
            "\t======================================================================\n",
            "\n",
            "\n",
            "✅ 질문: AI 수업을 듣고 싶어요\n",
            "======================================================================\n",
            "\n",
            "\t[1순위] 유사도: 0.7819\n",
            "\t유사한 질문: 졸업 요건이 어떻게 되나요?\n",
            "\t답변: 졸업하려면 전공 60학점, 교양 30학점 등 총 140학점이 필요하며, 평점 2.0 이상을 유지해야 합니다.\n",
            "\t ----------------------------------------------------------------------\n",
            "\n",
            "\t[2순위] 유사도: 0.7431\n",
            "\t유사한 질문: 복수전공 신청은 어떻게 하나요?\n",
            "\t답변: 복수전공은 2학년 2학기부터 신청 가능하며, 학사포털에서 온라인으로 신청하시면 됩니다.\n",
            "\t ----------------------------------------------------------------------\n",
            "\n",
            "\t======================================================================\n",
            "\t★ 최종 답변 (신뢰도: 0.7819)\n",
            "\t답변: 졸업하려면 전공 60학점, 교양 30학점 등 총 140학점이 필요하며, 평점 2.0 이상을 유지해야 합니다.\n",
            "\t======================================================================\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **[미션] 나만의 FAQ 봇 만들기**\n",
        "앞에서 실습해 본 `예제 2: 간단한 질문-답변 검색 시스템 (FAQ 봇)`에 자신만의 적절한 데이터를 수집하고 반영하여 OOO 봇을 만들어 보세요."
      ],
      "metadata": {
        "id": "suZ4NJHbkGoV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "# 1. 모델 로드\n",
        "model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2') # 기존 sentence-transformers/all-mpnet-base-v2\n",
        "\n",
        "# 2. FAQ 데이터베이스 (메이플스토리 '렌' 관련)\n",
        "faq_database = {\n",
        "    \"questions\": [\n",
        "        \"렌(Len)은 어떤 캐릭터인가요?\",\n",
        "        \"렌(Len)은 언제 업데이트된 캐릭터인가요?\",\n",
        "        \"렌은 어떤 직업군(모험가/영웅/아니마 등)에 속하나요?\",\n",
        "        \"렌의 무기 종류와 보조 무기는 무엇인가요?\",\n",
        "        \"렌의 장비 특징은 무엇인가요?\"\n",
        "        \"렌의 콘셉트(스토리 배경)는 어떤 내용인가요?\",\n",
        "        \"렌의 주요 스킬 구조와 전투 스타일은 어떤가요?\",\n",
        "        \"렌은 전투할 때 조작 난이도가 높은 편인가요?\",\n",
        "        \"렌은 사냥과 보스전 중 어디에 더 강점이 있나요?\",\n",
        "        \"렌의 링크 스킬 효과는 무엇인가요?\",\n",
        "        \"렌을 처음 키울 때 추천하는 스탯 배분은 무엇인가요?\",\n",
        "        \"렌은 어떤 능력치를 주로 올리면 되나요?\",\n",
        "        \"렌의 초보자 난이도는 어느 정도인가요?\",\n",
        "        \"렌은 다른 직업보다 배우기 쉬운 편인가요?\",\n",
        "        \"렌의 단점이나 주의해야 할 점은 무엇인가요?\",\n",
        "        \"렌과 시너지가 좋은 다른 직업은 무엇인가요?\"\n",
        "    ],\n",
        "    \"answers\": [\n",
        "        \"렌은 아니마 직업군에 속하는 전사 캐릭터로, 동양풍의 무기를 다루는 독특한 콘셉트를 가지고 있습니다. 두 종류의 검을 활용하며, 스토리와 전투 스타일 모두 신선하다는 평가를 받고 있습니다.\",\n",
        "        \"렌은 2025년 여름 대규모 패치인 'Assemble!' 업데이트에서 처음 공개된 신규 직업입니다. 약 3년 만의 신캐릭터라 많은 유저들의 기대와 관심을 모았습니다.\",\n",
        "        \"렌은 아니마(Anima) 계열에 속하는 전사형 캐릭터입니다. 아니마 직업군 특유의 동양적 세계관을 반영하면서도, 전투 스타일은 비교적 간단하고 직관적으로 설계되었습니다.\",\n",
        "        \"렌의 주 무기는 한손검이며, 'Plum Blossom Sword'와 'Lost Soul Sword'라는 두 가지 검을 상황에 맞게 사용합니다. 보조 무기는 없습니다.\",\n",
        "        \"렌은 보조 무기 대신 두 검을 번갈아 사용한다는 특징이 있습니다. 이 때문에 장비 세팅은 일반 전사와 유사하지만, 무기와 스킬 연계 방식이 조금 더 독창적입니다.\",\n",
        "        \"렌은 이무기 ‘사야(Saya)’와 함께 모험을 떠나는 전사로, 동양적인 색채가 짙은 세계관을 배경으로 합니다. 스토리 자체가 캐릭터의 테마와 스킬 연계에도 반영되어 있어 몰입감을 높입니다.\",\n",
        "        \"렌의 스킬은 홀드(hold) 방식으로 키를 누르고 유지하며 연속 공격을 이어가는 구조입니다. 또 자동 발동 스킬이 많아 초보자도 쉽게 다룰 수 있으며, 전투 중 이동과 점프가 가능해 유연한 스타일을 보여줍니다.\",\n",
        "        \"렌은 직관적인 스킬 구조 덕분에 조작 난이도가 낮은 편으로 평가됩니다. 기본 공격이 단순하면서도 연계가 매끄럽기 때문에 숙련도가 낮은 유저도 쉽게 다룰 수 있습니다.\",\n",
        "        \"렌은 사냥과 보스전 모두 무난한 성능을 보여주는 밸런스형 캐릭터입니다. 특히 보스전에서는 링크 스킬을 통해 생존력이 강화되어 안정적으로 싸울 수 있다는 장점이 있습니다.\",\n",
        "        \"렌의 링크 스킬은 퍼센트 HP 기반 공격 피해를 감소시키는 효과를 가지고 있습니다. 이 덕분에 강력한 보스의 공격을 견디는 데 유리하며, 파티 플레이에서도 큰 도움이 됩니다.\",\n",
        "        \"렌은 STR(힘)을 주 스탯으로 삼기 때문에 자동 스탯 배분을 사용하는 것이 가장 효율적입니다. 별도의 수동 분배가 필요하지 않아 초보자도 부담 없이 성장시킬 수 있습니다.\",\n",
        "        \"렌은 전사형 캐릭터로서 공격력과 관련된 STR을 집중적으로 올리는 것이 핵심입니다. 자동 분배를 사용하면 자연스럽게 STR 위주로 배분되므로 안정적입니다.\",\n",
        "        \"렌은 초보자도 쉽게 다룰 수 있는 난이도 중하 수준으로 평가됩니다. 조작이 단순하고 자동 발동 스킬이 많아, 처음 시작하는 유저도 빠르게 적응할 수 있습니다.\",\n",
        "        \"네, 렌은 다른 직업군에 비해 조작 난이도가 낮아 배우기 쉬운 편입니다. 따라서 메이플스토리를 처음 접하는 유저나 서브 캐릭터로 가볍게 키우기에도 적합합니다.\",\n",
        "        \"렌은 지속 딜링 구조가 최적화되지 않아 숙련도에 따라 성능 차이가 발생할 수 있습니다. 또한 신규 직업 특성상 추후 밸런스 패치가 이루어질 가능성이 높다는 점도 염두에 두어야 합니다.\",\n",
        "        \"렌은 방어적인 성격을 가진 데몬어벤져나 팔라딘과 시너지가 잘 맞습니다. 또한 경험치 증가 링크 스킬을 가진 메르세데스 등과 함께하면 육성과 파티 플레이에서 더욱 강력한 효과를 발휘할 수 있습니다.\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "# 3. FAQ 질문들을 임베딩으로 변환 (미리 계산해두면 효율적)\n",
        "print(\"\\nFAQ 데이터베이스를 임베딩으로 변환 중...\")\n",
        "faq_embeddings = model.encode(faq_database[\"questions\"])\n",
        "print(f\"총 {len(faq_database['questions'])}개의 FAQ가 준비되었습니다.\")\n",
        "print(\"-\" * 70,'\\n')\n",
        "\n",
        "\n",
        "# 4. 사용자 질문 처리 함수\n",
        "def find_answer(user_question, top_k=3):\n",
        "    \"\"\"\n",
        "    사용자 질문에 가장 유사한 FAQ를 찾아 답변을 반환\n",
        "\n",
        "    Parameters:\n",
        "        user_question: 사용자의 질문 (문자열)\n",
        "        top_k: 상위 몇 개의 유사한 질문을 보여줄지\n",
        "    \"\"\"\n",
        "    # 사용자 질문을 임베딩으로 변환\n",
        "    question_embedding = model.encode([user_question])\n",
        "\n",
        "    # 유사도 계산\n",
        "    similarities = cosine_similarity(question_embedding, faq_embeddings)[0]\n",
        "\n",
        "    # 상위 k개의 가장 유사한 질문 찾기\n",
        "    top_indices = np.argsort(similarities)[-top_k:][::-1]\n",
        "\n",
        "    print(f\"✅ 질문: {user_question}\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    for rank, idx in enumerate(top_indices, 1):\n",
        "        print(f\"\\n\\t[{rank}순위] 유사도: {similarities[idx]:.4f}\")\n",
        "        print(f\"\\t유사한 질문: {faq_database['questions'][idx]}\")\n",
        "        print(f\"\\t답변: {faq_database['answers'][idx]}\")\n",
        "        print(\"\\t\", \"-\" * 70)\n",
        "\n",
        "    # 가장 유사한 답변 반환\n",
        "    best_match_idx = top_indices[0]\n",
        "    return faq_database['answers'][best_match_idx], similarities[best_match_idx]\n",
        "\n",
        "\n",
        "# 5. 테스트 시나리오\n",
        "test_questions = [\n",
        "    \"렌은 어떤 계열 직업인가요?\",  # '직업군'과 유사\n",
        "    \"렌과 같이 키우기 좋은 캐릭터는 뭐야?\",  # '직업 시너지'와 유사\n",
        "    \"렌 스탯은 어떻게 올리면 좋을까?\",  # '스탯 배분'와 유사\n",
        "    \"렌의 200렙에서 추천하는 사냥터를 추천해줘\"  # FAQ에 없는 질문\n",
        "]\n",
        "\n",
        "\n",
        "for test_q in test_questions:\n",
        "    answer, similarity = find_answer(test_q, top_k=2)\n",
        "    print(f\"\\n\\t{'='*70}\")\n",
        "    print(f\"\\t★ 최종 답변 (신뢰도: {similarity:.4f})\")\n",
        "\n",
        "    if similarity < 0.5:  # 유사도가 낮으면 경고\n",
        "        print(\"⚠️ 유사도가 낮습니다. 관련된 질문이 FAQ에 없을 수 있습니다.\")\n",
        "\n",
        "    print(f\"\\t답변: {answer}\")\n",
        "    print(f\"\\t{'='*70}\\n\\n\")\n"
      ],
      "metadata": {
        "id": "23eMnz2fkZuy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72963f39-e109-4777-cfe7-de7e75f54b4e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "FAQ 데이터베이스를 임베딩으로 변환 중...\n",
            "총 15개의 FAQ가 준비되었습니다.\n",
            "---------------------------------------------------------------------- \n",
            "\n",
            "✅ 질문: 렌은 어떤 계열 직업인가요?\n",
            "======================================================================\n",
            "\n",
            "\t[1순위] 유사도: 0.8372\n",
            "\t유사한 질문: 렌은 어떤 직업군(모험가/영웅/아니마 등)에 속하나요?\n",
            "\t답변: 렌은 아니마(Anima) 계열에 속하는 전사형 캐릭터입니다. 아니마 직업군 특유의 동양적 세계관을 반영하면서도, 전투 스타일은 비교적 간단하고 직관적으로 설계되었습니다.\n",
            "\t ----------------------------------------------------------------------\n",
            "\n",
            "\t[2순위] 유사도: 0.7343\n",
            "\t유사한 질문: 렌과 시너지가 좋은 다른 직업은 무엇인가요?\n",
            "\t답변: 렌은 지속 딜링 구조가 최적화되지 않아 숙련도에 따라 성능 차이가 발생할 수 있습니다. 또한 신규 직업 특성상 추후 밸런스 패치가 이루어질 가능성이 높다는 점도 염두에 두어야 합니다.\n",
            "\t ----------------------------------------------------------------------\n",
            "\n",
            "\t======================================================================\n",
            "\t★ 최종 답변 (신뢰도: 0.8372)\n",
            "\t답변: 렌은 아니마(Anima) 계열에 속하는 전사형 캐릭터입니다. 아니마 직업군 특유의 동양적 세계관을 반영하면서도, 전투 스타일은 비교적 간단하고 직관적으로 설계되었습니다.\n",
            "\t======================================================================\n",
            "\n",
            "\n",
            "✅ 질문: 렌과 같이 키우기 좋은 캐릭터는 뭐야?\n",
            "======================================================================\n",
            "\n",
            "\t[1순위] 유사도: 0.8390\n",
            "\t유사한 질문: 렌(Len)은 어떤 캐릭터인가요?\n",
            "\t답변: 렌은 아니마 직업군에 속하는 전사 캐릭터로, 동양풍의 무기를 다루는 독특한 콘셉트를 가지고 있습니다. 두 종류의 검을 활용하며, 스토리와 전투 스타일 모두 신선하다는 평가를 받고 있습니다.\n",
            "\t ----------------------------------------------------------------------\n",
            "\n",
            "\t[2순위] 유사도: 0.7075\n",
            "\t유사한 질문: 렌은 어떤 직업군(모험가/영웅/아니마 등)에 속하나요?\n",
            "\t답변: 렌은 아니마(Anima) 계열에 속하는 전사형 캐릭터입니다. 아니마 직업군 특유의 동양적 세계관을 반영하면서도, 전투 스타일은 비교적 간단하고 직관적으로 설계되었습니다.\n",
            "\t ----------------------------------------------------------------------\n",
            "\n",
            "\t======================================================================\n",
            "\t★ 최종 답변 (신뢰도: 0.8390)\n",
            "\t답변: 렌은 아니마 직업군에 속하는 전사 캐릭터로, 동양풍의 무기를 다루는 독특한 콘셉트를 가지고 있습니다. 두 종류의 검을 활용하며, 스토리와 전투 스타일 모두 신선하다는 평가를 받고 있습니다.\n",
            "\t======================================================================\n",
            "\n",
            "\n",
            "✅ 질문: 렌 스탯은 어떻게 올리면 좋을까?\n",
            "======================================================================\n",
            "\n",
            "\t[1순위] 유사도: 0.5729\n",
            "\t유사한 질문: 렌의 단점이나 주의해야 할 점은 무엇인가요?\n",
            "\t답변: 네, 렌은 다른 직업군에 비해 조작 난이도가 낮아 배우기 쉬운 편입니다. 따라서 메이플스토리를 처음 접하는 유저나 서브 캐릭터로 가볍게 키우기에도 적합합니다.\n",
            "\t ----------------------------------------------------------------------\n",
            "\n",
            "\t[2순위] 유사도: 0.5725\n",
            "\t유사한 질문: 렌은 어떤 능력치를 주로 올리면 되나요?\n",
            "\t답변: 렌은 STR(힘)을 주 스탯으로 삼기 때문에 자동 스탯 배분을 사용하는 것이 가장 효율적입니다. 별도의 수동 분배가 필요하지 않아 초보자도 부담 없이 성장시킬 수 있습니다.\n",
            "\t ----------------------------------------------------------------------\n",
            "\n",
            "\t======================================================================\n",
            "\t★ 최종 답변 (신뢰도: 0.5729)\n",
            "\t답변: 네, 렌은 다른 직업군에 비해 조작 난이도가 낮아 배우기 쉬운 편입니다. 따라서 메이플스토리를 처음 접하는 유저나 서브 캐릭터로 가볍게 키우기에도 적합합니다.\n",
            "\t======================================================================\n",
            "\n",
            "\n",
            "✅ 질문: 렌의 200렙에서 추천하는 사냥터를 추천해줘\n",
            "======================================================================\n",
            "\n",
            "\t[1순위] 유사도: 0.3544\n",
            "\t유사한 질문: 렌의 무기 종류와 보조 무기는 무엇인가요?\n",
            "\t답변: 렌의 주 무기는 한손검이며, 'Plum Blossom Sword'와 'Lost Soul Sword'라는 두 가지 검을 상황에 맞게 사용합니다. 보조 무기는 없습니다.\n",
            "\t ----------------------------------------------------------------------\n",
            "\n",
            "\t[2순위] 유사도: 0.3418\n",
            "\t유사한 질문: 렌은 사냥과 보스전 중 어디에 더 강점이 있나요?\n",
            "\t답변: 렌은 직관적인 스킬 구조 덕분에 조작 난이도가 낮은 편으로 평가됩니다. 기본 공격이 단순하면서도 연계가 매끄럽기 때문에 숙련도가 낮은 유저도 쉽게 다룰 수 있습니다.\n",
            "\t ----------------------------------------------------------------------\n",
            "\n",
            "\t======================================================================\n",
            "\t★ 최종 답변 (신뢰도: 0.3544)\n",
            "⚠️ 유사도가 낮습니다. 관련된 질문이 FAQ에 없을 수 있습니다.\n",
            "\t답변: 렌의 주 무기는 한손검이며, 'Plum Blossom Sword'와 'Lost Soul Sword'라는 두 가지 검을 상황에 맞게 사용합니다. 보조 무기는 없습니다.\n",
            "\t======================================================================\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMSgyKKS4xUx"
      },
      "source": [
        "## **임베딩으로 노래 추천하기**\n",
        "\n",
        "- 데이터셋: 코넬대학교 슈오첸이 모은 데이터셋(미국 전역에 있는 수백개의 라이도 방송국에서 가져온 재생목록)\n",
        "    - 노래 재생목록 :\n",
        "        - https://storage.googleapis.com/maps-premium/dataset/yes_complete/train.txt\n",
        "    - 노래 메타데이터: (제목, 아티스트)\n",
        "        - https://storage.googleapis.com/maps-premium/dataset/yes_complete/song_hash.txt\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "id": "fLHAdPqghG3z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 708
        },
        "outputId": "4424549f-8226-452f-f143-88092511952b"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.3.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Collecting numpy<2.0,>=1.18.5 (from gensim)\n",
            "  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
            "  Downloading scipy-1.13.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.3.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open>=1.8.1->gensim) (1.17.3)\n",
            "Downloading gensim-4.3.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.6/26.6 MB\u001b[0m \u001b[31m88.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m119.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.13.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.2/38.2 MB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, scipy, gensim\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.16.2\n",
            "    Uninstalling scipy-1.16.2:\n",
            "      Successfully uninstalled scipy-1.16.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "tsfresh 0.21.1 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.1 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gensim-4.3.3 numpy-1.26.4 scipy-1.13.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy",
                  "scipy"
                ]
              },
              "id": "2e68629265404be8b958da2a049cd1d9"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 데이터셋 로드하기"
      ],
      "metadata": {
        "id": "YynR6bbHi3SH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "3dJdWzT67nDL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc864e58-fd08-4a38-c4ba-9d1bd4246b9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11138\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from urllib import request\n",
        "\n",
        "# 재생목록 데이터셋 파일을 가져옵니다.\n",
        "data = request.urlopen('https://storage.googleapis.com/maps-premium/dataset/yes_complete/train.txt')\n",
        "\n",
        "# 재생목록 파일을 파싱합니다. 처음 두 줄은 메타데이터만 담고 있으므로 건너뜁니다.\n",
        "lines = data.read().decode(\"utf-8\").split('\\n')[2:]\n",
        "print(len(lines))\n",
        "\n",
        "# 하나의 노래만 있는 재생목록은 삭제합니다.\n",
        "playlists = [s.rstrip().split() for s in lines if len(s.split()) > 1]\n",
        "\n",
        "# 노래의 메타데이터를 로드합니다.\n",
        "songs_file = request.urlopen('https://storage.googleapis.com/maps-premium/dataset/yes_complete/song_hash.txt')\n",
        "songs_file = songs_file.read().decode(\"utf-8\").split('\\n')\n",
        "songs = [s.rstrip().split('\\t') for s in songs_file]\n",
        "songs_df = pd.DataFrame(data=songs, columns = ['id', 'title', 'artist'])\n",
        "songs_df = songs_df.set_index('id')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "Q3zirG-lo3H8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef245e62-59a6-4d69-8558-4fcda6f26f85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "재생목록 #1:\n",
            "  ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '2', '42', '43', '44', '45', '46', '47', '48', '20', '49', '8', '50', '51', '52', '53', '54', '55', '56', '57', '25', '58', '59', '60', '61', '62', '3', '63', '64', '65', '66', '46', '47', '67', '2', '48', '68', '69', '70', '57', '50', '71', '72', '53', '73', '25', '74', '59', '20', '46', '75', '76', '77', '59', '20', '43'] \n",
            "\n",
            "재생목록 #2:\n",
            "  ['78', '79', '80', '3', '62', '81', '14', '82', '48', '83', '84', '17', '85', '86', '87', '88', '74', '89', '90', '91', '4', '73', '62', '92', '17', '53', '59', '93', '94', '51', '50', '27', '95', '48', '96', '97', '98', '99', '100', '57', '101', '102', '25', '103', '3', '104', '105', '106', '107', '47', '108', '109', '110', '111', '112', '113', '25', '63', '62', '114', '115', '84', '116', '117', '118', '119', '120', '121', '122', '123', '50', '70', '71', '124', '17', '85', '14', '82', '48', '125', '47', '46', '72', '53', '25', '73', '4', '126', '59', '74', '20', '43', '127', '128', '129', '13', '82', '48', '130', '131', '132', '133', '134', '135', '136', '137', '59', '46', '138', '43', '20', '139', '140', '73', '57', '70', '141', '3', '1', '74', '142', '143', '144', '145', '48', '13', '25', '146', '50', '147', '126', '59', '20', '148', '149', '150', '151', '152', '56', '153', '154', '155', '156', '157', '158', '159', '160', '161', '162', '163', '164', '165', '166', '167', '168', '169', '170', '171', '172', '173', '174', '175', '60', '176', '51', '177', '178', '179', '180', '181', '182', '183', '184', '185', '57', '186', '187', '188', '189', '190', '191', '46', '192', '193', '194', '195', '196', '197', '198', '25', '199', '200', '49', '201', '100', '202', '203', '204', '205', '206', '207', '32', '208', '209', '210']\n"
          ]
        }
      ],
      "source": [
        "print('재생목록 #1:\\n ', playlists[0], '\\n')\n",
        "print('재생목록 #2:\\n ', playlists[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Word2Vec 훈련하기\n",
        "    - 결과물: 각 노래에 대해 계산된 임베딩 결과 --> 이 임베딩으로 비슷한 노래를 찾을 수 있다.)"
      ],
      "metadata": {
        "id": "V0iB1PqNjASr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "EaUz3E0P7sJs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "outputId": "76114c3c-f69f-4972-d796-bf0246d9e7df"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2683588673.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Word2Vec 모델을 훈련합니다.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m model = Word2Vec(\n\u001b[1;32m      5\u001b[0m     \u001b[0mplaylists\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gensim/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mparsing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatutils\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterfaces\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimilarities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutils\u001b[0m  \u001b[0;31m# noqa:F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gensim/corpora/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# bring corpus classes directly into package namespace, to save some typing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mindexedcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mIndexedCorpus\u001b[0m  \u001b[0;31m# noqa:F401 must appear before the other classes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmmcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMmCorpus\u001b[0m  \u001b[0;31m# noqa:F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gensim/corpora/indexedcorpus.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minterfaces\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mlogger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetLogger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gensim/interfaces.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gensim/matutils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m   1032\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m     \u001b[0;31m# try to load fast, cythonized code if possible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1034\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_matutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogsumexp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean_absolute_difference\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirichlet_expectation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1035\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gensim/_matutils.pyx\u001b[0m in \u001b[0;36minit gensim._matutils\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
          ]
        }
      ],
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Word2Vec 모델을 훈련합니다.\n",
        "model = Word2Vec(\n",
        "    playlists, vector_size=32, window=20, negative=50, min_count=1, workers=4\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9EFGWesO8rOJ"
      },
      "outputs": [],
      "source": [
        "song_id = 2172\n",
        "\n",
        "# 노래 ID 2172와 비슷한 노래를 찾으라고 모델에게 요청합니다.\n",
        "model.wv.most_similar(positive=str(song_id))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AMiY6isXqKk4"
      },
      "outputs": [],
      "source": [
        "print(songs_df.iloc[2172])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aOzWENxr2Fl3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def print_recommendations(song_id):\n",
        "    similar_songs = np.array(\n",
        "        model.wv.most_similar(positive=str(song_id),topn=5)\n",
        "    )[:,0]\n",
        "    return  songs_df.iloc[similar_songs]\n",
        "\n",
        "# 추천 노래 출력\n",
        "print_recommendations(2172)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TIHiN62g1NMi"
      },
      "outputs": [],
      "source": [
        "print_recommendations(842)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}