{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanghyun-ai/ktcloud_genai/blob/main/%EC%8B%A4%EC%8A%B5%EC%BD%94%EB%93%9C/111_LLM_%ED%86%A0%ED%81%B0%EA%B3%BC%EC%9E%84%EB%B2%A0%EB%94%A9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Token & Embedding 자세히 살펴보기**"
      ],
      "metadata": {
        "id": "_intVJo1FXQF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "- 💡 **NOTE**\n",
        "    - 이 노트북의 코드를 실행하려면 GPU를 사용하는 것이 좋습니다. 구글 코랩에서는 **런타임 > 런타임 유형 변경 > 하드웨어 가속기 > T4 GPU**를 선택하세요.\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "81_Ybs4LI7IX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Tokenization**"
      ],
      "metadata": {
        "id": "X8ya0DICf_mC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4Z1qf7ir_O8k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08457399-ba4f-44b6-9562-27dcf4a066d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers==4.48.3\n",
            "  Downloading transformers-4.48.3-py3-none-any.whl.metadata (44 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/44.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers==4.48.3) (3.19.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.48.3) (0.35.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.48.3) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.48.3) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.48.3) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.48.3) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers==4.48.3) (2.32.4)\n",
            "Collecting tokenizers<0.22,>=0.21 (from transformers==4.48.3)\n",
            "  Downloading tokenizers-0.21.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.48.3) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers==4.48.3) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers==4.48.3) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers==4.48.3) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers==4.48.3) (1.1.10)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.48.3) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.48.3) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.48.3) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.48.3) (2025.8.3)\n",
            "Downloading transformers-4.48.3-py3-none-any.whl (9.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m63.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.21.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m56.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tokenizers, transformers\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.22.0\n",
            "    Uninstalling tokenizers-0.22.0:\n",
            "      Successfully uninstalled tokenizers-0.22.0\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.56.1\n",
            "    Uninstalling transformers-4.56.1:\n",
            "      Successfully uninstalled transformers-4.56.1\n",
            "Successfully installed tokenizers-0.21.4 transformers-4.48.3\n"
          ]
        }
      ],
      "source": [
        "# Phi-3 모델과 호환성 때문에 transformers 4.48.3 버전을 사용합니다.\n",
        "!pip install transformers==4.48.3"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 깃허브에서 위젯 상태 오류를 피하기 위해 진행 표시줄을 나타내지 않도록 설정합니다.\n",
        "from transformers.utils import logging\n",
        "\n",
        "logging.disable_progress_bar()"
      ],
      "metadata": {
        "id": "BKozl7tQzNjm"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQHfpqT_t9-K"
      },
      "source": [
        "## LLM 토큰화 확인\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. 모델 로드하고 토크나이저 지정하기**"
      ],
      "metadata": {
        "id": "xg49D27lZWwD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "jjU8NBHnwA4j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4945993-cc87-4839-dd2d-498e20942870"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-3-mini-4k-instruct:\n",
            "- configuration_phi3.py\n",
            ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
            "A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-3-mini-4k-instruct:\n",
            "- modeling_phi3.py\n",
            ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
            "WARNING:transformers_modules.microsoft.Phi-3-mini-4k-instruct.0a67737cc96d2554230f90338b163bc6380a2a85.modeling_phi3:`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
            "WARNING:transformers_modules.microsoft.Phi-3-mini-4k-instruct.0a67737cc96d2554230f90338b163bc6380a2a85.modeling_phi3:Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# 모델과 토크나이저를 로드합니다.\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"microsoft/Phi-3-mini-4k-instruct\",\n",
        "    device_map=\"cuda\",\n",
        "    torch_dtype=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.입력 프롬프트를 토큰으로 나누기**"
      ],
      "metadata": {
        "id": "wdSVV0tZZ2GE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "_iVl5yePuq3B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0c81564-7fce-4c76-fe17-09fa8c7be1e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n",
            "`get_max_cache()` is deprecated for all Cache classes. Use `get_max_cache_shape()` instead. Calling `get_max_cache()` will raise error from v4.48\n",
            "WARNING:transformers_modules.microsoft.Phi-3-mini-4k-instruct.0a67737cc96d2554230f90338b163bc6380a2a85.modeling_phi3:You are not running the flash-attention implementation, expect numerical differences.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Write an email apologizing to Sarah for the tragic gardening mishap.\n",
            "Explain how it happened.<|assistant|> Subject: Sincere Apologies for the Gardening Mishap\n",
            "\n",
            "Dear Sarah\n"
          ]
        }
      ],
      "source": [
        "prompt = '''Write an email apologizing to Sarah for the tragic gardening mishap.\n",
        "Explain how it happened.<|assistant|>''' # 여기서 prompt == input\n",
        "\n",
        "# 입력 프롬프트를 토큰으로 나눕니다. (return_tensors=pt는 파이토치)\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
        "\n",
        "# 텍스트를 생성합니다.\n",
        "generation_output = model.generate(\n",
        "  input_ids=input_ids,\n",
        "  max_new_tokens=20   # 토큰을 20개 생성\n",
        ")\n",
        "\n",
        "# 출력을 프린트합니다.\n",
        "print(tokenizer.decode(generation_output[0]))  # decode 메서드: 토큰ID를 실제 텍스트로 변환"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "JmzgbbdKuvHt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed837fb3-268e-40d7-e9ee-98a0ef53d1d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[14350,   385,  4876, 27746,  5281,   304, 19235,   363,   278, 25305,\n",
            "           293, 16423,   292,   286,   728,   481, 29889,    13,  9544,  7420,\n",
            "           920,   372,  9559, 29889, 32001]], device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "print(input_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "W4vsjbxwu1K1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9527902-b510-41a2-e527-6fa7e0db1680"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Write\n",
            "an\n",
            "email\n",
            "apolog\n",
            "izing\n",
            "to\n",
            "Sarah\n",
            "for\n",
            "the\n",
            "trag\n",
            "ic\n",
            "garden\n",
            "ing\n",
            "m\n",
            "ish\n",
            "ap\n",
            ".\n",
            "\n",
            "\n",
            "Exp\n",
            "lain\n",
            "how\n",
            "it\n",
            "happened\n",
            ".\n",
            "<|assistant|>\n"
          ]
        }
      ],
      "source": [
        "for id in input_ids[0]:\n",
        "   print(tokenizer.decode(id))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "A9wRZ3J3u4z1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "217ca271-fb0d-46d3-a70e-12be8241370a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[14350,   385,  4876, 27746,  5281,   304, 19235,   363,   278, 25305,\n",
              "           293, 16423,   292,   286,   728,   481, 29889,    13,  9544,  7420,\n",
              "           920,   372,  9559, 29889, 32001,  3323,   622, 29901,   317,  3742,\n",
              "           406,  6225, 11763,   363,   278, 19906,   292,   341,   728,   481,\n",
              "            13,    13, 29928,   799, 19235]], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "generation_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "7QlHLof3u8A3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6e559be-49e8-4088-f038-bb614b058334"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sub\n",
            "ject\n",
            "Subject\n",
            ":\n"
          ]
        }
      ],
      "source": [
        "print(tokenizer.decode(3323))\n",
        "print(tokenizer.decode(622))\n",
        "print(tokenizer.decode([3323, 622]))\n",
        "print(tokenizer.decode(29901))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **토크나이저가 텍스트를 분할하는 방법**\n",
        "\n",
        "1. 모델 설계시 모델 작성자가 토큰화 방법을 선택\n",
        "    - GPT 모델 : BPE(byte pair encoding)\n",
        "    - BERT 모델 : WordPiece\n",
        "2. 토큰화 방법을 선택한 후에 어휘사전 크기와 특수 토큰 같은 토크나이저 설계상의 여러 가지 선택을 해야함\n",
        "3. 토크나이저는 특정 데이터셋에서 훈련하여 해당 데이터셋을 표현하는 최상의 어휘사전을 구축해야 함"
      ],
      "metadata": {
        "id": "fV2mGa9VcpGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **토큰 종류**\n",
        "\n",
        "- **단어 토큰**\n",
        "    - word2vec와 같은 초기 토큰화에 사용됨 --> 현재는 덜 사용됨\n",
        "    - 지금도 추천 시스템과 같은 곳에서 사용\n",
        "    - (단점: 훈련된 후에 데이터셋에 새롭게 추가된 단어는 처리할 수 없다.-->되도록 많은 어휘사전을 만들어야 한다.)\n",
        "\n",
        "- **부분단어 토큰** (완전단어+부분완전단어 포함)\n",
        "    - 새로운 단어를 (어휘사전에 포함되어 있을 가능성이 높은) 더 작은 단위로 나눈다.\n",
        "    - **평균적으로 토큰당 세 개의 문자로 구성됨**\n",
        "- **문자 토큰**\n",
        "    - 대체할 원시 문자가 있기 때문에 새로운 단어를 잘 처리할 수 있다.\n",
        "    - 토큰화는 쉽지만 모델링은 어렵다.--> 문자를 조합하는 정보까지 모델링해야함)\n",
        "- **바이트 토큰**\n",
        "    - 유니코드 문자를 표현하는 바이트로 토큰을 분할하는 방법\n",
        "    - '토큰화-프리 인코딩' 라고 부름\n",
        "    - 다국어 환경에서 경쟁력이 있다고 봄"
      ],
      "metadata": {
        "id": "79cY6Iu9d8FE"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9nRducW48bd"
      },
      "source": [
        "## **훈련된 LLM 토큰나이저 비교하기**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **중점 비교 요소**\n",
        "    - **토큰화 방법** :  BEP, SentencePiece, WordPiece\n",
        "    - **토크나이저 파라미터**\n",
        "        - 어휘사전 크기 : 토크나이저가 어휘사전에 얼마나 많은 토큰을 포함할 건가?\n",
        "        - 특수 토큰 : 모델이 추적해야할 특수 토큰은 무엇인가?\n",
        "        - 대소문자 : 영어와 같은 대소문자를 어떻게 다뤄야 할까?"
      ],
      "metadata": {
        "id": "nS-tC9BJ5dcY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "7W0xFIVo5A0S"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "colors_list = [\n",
        "    '102;194;165', '252;141;98', '141;160;203',\n",
        "    '231;138;195', '166;216;84', '255;217;47'\n",
        "]\n",
        "\n",
        "def show_tokens(sentence, tokenizer_name):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
        "    token_ids = tokenizer(sentence).input_ids\n",
        "    for idx, t in enumerate(token_ids):\n",
        "        # 텍스트를 인코딩한 후 다시 디코딩했을 때 원본 텍스트와 동일해지려면\n",
        "        # clean_up_tokenization_spaces를 False로 지정해야 합니다.\n",
        "        # 현재 이 매개변수의 기본값은 None(True에 해당)이며\n",
        "        # transformers 4.45에서 True로 바뀔 예정입니다.\n",
        "        # https://github.com/huggingface/transformers/issues/31884\n",
        "        print(\n",
        "            f'\\x1b[0;30;48;2;{colors_list[idx % len(colors_list)]}m' +\n",
        "            tokenizer.decode(t, clean_up_tokenization_spaces=False) +\n",
        "            '\\x1b[0m',\n",
        "            end=' '\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Gcc3JjwX5DK-"
      },
      "outputs": [],
      "source": [
        "text = \"\"\"\n",
        "English and CAPITALIZATION\n",
        "🎵 鸟\n",
        "show_tokens False None elif == >= else: two tabs:\"\t\t\" four spaces:\"    \"\n",
        "12.0*50=600\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **BERT 베이스 모델(uncased)(2018)**\n",
        "    - 토큰화 방법 : WordPiece\n",
        "    - 어휘사전 크기 : 30, 522\n",
        "    - 특수 토큰:\n",
        "        - [UNK] unk_token : 토크나이저가 인코딩 방법을 모르는 토큰\n",
        "        - [SEP] sep_toketn :  특정 작업에서 두 개의 텍스트를 구분하기 위한 토큰(cross-endcoder)\n",
        "        - [PAD] pad_token :  모델 입력에서 사용되지 않는 위치를 채우기 위한 패딩 토큰 --> 모델은 특정 길이(문맥 크기)의 입력을 기대하기 때문\n",
        "        - [CLS] cls_token :  분류 작업을 위한 특수 토큰\n",
        "        - [MASK] mask_token : 훈련 과정 동안 일부 토큰을 감추기 위해 사용되는 마스킹 토큰\n",
        "    - 특징 : 줄 바꿈 인코딩 정보를 알지 못함"
      ],
      "metadata": {
        "id": "CGIMgVxflv7S"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "fCDGSXP75Hv-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bab0614b-1cc2-477f-baf0-95b123f36432"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0;30;48;2;102;194;165m[CLS]\u001b[0m \u001b[0;30;48;2;252;141;98menglish\u001b[0m \u001b[0;30;48;2;141;160;203mand\u001b[0m \u001b[0;30;48;2;231;138;195mcapital\u001b[0m \u001b[0;30;48;2;166;216;84m##ization\u001b[0m \u001b[0;30;48;2;255;217;47m[UNK]\u001b[0m \u001b[0;30;48;2;102;194;165m[UNK]\u001b[0m \u001b[0;30;48;2;252;141;98mshow\u001b[0m \u001b[0;30;48;2;141;160;203m_\u001b[0m \u001b[0;30;48;2;231;138;195mtoken\u001b[0m \u001b[0;30;48;2;166;216;84m##s\u001b[0m \u001b[0;30;48;2;255;217;47mfalse\u001b[0m \u001b[0;30;48;2;102;194;165mnone\u001b[0m \u001b[0;30;48;2;252;141;98meli\u001b[0m \u001b[0;30;48;2;141;160;203m##f\u001b[0m \u001b[0;30;48;2;231;138;195m=\u001b[0m \u001b[0;30;48;2;166;216;84m=\u001b[0m \u001b[0;30;48;2;255;217;47m>\u001b[0m \u001b[0;30;48;2;102;194;165m=\u001b[0m \u001b[0;30;48;2;252;141;98melse\u001b[0m \u001b[0;30;48;2;141;160;203m:\u001b[0m \u001b[0;30;48;2;231;138;195mtwo\u001b[0m \u001b[0;30;48;2;166;216;84mtab\u001b[0m \u001b[0;30;48;2;255;217;47m##s\u001b[0m \u001b[0;30;48;2;102;194;165m:\u001b[0m \u001b[0;30;48;2;252;141;98m\"\u001b[0m \u001b[0;30;48;2;141;160;203m\"\u001b[0m \u001b[0;30;48;2;231;138;195mfour\u001b[0m \u001b[0;30;48;2;166;216;84mspaces\u001b[0m \u001b[0;30;48;2;255;217;47m:\u001b[0m \u001b[0;30;48;2;102;194;165m\"\u001b[0m \u001b[0;30;48;2;252;141;98m\"\u001b[0m \u001b[0;30;48;2;141;160;203m12\u001b[0m \u001b[0;30;48;2;231;138;195m.\u001b[0m \u001b[0;30;48;2;166;216;84m0\u001b[0m \u001b[0;30;48;2;255;217;47m*\u001b[0m \u001b[0;30;48;2;102;194;165m50\u001b[0m \u001b[0;30;48;2;252;141;98m=\u001b[0m \u001b[0;30;48;2;141;160;203m600\u001b[0m \u001b[0;30;48;2;231;138;195m[SEP]\u001b[0m "
          ]
        }
      ],
      "source": [
        "show_tokens(text, \"bert-base-uncased\")  # Uncased : 대소문자를 구분하지 않음"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **BERT 베이스 모델(cased)(2018)**\n",
        "    - 토큰화 방법 : WordPiece\n",
        "    - 어휘사전 크기 : 28,996\n",
        "    - 특수 토큰: uncased 버전과 동일"
      ],
      "metadata": {
        "id": "Aks_7WuCnqKS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "대소문자는 구분하지만 완성도 높지 X"
      ],
      "metadata": {
        "id": "m4W12-IjxnuF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "0Ay_NX3K5HyP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22930563-b5f0-48c3-eb80-c25fdfddc1d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0;30;48;2;102;194;165m[CLS]\u001b[0m \u001b[0;30;48;2;252;141;98mEnglish\u001b[0m \u001b[0;30;48;2;141;160;203mand\u001b[0m \u001b[0;30;48;2;231;138;195mCA\u001b[0m \u001b[0;30;48;2;166;216;84m##PI\u001b[0m \u001b[0;30;48;2;255;217;47m##TA\u001b[0m \u001b[0;30;48;2;102;194;165m##L\u001b[0m \u001b[0;30;48;2;252;141;98m##I\u001b[0m \u001b[0;30;48;2;141;160;203m##Z\u001b[0m \u001b[0;30;48;2;231;138;195m##AT\u001b[0m \u001b[0;30;48;2;166;216;84m##ION\u001b[0m \u001b[0;30;48;2;255;217;47m[UNK]\u001b[0m \u001b[0;30;48;2;102;194;165m[UNK]\u001b[0m \u001b[0;30;48;2;252;141;98mshow\u001b[0m \u001b[0;30;48;2;141;160;203m_\u001b[0m \u001b[0;30;48;2;231;138;195mtoken\u001b[0m \u001b[0;30;48;2;166;216;84m##s\u001b[0m \u001b[0;30;48;2;255;217;47mF\u001b[0m \u001b[0;30;48;2;102;194;165m##als\u001b[0m \u001b[0;30;48;2;252;141;98m##e\u001b[0m \u001b[0;30;48;2;141;160;203mNone\u001b[0m \u001b[0;30;48;2;231;138;195mel\u001b[0m \u001b[0;30;48;2;166;216;84m##if\u001b[0m \u001b[0;30;48;2;255;217;47m=\u001b[0m \u001b[0;30;48;2;102;194;165m=\u001b[0m \u001b[0;30;48;2;252;141;98m>\u001b[0m \u001b[0;30;48;2;141;160;203m=\u001b[0m \u001b[0;30;48;2;231;138;195melse\u001b[0m \u001b[0;30;48;2;166;216;84m:\u001b[0m \u001b[0;30;48;2;255;217;47mtwo\u001b[0m \u001b[0;30;48;2;102;194;165mta\u001b[0m \u001b[0;30;48;2;252;141;98m##bs\u001b[0m \u001b[0;30;48;2;141;160;203m:\u001b[0m \u001b[0;30;48;2;231;138;195m\"\u001b[0m \u001b[0;30;48;2;166;216;84m\"\u001b[0m \u001b[0;30;48;2;255;217;47mfour\u001b[0m \u001b[0;30;48;2;102;194;165mspaces\u001b[0m \u001b[0;30;48;2;252;141;98m:\u001b[0m \u001b[0;30;48;2;141;160;203m\"\u001b[0m \u001b[0;30;48;2;231;138;195m\"\u001b[0m \u001b[0;30;48;2;166;216;84m12\u001b[0m \u001b[0;30;48;2;255;217;47m.\u001b[0m \u001b[0;30;48;2;102;194;165m0\u001b[0m \u001b[0;30;48;2;252;141;98m*\u001b[0m \u001b[0;30;48;2;141;160;203m50\u001b[0m \u001b[0;30;48;2;231;138;195m=\u001b[0m \u001b[0;30;48;2;166;216;84m600\u001b[0m \u001b[0;30;48;2;255;217;47m[SEP]\u001b[0m "
          ]
        }
      ],
      "source": [
        "show_tokens(text, \"bert-base-cased\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **GPT-2(2019)**\n",
        "    - 토큰화 방법 : BPE\n",
        "    - 어휘사전 크기 : 50,257\n",
        "    - 특수 토큰:\n",
        "        - <|endoftext|>\n",
        "    - 특징: 줄바꿈이 토크나이저 내에서 표현됨"
      ],
      "metadata": {
        "id": "Gm9BFfkgoQL1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "K_k5QduY5H0u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd9d8e5a-78fd-4e64-f8bd-113adb3f66f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0;30;48;2;102;194;165m\n",
            "\u001b[0m \u001b[0;30;48;2;252;141;98mEnglish\u001b[0m \u001b[0;30;48;2;141;160;203m and\u001b[0m \u001b[0;30;48;2;231;138;195m CAP\u001b[0m \u001b[0;30;48;2;166;216;84mITAL\u001b[0m \u001b[0;30;48;2;255;217;47mIZ\u001b[0m \u001b[0;30;48;2;102;194;165mATION\u001b[0m \u001b[0;30;48;2;252;141;98m\n",
            "\u001b[0m \u001b[0;30;48;2;141;160;203m�\u001b[0m \u001b[0;30;48;2;231;138;195m�\u001b[0m \u001b[0;30;48;2;166;216;84m�\u001b[0m \u001b[0;30;48;2;255;217;47m �\u001b[0m \u001b[0;30;48;2;102;194;165m�\u001b[0m \u001b[0;30;48;2;252;141;98m�\u001b[0m \u001b[0;30;48;2;141;160;203m\n",
            "\u001b[0m \u001b[0;30;48;2;231;138;195mshow\u001b[0m \u001b[0;30;48;2;166;216;84m_\u001b[0m \u001b[0;30;48;2;255;217;47mt\u001b[0m \u001b[0;30;48;2;102;194;165mok\u001b[0m \u001b[0;30;48;2;252;141;98mens\u001b[0m \u001b[0;30;48;2;141;160;203m False\u001b[0m \u001b[0;30;48;2;231;138;195m None\u001b[0m \u001b[0;30;48;2;166;216;84m el\u001b[0m \u001b[0;30;48;2;255;217;47mif\u001b[0m \u001b[0;30;48;2;102;194;165m ==\u001b[0m \u001b[0;30;48;2;252;141;98m >=\u001b[0m \u001b[0;30;48;2;141;160;203m else\u001b[0m \u001b[0;30;48;2;231;138;195m:\u001b[0m \u001b[0;30;48;2;166;216;84m two\u001b[0m \u001b[0;30;48;2;255;217;47m tabs\u001b[0m \u001b[0;30;48;2;102;194;165m:\"\u001b[0m \u001b[0;30;48;2;252;141;98m\t\u001b[0m \u001b[0;30;48;2;141;160;203m\t\u001b[0m \u001b[0;30;48;2;231;138;195m\"\u001b[0m \u001b[0;30;48;2;166;216;84m four\u001b[0m \u001b[0;30;48;2;255;217;47m spaces\u001b[0m \u001b[0;30;48;2;102;194;165m:\"\u001b[0m \u001b[0;30;48;2;252;141;98m \u001b[0m \u001b[0;30;48;2;141;160;203m \u001b[0m \u001b[0;30;48;2;231;138;195m \u001b[0m \u001b[0;30;48;2;166;216;84m \"\u001b[0m \u001b[0;30;48;2;255;217;47m\n",
            "\u001b[0m \u001b[0;30;48;2;102;194;165m12\u001b[0m \u001b[0;30;48;2;252;141;98m.\u001b[0m \u001b[0;30;48;2;141;160;203m0\u001b[0m \u001b[0;30;48;2;231;138;195m*\u001b[0m \u001b[0;30;48;2;166;216;84m50\u001b[0m \u001b[0;30;48;2;255;217;47m=\u001b[0m \u001b[0;30;48;2;102;194;165m600\u001b[0m \u001b[0;30;48;2;252;141;98m\n",
            "\u001b[0m "
          ]
        }
      ],
      "source": [
        "show_tokens(text, \"gpt2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Flan-T5(2022)**\n",
        "    - 토큰화 방법 : SentencePiece\n",
        "    - 어휘사전 크기 : 32,100\n",
        "    - 특수 토큰:\n",
        "        - `<unk>` unk_token\n",
        "        - `<pad>` pad_token\n",
        "    - 특징 :\n",
        "        - 줄바꿈이나 공백 토큰이 없음 --> 모델이 코드를 다루기 어렵다.\n",
        "        - 이모자와 한자가 모두 <unk> 토큰으로 바꾸었음 --> 모델이 이런 토큰을 식별하지 못함"
      ],
      "metadata": {
        "id": "G0CUY7TYpNsh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "EJn5nf3c5H2_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0362c56-86e7-4bb5-d679-3e255b270445"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0;30;48;2;102;194;165mEnglish\u001b[0m \u001b[0;30;48;2;252;141;98mand\u001b[0m \u001b[0;30;48;2;141;160;203mCA\u001b[0m \u001b[0;30;48;2;231;138;195mPI\u001b[0m \u001b[0;30;48;2;166;216;84mTAL\u001b[0m \u001b[0;30;48;2;255;217;47mIZ\u001b[0m \u001b[0;30;48;2;102;194;165mATION\u001b[0m \u001b[0;30;48;2;252;141;98m\u001b[0m \u001b[0;30;48;2;141;160;203m<unk>\u001b[0m \u001b[0;30;48;2;231;138;195m\u001b[0m \u001b[0;30;48;2;166;216;84m<unk>\u001b[0m \u001b[0;30;48;2;255;217;47mshow\u001b[0m \u001b[0;30;48;2;102;194;165m_\u001b[0m \u001b[0;30;48;2;252;141;98mto\u001b[0m \u001b[0;30;48;2;141;160;203mken\u001b[0m \u001b[0;30;48;2;231;138;195ms\u001b[0m \u001b[0;30;48;2;166;216;84mFal\u001b[0m \u001b[0;30;48;2;255;217;47ms\u001b[0m \u001b[0;30;48;2;102;194;165me\u001b[0m \u001b[0;30;48;2;252;141;98mNone\u001b[0m \u001b[0;30;48;2;141;160;203m\u001b[0m \u001b[0;30;48;2;231;138;195me\u001b[0m \u001b[0;30;48;2;166;216;84ml\u001b[0m \u001b[0;30;48;2;255;217;47mif\u001b[0m \u001b[0;30;48;2;102;194;165m=\u001b[0m \u001b[0;30;48;2;252;141;98m=\u001b[0m \u001b[0;30;48;2;141;160;203m>\u001b[0m \u001b[0;30;48;2;231;138;195m=\u001b[0m \u001b[0;30;48;2;166;216;84melse\u001b[0m \u001b[0;30;48;2;255;217;47m:\u001b[0m \u001b[0;30;48;2;102;194;165mtwo\u001b[0m \u001b[0;30;48;2;252;141;98mtab\u001b[0m \u001b[0;30;48;2;141;160;203ms\u001b[0m \u001b[0;30;48;2;231;138;195m:\u001b[0m \u001b[0;30;48;2;166;216;84m\"\u001b[0m \u001b[0;30;48;2;255;217;47m\"\u001b[0m \u001b[0;30;48;2;102;194;165mfour\u001b[0m \u001b[0;30;48;2;252;141;98mspaces\u001b[0m \u001b[0;30;48;2;141;160;203m:\u001b[0m \u001b[0;30;48;2;231;138;195m\"\u001b[0m \u001b[0;30;48;2;166;216;84m\"\u001b[0m \u001b[0;30;48;2;255;217;47m12.\u001b[0m \u001b[0;30;48;2;102;194;165m0\u001b[0m \u001b[0;30;48;2;252;141;98m*\u001b[0m \u001b[0;30;48;2;141;160;203m50\u001b[0m \u001b[0;30;48;2;231;138;195m=\u001b[0m \u001b[0;30;48;2;166;216;84m600\u001b[0m \u001b[0;30;48;2;255;217;47m\u001b[0m \u001b[0;30;48;2;102;194;165m</s>\u001b[0m "
          ]
        }
      ],
      "source": [
        "show_tokens(text, \"google/flan-t5-small\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **GPT-4(2023)**\n",
        "    - 토큰화 방법 : BPE\n",
        "    - 어휘사전 크기 : 100,000 이상\n",
        "    - 특수 토큰:\n",
        "        - `<endoftext>`\n",
        "        - 중간 토큰을 채우도록 훈련됨. 세 개의 특수 토큰을 사용해 앞, 뒤에 나오는 텍스트를 고려해 LLM이 완성된 문장을 생성함\n",
        "            - <|fim_prefix|>\n",
        "            - <|fim_middle|>\n",
        "            - <|fim_suffix|>\n",
        "    - 특징 :\n",
        "        - GPT-2 토크나이저와 비슷하게 동작\n",
        "        - GPT-4는 4개의 공백을 하나의 토큰으로 표현\n",
        "        - elif를 하나의 토큰으로 표현 --> 자연어외에 코드에 초점을 맞추고 있음\n",
        "        - 더 적은 토큰을 사용해 대부분의 단어를 표현함(CAPITALIZATION 두 개의 토큰)"
      ],
      "metadata": {
        "id": "bxZ45wZoqYBo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "1ymhAsTg5H5e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2779bf0-95a8-4b72-ffdf-8d64e7605acb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0;30;48;2;102;194;165m\n",
            "\u001b[0m \u001b[0;30;48;2;252;141;98mEnglish\u001b[0m \u001b[0;30;48;2;141;160;203m and\u001b[0m \u001b[0;30;48;2;231;138;195m CAPITAL\u001b[0m \u001b[0;30;48;2;166;216;84mIZATION\u001b[0m \u001b[0;30;48;2;255;217;47m\n",
            "\u001b[0m \u001b[0;30;48;2;102;194;165m�\u001b[0m \u001b[0;30;48;2;252;141;98m�\u001b[0m \u001b[0;30;48;2;141;160;203m�\u001b[0m \u001b[0;30;48;2;231;138;195m �\u001b[0m \u001b[0;30;48;2;166;216;84m�\u001b[0m \u001b[0;30;48;2;255;217;47m�\u001b[0m \u001b[0;30;48;2;102;194;165m\n",
            "\u001b[0m \u001b[0;30;48;2;252;141;98mshow\u001b[0m \u001b[0;30;48;2;141;160;203m_tokens\u001b[0m \u001b[0;30;48;2;231;138;195m False\u001b[0m \u001b[0;30;48;2;166;216;84m None\u001b[0m \u001b[0;30;48;2;255;217;47m elif\u001b[0m \u001b[0;30;48;2;102;194;165m ==\u001b[0m \u001b[0;30;48;2;252;141;98m >=\u001b[0m \u001b[0;30;48;2;141;160;203m else\u001b[0m \u001b[0;30;48;2;231;138;195m:\u001b[0m \u001b[0;30;48;2;166;216;84m two\u001b[0m \u001b[0;30;48;2;255;217;47m tabs\u001b[0m \u001b[0;30;48;2;102;194;165m:\"\u001b[0m \u001b[0;30;48;2;252;141;98m\t\u001b[0m \u001b[0;30;48;2;141;160;203m\t\u001b[0m \u001b[0;30;48;2;231;138;195m\"\u001b[0m \u001b[0;30;48;2;166;216;84m four\u001b[0m \u001b[0;30;48;2;255;217;47m spaces\u001b[0m \u001b[0;30;48;2;102;194;165m:\"\u001b[0m \u001b[0;30;48;2;252;141;98m   \u001b[0m \u001b[0;30;48;2;141;160;203m \"\n",
            "\u001b[0m \u001b[0;30;48;2;231;138;195m12\u001b[0m \u001b[0;30;48;2;166;216;84m.\u001b[0m \u001b[0;30;48;2;255;217;47m0\u001b[0m \u001b[0;30;48;2;102;194;165m*\u001b[0m \u001b[0;30;48;2;252;141;98m50\u001b[0m \u001b[0;30;48;2;141;160;203m=\u001b[0m \u001b[0;30;48;2;231;138;195m600\u001b[0m \u001b[0;30;48;2;166;216;84m\n",
            "\u001b[0m "
          ]
        }
      ],
      "source": [
        "# 공식 토크나이저는 `tiktoken`이지만 허깅 페이스 플랫폼에 동일한 토크나이저가 있습니다.\n",
        "show_tokens(text, \"Xenova/gpt-4\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **StarCoder2(2024)**\n",
        "    - 코드 생성에 초점을 맞춘 150개의 파라미터를 가진 디코더 모델\n",
        "    - 토큰화 방법 : BPE\n",
        "    - 어휘사전 크기 : 49,152\n",
        "    - 특수 토큰:\n",
        "        - `<endoftext>`\n",
        "        - 중간 채우기를 위한 토큰: `<fim_prefix>`, `<fim_middle>`, `<fim_suffix>`, `<fim_pad>`\n",
        "        - `<filename>`, `<reponame>`, `<gh_stars>`\n",
        "    - 특징 :\n",
        "        - 코드를 표현할 때 문맥 관리가 중요(예를 들어 파일에서 다른 파일에 정의된 함수를 호출하는 경우)\n",
        "        - 모델은 같은 저장소의 다른 파일에 있는 코드를 식별하고 다른 저장소에 있는 코드와 구분할 수 있어야함"
      ],
      "metadata": {
        "id": "iqgzgVfSsW7q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "3_vAyeTy5H7_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9253c41b-05ff-408a-d458-9af1610313a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0;30;48;2;102;194;165m\n",
            "\u001b[0m \u001b[0;30;48;2;252;141;98mEnglish\u001b[0m \u001b[0;30;48;2;141;160;203m and\u001b[0m \u001b[0;30;48;2;231;138;195m CAPITAL\u001b[0m \u001b[0;30;48;2;166;216;84mIZATION\u001b[0m \u001b[0;30;48;2;255;217;47m\n",
            "\u001b[0m \u001b[0;30;48;2;102;194;165m�\u001b[0m \u001b[0;30;48;2;252;141;98m�\u001b[0m \u001b[0;30;48;2;141;160;203m�\u001b[0m \u001b[0;30;48;2;231;138;195m \u001b[0m \u001b[0;30;48;2;166;216;84m�\u001b[0m \u001b[0;30;48;2;255;217;47m�\u001b[0m \u001b[0;30;48;2;102;194;165m\n",
            "\u001b[0m \u001b[0;30;48;2;252;141;98mshow\u001b[0m \u001b[0;30;48;2;141;160;203m_\u001b[0m \u001b[0;30;48;2;231;138;195mtokens\u001b[0m \u001b[0;30;48;2;166;216;84m False\u001b[0m \u001b[0;30;48;2;255;217;47m None\u001b[0m \u001b[0;30;48;2;102;194;165m elif\u001b[0m \u001b[0;30;48;2;252;141;98m ==\u001b[0m \u001b[0;30;48;2;141;160;203m >=\u001b[0m \u001b[0;30;48;2;231;138;195m else\u001b[0m \u001b[0;30;48;2;166;216;84m:\u001b[0m \u001b[0;30;48;2;255;217;47m two\u001b[0m \u001b[0;30;48;2;102;194;165m tabs\u001b[0m \u001b[0;30;48;2;252;141;98m:\"\u001b[0m \u001b[0;30;48;2;141;160;203m\t\u001b[0m \u001b[0;30;48;2;231;138;195m\t\u001b[0m \u001b[0;30;48;2;166;216;84m\"\u001b[0m \u001b[0;30;48;2;255;217;47m four\u001b[0m \u001b[0;30;48;2;102;194;165m spaces\u001b[0m \u001b[0;30;48;2;252;141;98m:\"\u001b[0m \u001b[0;30;48;2;141;160;203m   \u001b[0m \u001b[0;30;48;2;231;138;195m \"\u001b[0m \u001b[0;30;48;2;166;216;84m\n",
            "\u001b[0m \u001b[0;30;48;2;255;217;47m1\u001b[0m \u001b[0;30;48;2;102;194;165m2\u001b[0m \u001b[0;30;48;2;252;141;98m.\u001b[0m \u001b[0;30;48;2;141;160;203m0\u001b[0m \u001b[0;30;48;2;231;138;195m*\u001b[0m \u001b[0;30;48;2;166;216;84m5\u001b[0m \u001b[0;30;48;2;255;217;47m0\u001b[0m \u001b[0;30;48;2;102;194;165m=\u001b[0m \u001b[0;30;48;2;252;141;98m6\u001b[0m \u001b[0;30;48;2;141;160;203m0\u001b[0m \u001b[0;30;48;2;231;138;195m0\u001b[0m \u001b[0;30;48;2;166;216;84m\n",
            "\u001b[0m "
          ]
        }
      ],
      "source": [
        "show_tokens(text, \"bigcode/starcoder2-15b\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Galatica**\n",
        "\n",
        "    - 과학 지식에 초점을 맞추어 많은 과학 논문, 참고 자료, 지식 데이터에서 훈련됨.\n",
        "    - 토큰화에 더 주의를 기울여 데이터셋에 있는 뉘앙스에 민감함 --> 인요, 추론, 수학, 펩타이드 서열, DNA 서열을 위한 특수 토큰이 있음\n",
        "    -\n",
        "    - 토큰화 방법 : BPE\n",
        "    - 어휘사전 크기 : 50,000\n",
        "    - 특수 토큰:\n",
        "        - `<s>`\n",
        "        - `<pad>`\n",
        "        - `</s>`,\n",
        "        - `<unk>`\n",
        "        - 참조/인용은 [START_REF]와 [END_REF]로 감쌈\n",
        "        - 단계별 추론: <work>는 모델이 CoT(chain-of-thought)추론에 사용하는 토큰\n",
        "    - 특징 :\n",
        "        - 코드를 염두에 둠, StarCoder2와 비슷하게 동작\n",
        "        - 탭도 하나의 토큰으로 인코딩"
      ],
      "metadata": {
        "id": "XlLf_HfNuW0T"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "KeWcUdxY6I3u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dab3c0e2-afcd-4537-f7b1-812c83a41dd4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0;30;48;2;102;194;165m\n",
            "\u001b[0m \u001b[0;30;48;2;252;141;98mEnglish\u001b[0m \u001b[0;30;48;2;141;160;203m and\u001b[0m \u001b[0;30;48;2;231;138;195m CAP\u001b[0m \u001b[0;30;48;2;166;216;84mITAL\u001b[0m \u001b[0;30;48;2;255;217;47mIZATION\u001b[0m \u001b[0;30;48;2;102;194;165m\n",
            "\u001b[0m \u001b[0;30;48;2;252;141;98m�\u001b[0m \u001b[0;30;48;2;141;160;203m�\u001b[0m \u001b[0;30;48;2;231;138;195m�\u001b[0m \u001b[0;30;48;2;166;216;84m�\u001b[0m \u001b[0;30;48;2;255;217;47m �\u001b[0m \u001b[0;30;48;2;102;194;165m�\u001b[0m \u001b[0;30;48;2;252;141;98m�\u001b[0m \u001b[0;30;48;2;141;160;203m\n",
            "\u001b[0m \u001b[0;30;48;2;231;138;195mshow\u001b[0m \u001b[0;30;48;2;166;216;84m_\u001b[0m \u001b[0;30;48;2;255;217;47mtokens\u001b[0m \u001b[0;30;48;2;102;194;165m False\u001b[0m \u001b[0;30;48;2;252;141;98m None\u001b[0m \u001b[0;30;48;2;141;160;203m elif\u001b[0m \u001b[0;30;48;2;231;138;195m \u001b[0m \u001b[0;30;48;2;166;216;84m==\u001b[0m \u001b[0;30;48;2;255;217;47m \u001b[0m \u001b[0;30;48;2;102;194;165m>\u001b[0m \u001b[0;30;48;2;252;141;98m=\u001b[0m \u001b[0;30;48;2;141;160;203m else\u001b[0m \u001b[0;30;48;2;231;138;195m:\u001b[0m \u001b[0;30;48;2;166;216;84m two\u001b[0m \u001b[0;30;48;2;255;217;47m t\u001b[0m \u001b[0;30;48;2;102;194;165mabs\u001b[0m \u001b[0;30;48;2;252;141;98m:\u001b[0m \u001b[0;30;48;2;141;160;203m\"\u001b[0m \u001b[0;30;48;2;231;138;195m\t\t\u001b[0m \u001b[0;30;48;2;166;216;84m\"\u001b[0m \u001b[0;30;48;2;255;217;47m four\u001b[0m \u001b[0;30;48;2;102;194;165m spaces\u001b[0m \u001b[0;30;48;2;252;141;98m:\u001b[0m \u001b[0;30;48;2;141;160;203m\"\u001b[0m \u001b[0;30;48;2;231;138;195m    \u001b[0m \u001b[0;30;48;2;166;216;84m\"\u001b[0m \u001b[0;30;48;2;255;217;47m\n",
            "\u001b[0m \u001b[0;30;48;2;102;194;165m1\u001b[0m \u001b[0;30;48;2;252;141;98m2\u001b[0m \u001b[0;30;48;2;141;160;203m.\u001b[0m \u001b[0;30;48;2;231;138;195m0\u001b[0m \u001b[0;30;48;2;166;216;84m*\u001b[0m \u001b[0;30;48;2;255;217;47m5\u001b[0m \u001b[0;30;48;2;102;194;165m0\u001b[0m \u001b[0;30;48;2;252;141;98m=\u001b[0m \u001b[0;30;48;2;141;160;203m6\u001b[0m \u001b[0;30;48;2;231;138;195m0\u001b[0m \u001b[0;30;48;2;166;216;84m0\u001b[0m \u001b[0;30;48;2;255;217;47m\n",
            "\u001b[0m "
          ]
        }
      ],
      "source": [
        "show_tokens(text, \"facebook/galactica-1.3b\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Phi-3(Llama 2)**\n",
        "    - 여러 개의 특수 토큰을 추가한 Llama 2 토크나이저 재사용\n",
        "    -\n",
        "    - 토큰화 방법 : BPE\n",
        "    - 어휘사전 크기 : 32,000\n",
        "    - 특수 토큰:\n",
        "        - `<|endoftext|>`\n",
        "        - 채팅토큰\n",
        "            - `<|user|>`,\n",
        "            - `<|assistant|>`\n",
        "            - `<|system|>`\n",
        "    - 특징 :\n",
        "        - 채팅에 초점을 맞춤"
      ],
      "metadata": {
        "id": "f-Ua2aSSwb7F"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "__QNj2Cohzz2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b682617e-8bcd-4d0c-868d-e4803b8800a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0;30;48;2;102;194;165m\u001b[0m \u001b[0;30;48;2;252;141;98m\n",
            "\u001b[0m \u001b[0;30;48;2;141;160;203mEnglish\u001b[0m \u001b[0;30;48;2;231;138;195mand\u001b[0m \u001b[0;30;48;2;166;216;84mC\u001b[0m \u001b[0;30;48;2;255;217;47mAP\u001b[0m \u001b[0;30;48;2;102;194;165mIT\u001b[0m \u001b[0;30;48;2;252;141;98mAL\u001b[0m \u001b[0;30;48;2;141;160;203mIZ\u001b[0m \u001b[0;30;48;2;231;138;195mATION\u001b[0m \u001b[0;30;48;2;166;216;84m\n",
            "\u001b[0m \u001b[0;30;48;2;255;217;47m�\u001b[0m \u001b[0;30;48;2;102;194;165m�\u001b[0m \u001b[0;30;48;2;252;141;98m�\u001b[0m \u001b[0;30;48;2;141;160;203m�\u001b[0m \u001b[0;30;48;2;231;138;195m\u001b[0m \u001b[0;30;48;2;166;216;84m�\u001b[0m \u001b[0;30;48;2;255;217;47m�\u001b[0m \u001b[0;30;48;2;102;194;165m�\u001b[0m \u001b[0;30;48;2;252;141;98m\n",
            "\u001b[0m \u001b[0;30;48;2;141;160;203mshow\u001b[0m \u001b[0;30;48;2;231;138;195m_\u001b[0m \u001b[0;30;48;2;166;216;84mto\u001b[0m \u001b[0;30;48;2;255;217;47mkens\u001b[0m \u001b[0;30;48;2;102;194;165mFalse\u001b[0m \u001b[0;30;48;2;252;141;98mNone\u001b[0m \u001b[0;30;48;2;141;160;203melif\u001b[0m \u001b[0;30;48;2;231;138;195m==\u001b[0m \u001b[0;30;48;2;166;216;84m>=\u001b[0m \u001b[0;30;48;2;255;217;47melse\u001b[0m \u001b[0;30;48;2;102;194;165m:\u001b[0m \u001b[0;30;48;2;252;141;98mtwo\u001b[0m \u001b[0;30;48;2;141;160;203mtabs\u001b[0m \u001b[0;30;48;2;231;138;195m:\"\u001b[0m \u001b[0;30;48;2;166;216;84m\t\u001b[0m \u001b[0;30;48;2;255;217;47m\t\u001b[0m \u001b[0;30;48;2;102;194;165m\"\u001b[0m \u001b[0;30;48;2;252;141;98mfour\u001b[0m \u001b[0;30;48;2;141;160;203mspaces\u001b[0m \u001b[0;30;48;2;231;138;195m:\"\u001b[0m \u001b[0;30;48;2;166;216;84m  \u001b[0m \u001b[0;30;48;2;255;217;47m\"\u001b[0m \u001b[0;30;48;2;102;194;165m\n",
            "\u001b[0m \u001b[0;30;48;2;252;141;98m1\u001b[0m \u001b[0;30;48;2;141;160;203m2\u001b[0m \u001b[0;30;48;2;231;138;195m.\u001b[0m \u001b[0;30;48;2;166;216;84m0\u001b[0m \u001b[0;30;48;2;255;217;47m*\u001b[0m \u001b[0;30;48;2;102;194;165m5\u001b[0m \u001b[0;30;48;2;252;141;98m0\u001b[0m \u001b[0;30;48;2;141;160;203m=\u001b[0m \u001b[0;30;48;2;231;138;195m6\u001b[0m \u001b[0;30;48;2;166;216;84m0\u001b[0m \u001b[0;30;48;2;255;217;47m0\u001b[0m \u001b[0;30;48;2;102;194;165m\n",
            "\u001b[0m "
          ]
        }
      ],
      "source": [
        "show_tokens(text, \"microsoft/Phi-3-mini-4k-instruct\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "fyUC440f78ZS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **한국어 특화된 토크나이저**"
      ],
      "metadata": {
        "id": "cYujEh7i7Zk_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **한글의 특수성**:\n",
        "    - **교착어**: 조사가 붙어 단어 형태가 무한대로 변형 (\"학교\", \"학교가\", \"학교에서\", \"학교로부터\"...)\n",
        "    - **띄어쓰기 불규칙**: SNS, 댓글 등에서 띄어쓰기가 일관되지 않음\n",
        "    - **자모 조합**: 초성+중성+종성이 결합되어 하나의 음절 형성 (ex: ㄱ+ㅏ+ㅁ = 감)\n",
        "\n",
        "- 발전 과정:\n",
        "    - 2013-2016: KoNLPy, 형태소 분석기 (Mecab, Okt) 등장\n",
        "    - 2018: 다국어 BERT에 한국어 포함되었으나 성능 부족\n",
        "    - 2020: KoBERT (SKT) 출시 - 한국어 Wikipedia로 학습\n",
        "    - 2021: KoGPT, KoBART 등 한국어 특화 모델 등장\n",
        "    - 2022-2024: KoAlpaca, Polyglot-Ko, KULLM 등 다양한 한국어 LLM 개발\n",
        "\n",
        "- 주요 참고자료:\n",
        "    - SKT KoBERT: https://github.com/SKTBrain/KoBERT\n",
        "    - Kakao KoGPT: https://github.com/kakaobrain/kogpt"
      ],
      "metadata": {
        "id": "YmHB-GYj7fVy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **[미션]** 한국어 LLM 찾아 토큰화 해보기.\n",
        "- 허깅페이스에서 사전학습된 한국어 LLM을 여러 개(2개 이상) 찾고 위와 같이 토큰화 & 비교해보세요."
      ],
      "metadata": {
        "id": "vuXMBfD9F2vc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ko_text = \"\"\"\n",
        "    안녕하세요! 저는 인공지능을 공부하고 있습니다.,\n",
        "    한글은 세종대왕이 창제하신 문자입니다. 😊,\n",
        "    카카오와 네이버는 한국의 대표적인 IT 기업입니다.,\n",
        "    자연어처리(NLP)는 컴퓨터가 인간의 언어를 이해하도록 하는 기술이다.,\n",
        "\"\"\"\n",
        "\n",
        "# 예제 : SKT의 KoBERT - 비교적 안정적\n",
        "show_tokens(ko_text, \"skt/kobert-base-v1\")"
      ],
      "metadata": {
        "id": "c1mnnX3D_BWA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22d4a3c4-7627-4a8c-f28a-6da5e3b5b8cc"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0;30;48;2;102;194;165m\u001b[0m \u001b[0;30;48;2;252;141;98mᄋ\u001b[0m \u001b[0;30;48;2;141;160;203m[UNK]\u001b[0m \u001b[0;30;48;2;231;138;195mᄒ\u001b[0m \u001b[0;30;48;2;166;216;84m[UNK]\u001b[0m \u001b[0;30;48;2;255;217;47mᄋ\u001b[0m \u001b[0;30;48;2;102;194;165m[UNK]\u001b[0m \u001b[0;30;48;2;252;141;98m!\u001b[0m \u001b[0;30;48;2;141;160;203m\u001b[0m \u001b[0;30;48;2;231;138;195m[UNK]\u001b[0m \u001b[0;30;48;2;166;216;84m\u001b[0m \u001b[0;30;48;2;255;217;47mᄋ\u001b[0m \u001b[0;30;48;2;102;194;165mᅵ\u001b[0m \u001b[0;30;48;2;252;141;98m[UNK]\u001b[0m \u001b[0;30;48;2;141;160;203mᄀ\u001b[0m \u001b[0;30;48;2;231;138;195m[UNK]\u001b[0m \u001b[0;30;48;2;166;216;84mᅵ\u001b[0m \u001b[0;30;48;2;255;217;47m[UNK]\u001b[0m \u001b[0;30;48;2;102;194;165mᄋ\u001b[0m \u001b[0;30;48;2;252;141;98m[UNK]\u001b[0m \u001b[0;30;48;2;141;160;203m\u001b[0m \u001b[0;30;48;2;231;138;195mᄀ\u001b[0m \u001b[0;30;48;2;166;216;84m[UNK]\u001b[0m \u001b[0;30;48;2;255;217;47mᄒ\u001b[0m \u001b[0;30;48;2;102;194;165m[UNK]\u001b[0m \u001b[0;30;48;2;252;141;98mᄀ\u001b[0m \u001b[0;30;48;2;141;160;203m[UNK]\u001b[0m \u001b[0;30;48;2;231;138;195m\u001b[0m \u001b[0;30;48;2;166;216;84mᄋ\u001b[0m \u001b[0;30;48;2;255;217;47mᅵ\u001b[0m \u001b[0;30;48;2;102;194;165m[UNK]\u001b[0m \u001b[0;30;48;2;252;141;98mᅵ\u001b[0m \u001b[0;30;48;2;141;160;203m[UNK]\u001b[0m \u001b[0;30;48;2;231;138;195m.\u001b[0m \u001b[0;30;48;2;166;216;84m,\u001b[0m \u001b[0;30;48;2;255;217;47m\u001b[0m \u001b[0;30;48;2;102;194;165mᄒ\u001b[0m \u001b[0;30;48;2;252;141;98m[UNK]\u001b[0m \u001b[0;30;48;2;141;160;203mᄀ\u001b[0m \u001b[0;30;48;2;231;138;195m[UNK]\u001b[0m \u001b[0;30;48;2;166;216;84mᄋ\u001b[0m \u001b[0;30;48;2;255;217;47m[UNK]\u001b[0m \u001b[0;30;48;2;102;194;165m\u001b[0m \u001b[0;30;48;2;252;141;98m[UNK]\u001b[0m \u001b[0;30;48;2;141;160;203mᄋ\u001b[0m \u001b[0;30;48;2;231;138;195m[UNK]\u001b[0m \u001b[0;30;48;2;166;216;84mᄋ\u001b[0m \u001b[0;30;48;2;255;217;47mᅵ\u001b[0m \u001b[0;30;48;2;102;194;165m\u001b[0m \u001b[0;30;48;2;252;141;98m[UNK]\u001b[0m \u001b[0;30;48;2;141;160;203mᄒ\u001b[0m \u001b[0;30;48;2;231;138;195m[UNK]\u001b[0m \u001b[0;30;48;2;166;216;84mᅵ\u001b[0m \u001b[0;30;48;2;255;217;47m[UNK]\u001b[0m \u001b[0;30;48;2;102;194;165m\u001b[0m \u001b[0;30;48;2;252;141;98m[UNK]\u001b[0m \u001b[0;30;48;2;141;160;203mᄋ\u001b[0m \u001b[0;30;48;2;231;138;195mᅵ\u001b[0m \u001b[0;30;48;2;166;216;84m[UNK]\u001b[0m \u001b[0;30;48;2;255;217;47mᅵ\u001b[0m \u001b[0;30;48;2;102;194;165m[UNK]\u001b[0m \u001b[0;30;48;2;252;141;98m.\u001b[0m \u001b[0;30;48;2;141;160;203m\u001b[0m \u001b[0;30;48;2;231;138;195m[UNK]\u001b[0m \u001b[0;30;48;2;166;216;84m,\u001b[0m \u001b[0;30;48;2;255;217;47m\u001b[0m \u001b[0;30;48;2;102;194;165mᄏ\u001b[0m \u001b[0;30;48;2;252;141;98m[UNK]\u001b[0m \u001b[0;30;48;2;141;160;203mᄏ\u001b[0m \u001b[0;30;48;2;231;138;195m[UNK]\u001b[0m \u001b[0;30;48;2;166;216;84mᄋ\u001b[0m \u001b[0;30;48;2;255;217;47m[UNK]\u001b[0m \u001b[0;30;48;2;102;194;165mᄋ\u001b[0m \u001b[0;30;48;2;252;141;98m[UNK]\u001b[0m \u001b[0;30;48;2;141;160;203m\u001b[0m \u001b[0;30;48;2;231;138;195m[UNK]\u001b[0m \u001b[0;30;48;2;166;216;84mᄋ\u001b[0m \u001b[0;30;48;2;255;217;47mᅵ\u001b[0m \u001b[0;30;48;2;102;194;165m[UNK]\u001b[0m \u001b[0;30;48;2;252;141;98m\u001b[0m \u001b[0;30;48;2;141;160;203mᄒ\u001b[0m \u001b[0;30;48;2;231;138;195m[UNK]\u001b[0m \u001b[0;30;48;2;166;216;84mᄀ\u001b[0m \u001b[0;30;48;2;255;217;47m[UNK]\u001b[0m \u001b[0;30;48;2;102;194;165mᄋ\u001b[0m \u001b[0;30;48;2;252;141;98m[UNK]\u001b[0m \u001b[0;30;48;2;141;160;203m\u001b[0m \u001b[0;30;48;2;231;138;195m[UNK]\u001b[0m \u001b[0;30;48;2;166;216;84mᄋ\u001b[0m \u001b[0;30;48;2;255;217;47mᅵ\u001b[0m \u001b[0;30;48;2;102;194;165m[UNK]\u001b[0m \u001b[0;30;48;2;252;141;98mIT\u001b[0m \u001b[0;30;48;2;141;160;203m\u001b[0m \u001b[0;30;48;2;231;138;195mᄀ\u001b[0m \u001b[0;30;48;2;166;216;84mᅵ\u001b[0m \u001b[0;30;48;2;255;217;47mᄋ\u001b[0m \u001b[0;30;48;2;102;194;165m[UNK]\u001b[0m \u001b[0;30;48;2;252;141;98mᄋ\u001b[0m \u001b[0;30;48;2;141;160;203mᅵ\u001b[0m \u001b[0;30;48;2;231;138;195m[UNK]\u001b[0m \u001b[0;30;48;2;166;216;84mᅵ\u001b[0m \u001b[0;30;48;2;255;217;47m[UNK]\u001b[0m \u001b[0;30;48;2;102;194;165m.\u001b[0m \u001b[0;30;48;2;252;141;98m,\u001b[0m \u001b[0;30;48;2;141;160;203m\u001b[0m \u001b[0;30;48;2;231;138;195m[UNK]\u001b[0m \u001b[0;30;48;2;166;216;84mᄋ\u001b[0m \u001b[0;30;48;2;255;217;47m[UNK]\u001b[0m \u001b[0;30;48;2;102;194;165mᄋ\u001b[0m \u001b[0;30;48;2;252;141;98m[UNK]\u001b[0m \u001b[0;30;48;2;141;160;203mᅵ\u001b[0m \u001b[0;30;48;2;231;138;195m(\u001b[0m \u001b[0;30;48;2;166;216;84mN\u001b[0m \u001b[0;30;48;2;255;217;47mL\u001b[0m \u001b[0;30;48;2;102;194;165mP\u001b[0m \u001b[0;30;48;2;252;141;98m)\u001b[0m \u001b[0;30;48;2;141;160;203m[UNK]\u001b[0m \u001b[0;30;48;2;231;138;195m\u001b[0m \u001b[0;30;48;2;166;216;84mᄏ\u001b[0m \u001b[0;30;48;2;255;217;47m[UNK]\u001b[0m \u001b[0;30;48;2;102;194;165mᄀ\u001b[0m \u001b[0;30;48;2;252;141;98m[UNK]\u001b[0m \u001b[0;30;48;2;141;160;203m\u001b[0m \u001b[0;30;48;2;231;138;195mᄋ\u001b[0m \u001b[0;30;48;2;166;216;84mᅵ\u001b[0m \u001b[0;30;48;2;255;217;47m[UNK]\u001b[0m \u001b[0;30;48;2;102;194;165mᄀ\u001b[0m \u001b[0;30;48;2;252;141;98m[UNK]\u001b[0m \u001b[0;30;48;2;141;160;203mᄋ\u001b[0m \u001b[0;30;48;2;231;138;195m[UNK]\u001b[0m \u001b[0;30;48;2;166;216;84m\u001b[0m \u001b[0;30;48;2;255;217;47mᄋ\u001b[0m \u001b[0;30;48;2;102;194;165m[UNK]\u001b[0m \u001b[0;30;48;2;252;141;98mᄋ\u001b[0m \u001b[0;30;48;2;141;160;203m[UNK]\u001b[0m \u001b[0;30;48;2;231;138;195m\u001b[0m \u001b[0;30;48;2;166;216;84mᄋ\u001b[0m \u001b[0;30;48;2;255;217;47mᅵ\u001b[0m \u001b[0;30;48;2;102;194;165mᄒ\u001b[0m \u001b[0;30;48;2;252;141;98m[UNK]\u001b[0m \u001b[0;30;48;2;141;160;203mᄒ\u001b[0m \u001b[0;30;48;2;231;138;195m[UNK]\u001b[0m \u001b[0;30;48;2;166;216;84m\u001b[0m \u001b[0;30;48;2;255;217;47mᄒ\u001b[0m \u001b[0;30;48;2;102;194;165m[UNK]\u001b[0m \u001b[0;30;48;2;252;141;98m\u001b[0m \u001b[0;30;48;2;141;160;203mᄀ\u001b[0m \u001b[0;30;48;2;231;138;195mᅵ\u001b[0m \u001b[0;30;48;2;166;216;84m[UNK]\u001b[0m \u001b[0;30;48;2;255;217;47mᄋ\u001b[0m \u001b[0;30;48;2;102;194;165mᅵ\u001b[0m \u001b[0;30;48;2;252;141;98m[UNK]\u001b[0m \u001b[0;30;48;2;141;160;203m.\u001b[0m \u001b[0;30;48;2;231;138;195m,\u001b[0m \u001b[0;30;48;2;166;216;84m\u001b[0m \u001b[0;30;48;2;255;217;47m[UNK]\u001b[0m \u001b[0;30;48;2;102;194;165m[UNK]\u001b[0m "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# EleutherAI의 Polyglot-Ko 시리즈 (1.3b + 3.8b + 12.8b 테스트 = 결과 동일 )\n",
        "\n",
        "ko_text = \"\"\"\n",
        "    안녕하세요! 저는 인공지능을 공부하고 있습니다.,\n",
        "    한글은 세종대왕이 창제하신 문자입니다. 😊,\n",
        "    카카오와 네이버는 한국의 대표적인 IT 기업입니다.,\n",
        "    자연어처리(NLP)는 컴퓨터가 인간의 언어를 이해하도록 하는 기술이다.,\n",
        "\"\"\"\n",
        "\n",
        "# kobert-base보단 분류를 잘 하지만, 유니코드 처리되는 문자도 있고 안정적이진 않음\n",
        "show_tokens(ko_text, \"EleutherAI/polyglot-ko-1.3b\")"
      ],
      "metadata": {
        "id": "n_egY3Ex_tH9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00b3d7be-556e-4526-aa91-9cb95119349d"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0;30;48;2;102;194;165m\n",
            "\u001b[0m \u001b[0;30;48;2;252;141;98m \u001b[0m \u001b[0;30;48;2;141;160;203m \u001b[0m \u001b[0;30;48;2;231;138;195m \u001b[0m \u001b[0;30;48;2;166;216;84m 안녕\u001b[0m \u001b[0;30;48;2;255;217;47m하\u001b[0m \u001b[0;30;48;2;102;194;165m세요\u001b[0m \u001b[0;30;48;2;252;141;98m!\u001b[0m \u001b[0;30;48;2;141;160;203m 저\u001b[0m \u001b[0;30;48;2;231;138;195m는\u001b[0m \u001b[0;30;48;2;166;216;84m 인공지능\u001b[0m \u001b[0;30;48;2;255;217;47m을\u001b[0m \u001b[0;30;48;2;102;194;165m 공부\u001b[0m \u001b[0;30;48;2;252;141;98m하고\u001b[0m \u001b[0;30;48;2;141;160;203m 있\u001b[0m \u001b[0;30;48;2;231;138;195m습니다\u001b[0m \u001b[0;30;48;2;166;216;84m.\u001b[0m \u001b[0;30;48;2;255;217;47m,\u001b[0m \u001b[0;30;48;2;102;194;165m\n",
            "\u001b[0m \u001b[0;30;48;2;252;141;98m \u001b[0m \u001b[0;30;48;2;141;160;203m \u001b[0m \u001b[0;30;48;2;231;138;195m \u001b[0m \u001b[0;30;48;2;166;216;84m 한글\u001b[0m \u001b[0;30;48;2;255;217;47m은\u001b[0m \u001b[0;30;48;2;102;194;165m 세종\u001b[0m \u001b[0;30;48;2;252;141;98m대왕\u001b[0m \u001b[0;30;48;2;141;160;203m이\u001b[0m \u001b[0;30;48;2;231;138;195m 창\u001b[0m \u001b[0;30;48;2;166;216;84m제\u001b[0m \u001b[0;30;48;2;255;217;47m하\u001b[0m \u001b[0;30;48;2;102;194;165m신\u001b[0m \u001b[0;30;48;2;252;141;98m 문자\u001b[0m \u001b[0;30;48;2;141;160;203m입니다\u001b[0m \u001b[0;30;48;2;231;138;195m.\u001b[0m \u001b[0;30;48;2;166;216;84m \u001b[0m \u001b[0;30;48;2;255;217;47m�\u001b[0m \u001b[0;30;48;2;102;194;165m�\u001b[0m \u001b[0;30;48;2;252;141;98m�\u001b[0m \u001b[0;30;48;2;141;160;203m�\u001b[0m \u001b[0;30;48;2;231;138;195m,\u001b[0m \u001b[0;30;48;2;166;216;84m\n",
            "\u001b[0m \u001b[0;30;48;2;255;217;47m \u001b[0m \u001b[0;30;48;2;102;194;165m \u001b[0m \u001b[0;30;48;2;252;141;98m \u001b[0m \u001b[0;30;48;2;141;160;203m 카카오\u001b[0m \u001b[0;30;48;2;231;138;195m와\u001b[0m \u001b[0;30;48;2;166;216;84m 네이버\u001b[0m \u001b[0;30;48;2;255;217;47m는\u001b[0m \u001b[0;30;48;2;102;194;165m 한국\u001b[0m \u001b[0;30;48;2;252;141;98m의\u001b[0m \u001b[0;30;48;2;141;160;203m 대표\u001b[0m \u001b[0;30;48;2;231;138;195m적\u001b[0m \u001b[0;30;48;2;166;216;84m인\u001b[0m \u001b[0;30;48;2;255;217;47m IT\u001b[0m \u001b[0;30;48;2;102;194;165m 기업\u001b[0m \u001b[0;30;48;2;252;141;98m입니다\u001b[0m \u001b[0;30;48;2;141;160;203m.\u001b[0m \u001b[0;30;48;2;231;138;195m,\u001b[0m \u001b[0;30;48;2;166;216;84m\n",
            "\u001b[0m \u001b[0;30;48;2;255;217;47m \u001b[0m \u001b[0;30;48;2;102;194;165m \u001b[0m \u001b[0;30;48;2;252;141;98m \u001b[0m \u001b[0;30;48;2;141;160;203m 자연\u001b[0m \u001b[0;30;48;2;231;138;195m어\u001b[0m \u001b[0;30;48;2;166;216;84m처리\u001b[0m \u001b[0;30;48;2;255;217;47m(\u001b[0m \u001b[0;30;48;2;102;194;165mNL\u001b[0m \u001b[0;30;48;2;252;141;98mP\u001b[0m \u001b[0;30;48;2;141;160;203m)\u001b[0m \u001b[0;30;48;2;231;138;195m는\u001b[0m \u001b[0;30;48;2;166;216;84m 컴퓨터\u001b[0m \u001b[0;30;48;2;255;217;47m가\u001b[0m \u001b[0;30;48;2;102;194;165m 인간\u001b[0m \u001b[0;30;48;2;252;141;98m의\u001b[0m \u001b[0;30;48;2;141;160;203m 언어\u001b[0m \u001b[0;30;48;2;231;138;195m를\u001b[0m \u001b[0;30;48;2;166;216;84m 이해\u001b[0m \u001b[0;30;48;2;255;217;47m하\u001b[0m \u001b[0;30;48;2;102;194;165m도록\u001b[0m \u001b[0;30;48;2;252;141;98m 하\u001b[0m \u001b[0;30;48;2;141;160;203m는\u001b[0m \u001b[0;30;48;2;231;138;195m 기술\u001b[0m \u001b[0;30;48;2;166;216;84m이\u001b[0m \u001b[0;30;48;2;255;217;47m다\u001b[0m \u001b[0;30;48;2;102;194;165m.\u001b[0m \u001b[0;30;48;2;252;141;98m,\u001b[0m \u001b[0;30;48;2;141;160;203m\n",
            "\u001b[0m "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Beomi의 KcBERT - 댓글 데이터 특화\n",
        "\n",
        "ko_text = \"\"\"\n",
        "    안녕하세요! 저는 인공지능을 공부하고 있습니다.,\n",
        "    한글은 세종대왕이 창제하신 문자입니다. 😊,\n",
        "    카카오와 네이버는 한국의 대표적인 IT 기업입니다.,\n",
        "    자연어처리(NLP)는 컴퓨터가 인간의 언어를 이해하도록 하는 기술이다.,\n",
        "\"\"\"\n",
        "\n",
        "# 이모지도 구별한다! + 불용어를 ##으로 체크하는 듯 함\n",
        "show_tokens(ko_text, \"beomi/kcbert-base\")"
      ],
      "metadata": {
        "id": "ltpFu7zq_qpE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "817f3249-d1c6-4046-e352-ce088ff64ed3"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0;30;48;2;102;194;165m[CLS]\u001b[0m \u001b[0;30;48;2;252;141;98m안녕\u001b[0m \u001b[0;30;48;2;141;160;203m##하세요\u001b[0m \u001b[0;30;48;2;231;138;195m!\u001b[0m \u001b[0;30;48;2;166;216;84m저는\u001b[0m \u001b[0;30;48;2;255;217;47m인공\u001b[0m \u001b[0;30;48;2;102;194;165m##지능\u001b[0m \u001b[0;30;48;2;252;141;98m##을\u001b[0m \u001b[0;30;48;2;141;160;203m공부하고\u001b[0m \u001b[0;30;48;2;231;138;195m있습니다\u001b[0m \u001b[0;30;48;2;166;216;84m.\u001b[0m \u001b[0;30;48;2;255;217;47m,\u001b[0m \u001b[0;30;48;2;102;194;165m한글\u001b[0m \u001b[0;30;48;2;252;141;98m##은\u001b[0m \u001b[0;30;48;2;141;160;203m세종대왕\u001b[0m \u001b[0;30;48;2;231;138;195m##이\u001b[0m \u001b[0;30;48;2;166;216;84m창\u001b[0m \u001b[0;30;48;2;255;217;47m##제\u001b[0m \u001b[0;30;48;2;102;194;165m##하신\u001b[0m \u001b[0;30;48;2;252;141;98m문자\u001b[0m \u001b[0;30;48;2;141;160;203m##입니다\u001b[0m \u001b[0;30;48;2;231;138;195m.\u001b[0m \u001b[0;30;48;2;166;216;84m😊\u001b[0m \u001b[0;30;48;2;255;217;47m,\u001b[0m \u001b[0;30;48;2;102;194;165m카카오\u001b[0m \u001b[0;30;48;2;252;141;98m##와\u001b[0m \u001b[0;30;48;2;141;160;203m네이버는\u001b[0m \u001b[0;30;48;2;231;138;195m한국의\u001b[0m \u001b[0;30;48;2;166;216;84m대표적인\u001b[0m \u001b[0;30;48;2;255;217;47mIT\u001b[0m \u001b[0;30;48;2;102;194;165m기업\u001b[0m \u001b[0;30;48;2;252;141;98m##입니다\u001b[0m \u001b[0;30;48;2;141;160;203m.\u001b[0m \u001b[0;30;48;2;231;138;195m,\u001b[0m \u001b[0;30;48;2;166;216;84m자연\u001b[0m \u001b[0;30;48;2;255;217;47m##어\u001b[0m \u001b[0;30;48;2;102;194;165m##처리\u001b[0m \u001b[0;30;48;2;252;141;98m(\u001b[0m \u001b[0;30;48;2;141;160;203mN\u001b[0m \u001b[0;30;48;2;231;138;195m##L\u001b[0m \u001b[0;30;48;2;166;216;84m##P\u001b[0m \u001b[0;30;48;2;255;217;47m)\u001b[0m \u001b[0;30;48;2;102;194;165m는\u001b[0m \u001b[0;30;48;2;252;141;98m컴퓨터\u001b[0m \u001b[0;30;48;2;141;160;203m##가\u001b[0m \u001b[0;30;48;2;231;138;195m인간의\u001b[0m \u001b[0;30;48;2;166;216;84m언어\u001b[0m \u001b[0;30;48;2;255;217;47m##를\u001b[0m \u001b[0;30;48;2;102;194;165m이해\u001b[0m \u001b[0;30;48;2;252;141;98m##하도록\u001b[0m \u001b[0;30;48;2;141;160;203m하는\u001b[0m \u001b[0;30;48;2;231;138;195m기술이\u001b[0m \u001b[0;30;48;2;166;216;84m##다\u001b[0m \u001b[0;30;48;2;255;217;47m.\u001b[0m \u001b[0;30;48;2;102;194;165m,\u001b[0m \u001b[0;30;48;2;252;141;98m[SEP]\u001b[0m "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# monologg의 KoBERT (Transformers 호환)\n",
        "\n",
        "ko_text = \"\"\"\n",
        "    안녕하세요! 저는 인공지능을 공부하고 있습니다.,\n",
        "    한글은 세종대왕이 창제하신 문자입니다. 😊,\n",
        "    카카오와 네이버는 한국의 대표적인 IT 기업입니다.,\n",
        "    자연어처리(NLP)는 컴퓨터가 인간의 언어를 이해하도록 하는 기술이다.,\n",
        "\"\"\"\n",
        "\n",
        "# 단어 기준보다 문자 하나씩 토큰화하기때문에 효율이 좋은것같지는 않음.\n",
        "show_tokens(ko_text, \"monologg/kobert\")"
      ],
      "metadata": {
        "id": "tX42XpfS_tLW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d858e60-d936-41d2-b02f-b3c64728897e"
      },
      "execution_count": 26,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The repository for monologg/kobert contains custom code which must be executed to correctly load the model. You can inspect the repository content at https://hf.co/monologg/kobert.\n",
            "You can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n",
            "\n",
            "Do you wish to run the custom code? [y/N] Y\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "A new version of the following files was downloaded from https://huggingface.co/monologg/kobert:\n",
            "- tokenization_kobert.py\n",
            ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0;30;48;2;102;194;165m[CLS]\u001b[0m \u001b[0;30;48;2;252;141;98m안\u001b[0m \u001b[0;30;48;2;141;160;203m녕\u001b[0m \u001b[0;30;48;2;231;138;195m하세요\u001b[0m \u001b[0;30;48;2;166;216;84m!\u001b[0m \u001b[0;30;48;2;255;217;47m저\u001b[0m \u001b[0;30;48;2;102;194;165m는\u001b[0m \u001b[0;30;48;2;252;141;98m인\u001b[0m \u001b[0;30;48;2;141;160;203m공\u001b[0m \u001b[0;30;48;2;231;138;195m지\u001b[0m \u001b[0;30;48;2;166;216;84m능\u001b[0m \u001b[0;30;48;2;255;217;47m을\u001b[0m \u001b[0;30;48;2;102;194;165m공부\u001b[0m \u001b[0;30;48;2;252;141;98m하고\u001b[0m \u001b[0;30;48;2;141;160;203m있습니다\u001b[0m \u001b[0;30;48;2;231;138;195m.\u001b[0m \u001b[0;30;48;2;166;216;84m,\u001b[0m \u001b[0;30;48;2;255;217;47m한\u001b[0m \u001b[0;30;48;2;102;194;165m글\u001b[0m \u001b[0;30;48;2;252;141;98m은\u001b[0m \u001b[0;30;48;2;141;160;203m세종\u001b[0m \u001b[0;30;48;2;231;138;195m대\u001b[0m \u001b[0;30;48;2;166;216;84m왕\u001b[0m \u001b[0;30;48;2;255;217;47m이\u001b[0m \u001b[0;30;48;2;102;194;165m창\u001b[0m \u001b[0;30;48;2;252;141;98m제\u001b[0m \u001b[0;30;48;2;141;160;203m하\u001b[0m \u001b[0;30;48;2;231;138;195m신\u001b[0m \u001b[0;30;48;2;166;216;84m문자\u001b[0m \u001b[0;30;48;2;255;217;47m입니다\u001b[0m \u001b[0;30;48;2;102;194;165m.\u001b[0m \u001b[0;30;48;2;252;141;98m\u001b[0m \u001b[0;30;48;2;141;160;203m[UNK]\u001b[0m \u001b[0;30;48;2;231;138;195m,\u001b[0m \u001b[0;30;48;2;166;216;84m카카오\u001b[0m \u001b[0;30;48;2;255;217;47m와\u001b[0m \u001b[0;30;48;2;102;194;165m네이버\u001b[0m \u001b[0;30;48;2;252;141;98m는\u001b[0m \u001b[0;30;48;2;141;160;203m한국의\u001b[0m \u001b[0;30;48;2;231;138;195m대표적인\u001b[0m \u001b[0;30;48;2;166;216;84mIT\u001b[0m \u001b[0;30;48;2;255;217;47m기업\u001b[0m \u001b[0;30;48;2;102;194;165m입니다\u001b[0m \u001b[0;30;48;2;252;141;98m.\u001b[0m \u001b[0;30;48;2;141;160;203m,\u001b[0m \u001b[0;30;48;2;231;138;195m자연\u001b[0m \u001b[0;30;48;2;166;216;84m어\u001b[0m \u001b[0;30;48;2;255;217;47m처리\u001b[0m \u001b[0;30;48;2;102;194;165m(\u001b[0m \u001b[0;30;48;2;252;141;98mN\u001b[0m \u001b[0;30;48;2;141;160;203mL\u001b[0m \u001b[0;30;48;2;231;138;195mP\u001b[0m \u001b[0;30;48;2;166;216;84m)\u001b[0m \u001b[0;30;48;2;255;217;47m는\u001b[0m \u001b[0;30;48;2;102;194;165m컴퓨터\u001b[0m \u001b[0;30;48;2;252;141;98m가\u001b[0m \u001b[0;30;48;2;141;160;203m인간\u001b[0m \u001b[0;30;48;2;231;138;195m의\u001b[0m \u001b[0;30;48;2;166;216;84m언\u001b[0m \u001b[0;30;48;2;255;217;47m어\u001b[0m \u001b[0;30;48;2;102;194;165m를\u001b[0m \u001b[0;30;48;2;252;141;98m이해\u001b[0m \u001b[0;30;48;2;141;160;203m하도록\u001b[0m \u001b[0;30;48;2;231;138;195m하는\u001b[0m \u001b[0;30;48;2;166;216;84m기술\u001b[0m \u001b[0;30;48;2;255;217;47m이다\u001b[0m \u001b[0;30;48;2;102;194;165m.\u001b[0m \u001b[0;30;48;2;252;141;98m,\u001b[0m \u001b[0;30;48;2;141;160;203m[SEP]\u001b[0m "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "wiL4pKvd8P_j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **토큰 임베딩(Token Embedding)**"
      ],
      "metadata": {
        "id": "7Cci9koNIGiZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **언어** --> **토큰의 시퀀스**\n",
        "- **충분히 좋은 모델**을 충분히 **큰 토큰 집합에서 훈련**한다면 **훈련 데이터셋에 있는 복잡한 패턴을 포착**하기 시작한다.\n",
        "- **Embedding** --> 수치표현, **언어에 있는 의미와 패턴을 포착하기 위한 수치 표현 공간**"
      ],
      "metadata": {
        "id": "1Y0VO0ciITwN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 토크나이저가 초기화되고 훈련되고 나면 이를 사용해 언어 모델을 훈련함\n",
        "    - 사전 훈련된 언어 모델이 해당 토크나이저와 연결되는 이유\n",
        "    - 모델을 재훈련하지 않고는 다른 토크나이저를 사용할 수 없음"
      ],
      "metadata": {
        "id": "ocQmgQX4LnOv"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Tu7OY4HvBEm"
      },
      "source": [
        "## (BERT와 같은)**언어 모델로 문맥을 고려한 단어 임베딩 만들기**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nsjz-VsYu9bB"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModel, AutoTokenizer\n",
        "\n",
        "# 토크나이저를 로드합니다.\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-base\")\n",
        "\n",
        "# 언어 모델을 로드합니다.\n",
        "model = AutoModel.from_pretrained(\"microsoft/deberta-v3-xsmall\")\n",
        "\n",
        "# 문장을 토큰으로 나눕니다.\n",
        "tokens = tokenizer('Hello world', return_tensors='pt')\n",
        "\n",
        "# 토큰을 처리합니다.\n",
        "output = model(**tokens)[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lQly_KcbvDce"
      },
      "outputs": [],
      "source": [
        "output.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8GcRrpPV0kVj"
      },
      "outputs": [],
      "source": [
        "for token in tokens['input_ids'][0]:\n",
        "    print(tokenizer.decode(token))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e8oHVC7B0lkk"
      },
      "outputs": [],
      "source": [
        "output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DdEDuLWa0r4L"
      },
      "source": [
        "## **텍스트 임베딩** (문장과 전체 문서)\n",
        "\n",
        "- 텍스트 임베딩이란 하나의 벡터로 토큰보다 긴 텍스트를 표현하는 것\n",
        "- **텍스트 임베딩 모델**은 텍스트 조각을 입력받아 텍스트를 표현하고 유영한 형태로 의미를 포착하는 하나의 벡터를 만드는 모델"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TQHWioIc0pQ8"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# 모델을 로드합니다.\n",
        "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
        "\n",
        "# 텍스트를 텍스트 임베딩으로 변환합니다.\n",
        "vector = model.encode(\"Best movie ever!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PDwfmBiC0uER"
      },
      "outputs": [],
      "source": [
        "vector.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMSgyKKS4xUx"
      },
      "source": [
        "## **임베딩으로 노래 추천하기**\n",
        "\n",
        "- 데이터셋: 코넬대학교 슈오첸이 모은 데이터셋(미국 전역에 있는 수백개의 라이도 방송국에서 가져온 재생목록)\n",
        "    - 노래 재생목록 :\n",
        "        - https://storage.googleapis.com/maps-premium/dataset/yes_complete/train.txt\n",
        "    - 노래 메타데이터: (제목, 아티스트)\n",
        "        - https://storage.googleapis.com/maps-premium/dataset/yes_complete/song_hash.txt\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "id": "fLHAdPqghG3z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 데이터셋 로드하기"
      ],
      "metadata": {
        "id": "YynR6bbHi3SH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3dJdWzT67nDL"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from urllib import request\n",
        "\n",
        "# 재생목록 데이터셋 파일을 가져옵니다.\n",
        "data = request.urlopen('https://storage.googleapis.com/maps-premium/dataset/yes_complete/train.txt')\n",
        "\n",
        "# 재생목록 파일을 파싱합니다. 처음 두 줄은 메타데이터만 담고 있으므로 건너뜁니다.\n",
        "lines = data.read().decode(\"utf-8\").split('\\n')[2:]\n",
        "print(len(lines))\n",
        "\n",
        "# 하나의 노래만 있는 재생목록은 삭제합니다.\n",
        "playlists = [s.rstrip().split() for s in lines if len(s.split()) > 1]\n",
        "\n",
        "# 노래의 메타데이터를 로드합니다.\n",
        "songs_file = request.urlopen('https://storage.googleapis.com/maps-premium/dataset/yes_complete/song_hash.txt')\n",
        "songs_file = songs_file.read().decode(\"utf-8\").split('\\n')\n",
        "songs = [s.rstrip().split('\\t') for s in songs_file]\n",
        "songs_df = pd.DataFrame(data=songs, columns = ['id', 'title', 'artist'])\n",
        "songs_df = songs_df.set_index('id')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q3zirG-lo3H8"
      },
      "outputs": [],
      "source": [
        "print('재생목록 #1:\\n ', playlists[0], '\\n')\n",
        "print('재생목록 #2:\\n ', playlists[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Word2Vec 훈련하기\n",
        "    - 결과물: 각 노래에 대해 계산된 임베딩 결과 --> 이 임베딩으로 비슷한 노래를 찾을 수 있다.)"
      ],
      "metadata": {
        "id": "V0iB1PqNjASr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EaUz3E0P7sJs"
      },
      "outputs": [],
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Word2Vec 모델을 훈련합니다.\n",
        "model = Word2Vec(\n",
        "    playlists, vector_size=32, window=20, negative=50, min_count=1, workers=4\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9EFGWesO8rOJ"
      },
      "outputs": [],
      "source": [
        "song_id = 2172\n",
        "\n",
        "# 노래 ID 2172와 비슷한 노래를 찾으라고 모델에게 요청합니다.\n",
        "model.wv.most_similar(positive=str(song_id))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AMiY6isXqKk4"
      },
      "outputs": [],
      "source": [
        "print(songs_df.iloc[2172])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aOzWENxr2Fl3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def print_recommendations(song_id):\n",
        "    similar_songs = np.array(\n",
        "        model.wv.most_similar(positive=str(song_id),topn=5)\n",
        "    )[:,0]\n",
        "    return  songs_df.iloc[similar_songs]\n",
        "\n",
        "# 추천 노래 출력\n",
        "print_recommendations(2172)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TIHiN62g1NMi"
      },
      "outputs": [],
      "source": [
        "print_recommendations(842)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}