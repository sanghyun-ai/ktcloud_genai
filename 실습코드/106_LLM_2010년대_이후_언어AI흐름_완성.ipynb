{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanghyun-ai/ktcloud_genai/blob/main/%EC%8B%A4%EC%8A%B5%EC%BD%94%EB%93%9C/106_LLM_2010%EB%85%84%EB%8C%80_%EC%9D%B4%ED%9B%84_%EC%96%B8%EC%96%B4AI%ED%9D%90%EB%A6%84_%EC%99%84%EC%84%B1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2010ë…„ëŒ€ ì´í›„ ì–¸ì–´ AIíë¦„**"
      ],
      "metadata": {
        "id": "xAPO58qbDYi8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "NAmo5uhXQ46c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ğŸ’¡**ì½”ë“œ ë‚´ìš©**\n",
        "    - ì–¸ì–´ AIì˜ 2010ë…„ ì´í›„ì˜ ëŒ€ëµì ì¸ íë¦„"
      ],
      "metadata": {
        "id": "KHIcfXn8RArA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "K-OX9xcYMp_f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **QuickTour**"
      ],
      "metadata": {
        "id": "CTlhVOfCa3jo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **ì˜ˆì œ 1: ë”¥ëŸ¬ë‹ ì´ì „ì˜ ë°©ì‹ - ë‹¨ì–´ ë¹ˆë„ë¡œ ê°ì„± ë¶„ì„í•˜ê¸° (TF-IDF)**\n",
        "\n",
        "- 2010ë…„ëŒ€ ì´ì „, ë‹¨ì–´ì˜ ì¶œí˜„ ë¹ˆë„ë¥¼ ì¤‘ìš”í•œ íŠ¹ì§•(Feature)ìœ¼ë¡œ ì‚¬ìš©í–ˆë˜ ê³ ì „ì ì¸ ë°©ì‹\n",
        "- scikit-learn ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©\n",
        "- **TfidfVectorizer**ë¥¼ ì‚¬ìš©í•´ ì‚¬ëŒì´ ì§ì ‘ **'ë‹¨ì–´ì˜ ë¹ˆë„ìˆ˜'ë¼ëŠ” íŠ¹ì§•ì„ ì¶”ì¶œ** -->  'íŠ¹ì§• ê³µí•™'ì˜ ì˜ˆì‹œ\n",
        "- ë”¥ëŸ¬ë‹ ëª¨ë¸ì²˜ëŸ¼ ë‹¨ì–´ì˜ ì˜ë¯¸ë‚˜ ë¬¸ë§¥ì„ ìŠ¤ìŠ¤ë¡œ í•™ìŠµí•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ **í†µê³„ì— ê¸°ë°˜**í•œë‹¤"
      ],
      "metadata": {
        "id": "1TCZmmIJs4D-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜\n",
        "# pip install scikit-learn\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "print(\"----------- ì˜ˆì œ 1: ë”¥ëŸ¬ë‹ ì´ì „ì˜ ê³ ì „ì ì¸ ê°ì„± ë¶„ì„ -----------\")\n",
        "\n",
        "# í›ˆë ¨ ë°ì´í„°: ì˜í™” ë¦¬ë·°ì™€ ê¸ì •(1)/ë¶€ì •(0) ë ˆì´ë¸”\n",
        "train_text = [\n",
        "    \"ì´ ì˜í™” ì •ë§ ì¬ë¯¸ìˆì–´ìš”. ë°°ìš°ë“¤ ì—°ê¸°ë„ ìµœê³ !\",\n",
        "    \"ì‹œê°„ ê°€ëŠ” ì¤„ ëª¨ë¥´ê³  ë´¤ë„¤ìš”. ê°•ë ¥ ì¶”ì²œí•©ë‹ˆë‹¤.\",\n",
        "    \"ê¸°ëŒ€í–ˆëŠ”ë° ë„ˆë¬´ ì‹¤ë§í–ˆì–´ìš”. ìŠ¤í† ë¦¬ê°€ ì§€ë£¨í•´ìš”.\",\n",
        "    \"ëˆì´ ì•„ê¹Œìš´ ì˜í™”. ë‹¤ì‹œëŠ” ì•ˆ ë³¼ë˜ìš”.\"\n",
        "]\n",
        "train_labels = [1, 1, 0, 0] # 1: ê¸ì •, 0: ë¶€ì •\n",
        "\n",
        "# 1. íŠ¹ì§• ê³µí•™(Feature Engineering): TF-IDFë¡œ ë¬¸ì¥ì„ ìˆ«ì ë²¡í„°ë¡œ ë³€í™˜\n",
        "# TF-IDF: ë‹¨ì–´ì˜ ì¤‘ìš”ë„ë¥¼ ê³„ì‚°í•˜ëŠ” í†µê³„ì  ë°©ë²•\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "X_train = tfidf_vectorizer.fit_transform(train_text)\n",
        "\n",
        "# ë‹¨ì–´ ì‚¬ì „ê³¼ ë³€í™˜ëœ ë²¡í„° í™•ì¸\n",
        "print(\"ë‹¨ì–´ ì‚¬ì „:\", tfidf_vectorizer.get_feature_names_out())\n",
        "print(\"TF-IDF ë²¡í„° (ì²« ë²ˆì§¸ ë¬¸ì¥):\", X_train[0].toarray())\n",
        "\n",
        "\n",
        "# 2. ëª¨ë¸ í•™ìŠµ: ë¡œì§€ìŠ¤í‹± íšŒê·€ ëª¨ë¸ë¡œ í•™ìŠµ\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, train_labels)\n",
        "\n",
        "# 3. ìƒˆë¡œìš´ ë°ì´í„°ë¡œ ì˜ˆì¸¡\n",
        "test_text = [\"ë°°ìš°ë“¤ ì—°ê¸°ê°€ ì•„ì‰¬ì› ì§€ë§Œ ìŠ¤í† ë¦¬ëŠ” í¥ë¯¸ë¡œì› ì–´ìš”.\"]\n",
        "X_test = tfidf_vectorizer.transform(test_text)\n",
        "prediction = model.predict(X_test)\n",
        "\n",
        "print(f\"\\ní…ŒìŠ¤íŠ¸ ë¬¸ì¥: '{test_text[0]}'\")\n",
        "print(\"ì˜ˆì¸¡ ê²°ê³¼:\", \"ê¸ì • ğŸ˜€\" if prediction[0] == 1 else \"ë¶€ì • ğŸ˜\")"
      ],
      "metadata": {
        "id": "6S4m0IoMIgDJ",
        "outputId": "6467c71a-38d7-495f-d55c-ab6862bf8154",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------- ì˜ˆì œ 1: ë”¥ëŸ¬ë‹ ì´ì „ì˜ ê³ ì „ì ì¸ ê°ì„± ë¶„ì„ -----------\n",
            "ë‹¨ì–´ ì‚¬ì „: ['ê°€ëŠ”' 'ê°•ë ¥' 'ê¸°ëŒ€í–ˆëŠ”ë°' 'ë„ˆë¬´' 'ë‹¤ì‹œëŠ”' 'ëˆì´' 'ëª¨ë¥´ê³ ' 'ë°°ìš°ë“¤' 'ë³¼ë˜ìš”' 'ë´¤ë„¤ìš”' 'ìŠ¤í† ë¦¬ê°€' 'ì‹œê°„'\n",
            " 'ì‹¤ë§í–ˆì–´ìš”' 'ì•„ê¹Œìš´' 'ì—°ê¸°ë„' 'ì˜í™”' 'ì¬ë¯¸ìˆì–´ìš”' 'ì •ë§' 'ì§€ë£¨í•´ìš”' 'ìµœê³ ' 'ì¶”ì²œí•©ë‹ˆë‹¤']\n",
            "TF-IDF ë²¡í„° (ì²« ë²ˆì§¸ ë¬¸ì¥): [[0.         0.         0.         0.         0.         0.\n",
            "  0.         0.42176478 0.         0.         0.         0.\n",
            "  0.         0.         0.42176478 0.3325242  0.42176478 0.42176478\n",
            "  0.         0.42176478 0.        ]]\n",
            "\n",
            "í…ŒìŠ¤íŠ¸ ë¬¸ì¥: 'ë°°ìš°ë“¤ ì—°ê¸°ê°€ ì•„ì‰¬ì› ì§€ë§Œ ìŠ¤í† ë¦¬ëŠ” í¥ë¯¸ë¡œì› ì–´ìš”.'\n",
            "ì˜ˆì¸¡ ê²°ê³¼: ê¸ì • ğŸ˜€\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **ì˜ˆì œ 2: ë‹¨ì–´ì˜ ì˜ë¯¸ë¥¼ ë²¡í„°ë¡œ! - ë¶„ì‚° í‘œí˜„ (Word2Vec)**\n",
        "\n",
        "- **gensim** : ë¬¸ì„œ ìœ ì‚¬ë„ ë¶„ì„ ë“± ì—¬ëŸ¬ NLP ì•Œê³ ë¦¬ì¦˜ì„ êµ¬í˜„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬"
      ],
      "metadata": {
        "id": "vpepkUF0JGEz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜\n",
        "!pip install gensim"
      ],
      "metadata": {
        "id": "YZPkLgNvJcXc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56a3d7dc-ac3e-4008-c0a6-79dae4c47642"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.3.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Collecting numpy<2.0,>=1.18.5 (from gensim)\n",
            "  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
            "  Downloading scipy-1.13.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.3.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open>=1.8.1->gensim) (1.17.3)\n",
            "Downloading gensim-4.3.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m26.6/26.6 MB\u001b[0m \u001b[31m41.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m69.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.13.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.2 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m38.2/38.2 MB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, scipy, gensim\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.16.2\n",
            "    Uninstalling scipy-1.16.2:\n",
            "      Successfully uninstalled scipy-1.16.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "print(\"\\n\\n----------- ì˜ˆì œ 2: Word2Vecìœ¼ë¡œ ë‹¨ì–´ì˜ ë¶„ì‚° í‘œí˜„ í•™ìŠµí•˜ê¸° -----------\")\n",
        "\n",
        "# Word2Vec í•™ìŠµì„ ìœ„í•œ ìƒ˜í”Œ ë¬¸ì¥ (í† í¬ë‚˜ì´ì§• ëœ í˜•íƒœ)\n",
        "sentences = [\n",
        "    ['ì™•', 'ë‚¨ì', 'ê°•í•˜ë‹¤'],\n",
        "    ['ì—¬ì™•', 'ì—¬ì', 'ì•„ë¦„ë‹µë‹¤'],\n",
        "    ['ë‚¨ì', 'í˜', 'ì¼'],\n",
        "    ['ì—¬ì', 'ì§€í˜œ', 'ì‚¬ë‘'],\n",
        "    ['ì™•', 'ê¶Œë ¥', 'ì§€ë°°'],\n",
        "    ['ì—¬ì™•', 'ê¶Œë ¥', 'ìš°ì•„í•¨']\n",
        "]\n",
        "\n",
        "# 1. Word2Vec ëª¨ë¸ í•™ìŠµ (ë¶„ì‚° í‘œí˜„ í•™ìŠµ)\n",
        "# vector_size: ë‹¨ì–´ë¥¼ í‘œí˜„í•  ë²¡í„°ì˜ ì°¨ì›\n",
        "# window: ì£¼ë³€ ë‹¨ì–´ë¥¼ ëª‡ ê°œê¹Œì§€ ë³¼ ê²ƒì¸ì§€\n",
        "# min_count: ìµœì†Œ ë“±ì¥ íšŸìˆ˜\n",
        "# sg=1: Skip-Gram ë°©ì‹ ì‚¬ìš© (ì£¼ë³€ ë‹¨ì–´ ì˜ˆì¸¡)\n",
        "model = Word2Vec(sentences, vector_size=100, window=2, min_count=1, sg=1)\n",
        "print(\"\\nâœ… Word2Vec ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!\")\n",
        "\n",
        "\n",
        "# 2. í•™ìŠµëœ ë‹¨ì–´ ë²¡í„° í™•ì¸\n",
        "king_vector = model.wv['ì™•']\n",
        "print(f\"\\nâœ… 'ì™•'ì˜ ë²¡í„° (ì¼ë¶€): {king_vector[:5]}...\")\n",
        "\n",
        "\n",
        "# 3. ë‹¨ì–´ ê°„ì˜ ìœ ì‚¬ë„ ê³„ì‚°\n",
        "# 'ì™•'ê³¼ ê°€ì¥ ìœ ì‚¬í•œ ë‹¨ì–´ëŠ” ë¬´ì—‡ì¼ê¹Œìš”?\n",
        "similar_words = model.wv.most_similar('ì™•')\n",
        "print(\"\\nâœ… 'ì™•'ê³¼ ê°€ì¥ ìœ ì‚¬í•œ ë‹¨ì–´:\", similar_words)\n",
        "\n",
        "\n",
        "# 4. ì¬ë¯¸ìˆëŠ” ë‹¨ì–´ ì‚°ìˆ  ì—°ì‚°!\n",
        "# 'ì™•' - 'ë‚¨ì' + 'ì—¬ì' = ?\n",
        "try:\n",
        "    result = model.wv.most_similar(positive=['ì—¬ì™•', 'ë‚¨ì'], negative=['ì—¬ì'])\n",
        "    print(\"\\nâœ… 'ì—¬ì™•' + 'ë‚¨ì' - 'ì—¬ì' â‰ˆ\", result)\n",
        "\n",
        "    result = model.wv.most_similar(positive=['ì™•', 'ì—¬ì'], negative=['ë‚¨ì'])\n",
        "    print(\"\\nâœ… 'ì™•' - 'ë‚¨ì' + 'ì—¬ì' â‰ˆ\", result)\n",
        "except KeyError as e:\n",
        "    print(f\"\\nâœ… ì•„ì‰½ê²Œë„ '{e.args[0]}' ë‹¨ì–´ê°€ ì‚¬ì „ì— ì—†ì–´ ê³„ì‚°í•  ìˆ˜ ì—†ì–´ìš”. ë” ë§ì€ ë°ì´í„°ë¡œ í•™ìŠµí•˜ë©´ ê°€ëŠ¥í•´ì§‘ë‹ˆë‹¤!\")\n"
      ],
      "metadata": {
        "id": "geQkVMOBIgIq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜\n",
        "# pip install gensim\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "print(\"----------- 'ì—¬ì™•'ì´ ë°˜ë“œì‹œ ì •ë‹µìœ¼ë¡œ ë‚˜ì˜¤ë„ë¡ ê°•ì œ í•™ìŠµì‹œí‚¨ ëª¨ë¸ -----------\")\n",
        "\n",
        "# 1. ë°ì´í„° ì¬êµ¬ì„±: ê´€ê³„ ê°•ì œ í•™ìŠµì„ ìœ„í•œ ì´ˆê³ ë°˜ë³µ/ë‹¨ìˆœí™” ë°ì´í„°\n",
        "# 'ë‚¨ì'ì™€ 'ì—¬ì'ì˜ ë²¡í„° ì°¨ì´ê°€ 'ì„±ë³„'ì„ì„ ëª…í™•íˆ ì•”ê¸°ì‹œí‚¤ê¸° ìœ„í•´\n",
        "# ë™ì¼í•œ êµ¬ì¡°ì˜ ë¬¸ì¥ì„ ì—¬ëŸ¬ ë‹¨ì–´ ìŒì— ê±¸ì³ ê·¹ë‹¨ì ìœ¼ë¡œ ë°˜ë³µí•©ë‹ˆë‹¤.\n",
        "\n",
        "force_learning_sentences = [\n",
        "    # --- í•µì‹¬ í‰í–‰ êµ¬ì¡° (ì™•/ì—¬ì™•) ë°˜ë³µ ---\n",
        "    ['ì™•', 'ë‚¨ì', 'êµ°ì£¼'],\n",
        "    ['ì—¬ì™•', 'ì—¬ì', 'êµ°ì£¼'],\n",
        "    ['ì™•', 'ë‚¨ì', 'ì™•ì¡±'],\n",
        "    ['ì—¬ì™•', 'ì—¬ì', 'ì™•ì¡±'],\n",
        "    ['ì™•', 'ë‚¨ì', 'ë¦¬ë”'],\n",
        "    ['ì—¬ì™•', 'ì—¬ì', 'ë¦¬ë”'],\n",
        "\n",
        "    # --- ë‹¤ë¥¸ ë‹¨ì–´ ìŒìœ¼ë¡œ ë™ì¼í•œ êµ¬ì¡° ë°˜ë³µ (ì„±ë³„ ê´€ê³„ ì¼ë°˜í™”) ---\n",
        "    ['ì™•ì', 'ë‚¨ì', 'í›„ê³„ì'],\n",
        "    ['ê³µì£¼', 'ì—¬ì', 'í›„ê³„ì'],\n",
        "    ['ì•„ë²„ì§€', 'ë‚¨ì', 'ë¶€ëª¨'],\n",
        "    ['ì–´ë¨¸ë‹ˆ', 'ì—¬ì', 'ë¶€ëª¨'],\n",
        "    ['ì•„ë“¤', 'ë‚¨ì', 'ìì‹'],\n",
        "    ['ë”¸', 'ì—¬ì', 'ìì‹'],\n",
        "    ['ë‚¨í¸', 'ë‚¨ì', 'ë°°ìš°ì'],\n",
        "    ['ì•„ë‚´', 'ì—¬ì', 'ë°°ìš°ì'],\n",
        "    ['ì‚¼ì´Œ', 'ë‚¨ì', 'ì¹œì²™'],\n",
        "    ['ì´ëª¨', 'ì—¬ì', 'ì¹œì²™'],\n",
        "    ['ë‚¨ë°°ìš°', 'ë‚¨ì', 'ë°°ìš°'],\n",
        "    ['ì—¬ë°°ìš°', 'ì—¬ì', 'ë°°ìš°'],\n",
        "\n",
        "    # --- ì¶”ê°€ì ì¸ ë¬¸ë§¥ì„ ìµœì†Œí™”í•˜ì—¬ í˜¼ë€ ë°©ì§€ ---\n",
        "    ['ë‚¨ì', 'ê°•ì¸í•¨'],\n",
        "    ['ì—¬ì', 'ì„¬ì„¸í•¨'],\n",
        "    ['ì™•', 'ê¶Œìœ„'],\n",
        "    ['ì—¬ì™•', 'í’ˆìœ„']\n",
        "]\n",
        "\n",
        "\n",
        "# 2. ëª¨ë¸ í•™ìŠµ (epochsë¥¼ ëŠ˜ë ¤ ì‘ì€ ë°ì´í„°ë¥¼ ë°˜ë³µ í•™ìŠµ)\n",
        "model = Word2Vec(\n",
        "    sentences=force_learning_sentences,\n",
        "    vector_size=50,     # ì‘ì€ ë°ì´í„°ì…‹ì´ë¯€ë¡œ ë²¡í„° ì°¨ì›ì„ ì¤„ì—¬ ì§‘ì¤‘ í•™ìŠµ\n",
        "    window=2,           # ì£¼ë³€ ë‹¨ì–´ ë²”ìœ„\n",
        "    min_count=1,        # ìµœì†Œ ë‹¨ì–´ ë¹ˆë„\n",
        "    sg=1,               # Skip-Gram ëª¨ë¸ ì‚¬ìš©\n",
        "    epochs=1000,         # !! ì‘ì€ ë°ì´í„°ë¥¼ 500ë²ˆ ë°˜ë³µ í•™ìŠµí•˜ì—¬ ê´€ê³„ë¥¼ ê°ì¸ì‹œí‚´\n",
        "    seed=42             # ê²°ê³¼ ì¬í˜„ì„ ìœ„í•œ ì‹œë“œ ê°’ ê³ ì •\n",
        ")\n",
        "print(\"Word2Vec ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!\")\n",
        "\n",
        "\n",
        "# 3. ê²°ê³¼ í™•ì¸: 'ì™• - ë‚¨ì + ì—¬ì' ì—°ì‚°\n",
        "try:\n",
        "    # positive: ë”í•  ë²¡í„°ë“¤, negative: ëº„ ë²¡í„°\n",
        "    result = model.wv.most_similar(positive=['ì™•', 'ì—¬ì'], negative=['ë‚¨ì'], topn=3)\n",
        "    print(f\"\\n'ì™•' - 'ë‚¨ì' + 'ì—¬ì' â‰ˆ {result}\")\n",
        "\n",
        "except KeyError as e:\n",
        "    print(f\"\\nê³„ì‚°ì— í•„ìš”í•œ ë‹¨ì–´ '{e.args[0]}'ê°€ í•™ìŠµë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "SDkRMI_9MODY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **ì˜ˆì œ 3: End-to-End ëª¨ë¸ ìˆ˜í–‰í•˜ê¸°**\n",
        "\n",
        "- ì›ë³¸ í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•˜ë©´ ë°”ë¡œ ê²°ê³¼ê°€ ë‚˜ì˜¤ëŠ” End-to-End ëª¨ë¸\n",
        "- Hugging Faceì˜ transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©"
      ],
      "metadata": {
        "id": "swxXfWZNOpm1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜\n",
        "# pip install transformers\n",
        "# pip install torch # ë˜ëŠ” tensorflow\n",
        "\n",
        "from transformers import pipeline\n",
        "\n",
        "print(\"\\n\\n----------- ì˜ˆì œ 3: End-to-End ëª¨ë¸ë¡œ ì¦‰ì‹œ ê°ì„± ë¶„ì„í•˜ê¸° -----------\")\n",
        "\n",
        "# 1. ë¯¸ë¦¬ í•™ìŠµëœ End-to-End ëª¨ë¸ ë¡œë“œ\n",
        "# Hugging Face ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ” ìˆ˜ë§ì€ ìµœì‹  AI ëª¨ë¸ì„ ëª‡ ì¤„ì˜ ì½”ë“œë¡œ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ìˆê²Œ í•´ì¤ë‹ˆë‹¤.\n",
        "# ëª¨ë¸ì´ ì•Œì•„ì„œ í† í¬ë‚˜ì´ì§•, ì„ë² ë”©, ì¶”ë¡ ê¹Œì§€ ëª¨ë“  ê³¼ì •ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.\n",
        "sentiment_analyzer = pipeline(\n",
        "    'sentiment-analysis',\n",
        "    model='matthewburke/korean_sentiment' # í•œêµ­ì–´ ê°ì„± ë¶„ì„ì— íŠ¹í™”ëœ ëª¨ë¸\n",
        ")\n",
        "\n",
        "# 2. ë¬¸ì¥ì„ ì…ë ¥í•˜ê³  ë°”ë¡œ ê²°ê³¼ í™•ì¸\n",
        "text1 = \"ì´ ê°•ì˜ëŠ” ì •ë§ ìœ ìµí•˜ê³  ë¯¸ë˜ì— í° ë„ì›€ì´ ë  ê²ƒ ê°™ì•„ìš”!\"\n",
        "text2 = \"ë‚´ìš©ì´ ë„ˆë¬´ ì–´ë ¤ì›Œì„œ í•˜ë‚˜ë„ ì´í•´ê°€ ì•ˆë¼ìš”...\"\n",
        "\n",
        "results = sentiment_analyzer([text1, text2])\n",
        "\n",
        "# 3. ê²°ê³¼ ì¶œë ¥\n",
        "for text, result in zip([text1, text2], results):\n",
        "    label = \"ê¸ì • ğŸ˜€\" if result['label'] == 'positive' else \"ë¶€ì • ğŸ˜\"\n",
        "    score = result['score'] * 100\n",
        "    print(f\"\\në¬¸ì¥: '{text}'\")\n",
        "    print(f\"ê²°ê³¼: {label} ({score:.2f}% í™•ë¥ )\")"
      ],
      "metadata": {
        "id": "ogYxl9SZIgSK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "Ej3JWUxwIguu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "0g36MizoMox8"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNkuxQaDMBt5"
      },
      "source": [
        "# **NLP ë”¥ëŸ¬ë‹ ëª¨ë¸**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vCdKDfUM9Lw"
      },
      "source": [
        "---\n",
        "- ğŸ’¡ **NOTE**\n",
        "    - NLP(ìì—°ì–´ì²˜ë¦¬) ë°œì „ê³¼ì •ì— ê¸°ì—¬í•œ ë”¥ëŸ¬ë‹ ëª¨ë¸ ì†Œê°œ\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPcm74RoEnQ2"
      },
      "source": [
        "## **RNN(Recurrent Neural Network)**\n",
        "- **ë©”ëª¨ë¦¬ë¥¼ ê°€ì§„ ì‹ ê²½ë§**\n",
        "-  ë§ˆì¹˜ ì‚¬ëŒì´ ì±…ì„ ì½ì„ ë•Œ ì• ë¬¸ì¥ì˜ ë‚´ìš©ì„ ê¸°ì–µí•˜ë©´ì„œ ë‹¤ìŒ ë¬¸ì¥ì„ ì´í•´í•˜ëŠ” ê²ƒê³¼ ìœ ì‚¬\n",
        "- ì—­ì‚¬ì  ë°°ê²½\n",
        "    - 1986ë…„: David Rumelhartê°€ ìˆœí™˜ ì‹ ê²½ë§ ê°œë… ì œì•ˆ\n",
        "    - ë°°ê²½: ê¸°ì¡´ í”¼ë“œí¬ì›Œë“œ ì‹ ê²½ë§ì€ ìˆœì„œê°€ ìˆëŠ” ë°ì´í„°(ì‹œê³„ì—´, ìì—°ì–´)ë¥¼ ì²˜ë¦¬í•˜ê¸° ì–´ë ¤ì› ìŒ\n",
        "    - í˜ì‹ ì : ì‹œê°„ì  ì˜ì¡´ì„±ì„ ëª¨ë¸ë§í•  ìˆ˜ ìˆê²Œ ë¨\n",
        "- ì•„í‚¤í…ì³ (ì°¸ê³ : https://wikidocs.net/22886)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![RNN](https://wikidocs.net/images/page/22886/rnn_image2_ver3.PNG \"RNN\")"
      ],
      "metadata": {
        "id": "zl5uXcJ9QQ8-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ì˜ˆì œ : ìŠ¤í¨ë©”ì¼ ë¶„ë¥˜ê¸°\n",
        "\n",
        "- **1.í•™ìŠµìš© ë°ì´í„° ì „ì²˜ë¦¬**"
      ],
      "metadata": {
        "id": "3ovGt97LQSez"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import urllib.request\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# ìŠ¤í¨ë©”ì¼ ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ\n",
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/ukairia777/tensorflow-nlp-tutorial/main/10.%20RNN%20Text%20Classification/dataset/spam.csv\", filename=\"spam.csv\")\n",
        "data = pd.read_csv('spam.csv', encoding='latin1')\n",
        "print('ì´ ìƒ˜í”Œì˜ ìˆ˜ :',len(data))\n",
        "\n",
        "# ë¶ˆí•„ìš”í•œ ì»¬ëŸ¼ ì‚­ì œ\n",
        "del data['Unnamed: 2']\n",
        "del data['Unnamed: 3']\n",
        "del data['Unnamed: 4']\n",
        "data['v1'] = data['v1'].replace(['ham','spam'],[0,1])\n",
        "\n",
        "\n",
        "# ë°ì´í„° í™•ì¸\n",
        "print('ê²°ì¸¡ê°’ ì—¬ë¶€ :',data.isnull().values.any())\n",
        "print('v2ì—´ì˜ ìœ ë‹ˆí¬í•œ ê°’ :',data['v2'].nunique())\n",
        "\n",
        "# v2 ì—´ì—ì„œ ì¤‘ë³µì¸ ë‚´ìš©ì´ ìˆë‹¤ë©´ ì¤‘ë³µ ì œê±°\n",
        "data.drop_duplicates(subset=['v2'], inplace=True)\n",
        "print('ì´ ìƒ˜í”Œì˜ ìˆ˜ :',len(data))\n",
        "print('ì •ìƒ ë©”ì¼ê³¼ ìŠ¤íŒ¸ ë©”ì¼ì˜ ê°œìˆ˜')\n",
        "print(data.groupby('v1').size().reset_index(name='count'))\n",
        "print('-' * 30)\n",
        "print(f'ì •ìƒ ë©”ì¼ì˜ ë¹„ìœ¨ = {round(data[\"v1\"].value_counts()[0]/len(data) * 100,3)}%')\n",
        "print(f'ìŠ¤íŒ¸ ë©”ì¼ì˜ ë¹„ìœ¨ = {round(data[\"v1\"].value_counts()[1]/len(data) * 100,3)}%')\n",
        "\n",
        "\n",
        "# í›ˆë ¨ ë°ì´í„° & í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶„ë¦¬\n",
        "X_data = data['v2']\n",
        "y_data = data['v1']\n",
        "print('ë©”ì¼ ë³¸ë¬¸ì˜ ê°œìˆ˜: {}'.format(len(X_data)))\n",
        "print('ë ˆì´ë¸”ì˜ ê°œìˆ˜: {}'.format(len(y_data)))\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data,\n",
        "                                                    test_size=0.2, random_state=0, stratify=y_data)\n",
        "\n",
        "print('--------í›ˆë ¨ ë°ì´í„°ì˜ ë¹„ìœ¨-----------')\n",
        "print(f'ì •ìƒ ë©”ì¼ = {round(y_train.value_counts()[0]/len(y_train) * 100,3)}%')\n",
        "print(f'ìŠ¤íŒ¸ ë©”ì¼ = {round(y_train.value_counts()[1]/len(y_train) * 100,3)}%')\n",
        "\n",
        "print('--------í…ŒìŠ¤íŠ¸ ë°ì´í„°ì˜ ë¹„ìœ¨-----------')\n",
        "print(f'ì •ìƒ ë©”ì¼ = {round(y_test.value_counts()[0]/len(y_test) * 100,3)}%')\n",
        "print(f'ìŠ¤íŒ¸ ë©”ì¼ = {round(y_test.value_counts()[1]/len(y_test) * 100,3)}%')"
      ],
      "metadata": {
        "id": "u90YliAPQRMp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ë¬¸ì¥ í† í°í™”\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "X_train_encoded = tokenizer.texts_to_sequences(X_train)\n",
        "print('ë©”ì¼ì˜ í† í°í™” ê²°ê³¼(5ê°œ):\\n', X_train_encoded[:5])\n",
        "print()\n",
        "word_to_index = tokenizer.word_index\n",
        "print('í† í°ì— ë¶€ì—¬ëœ ì •ìˆ˜:\\n', word_to_index)\n",
        "print()\n",
        "vocab_size = len(word_to_index) + 1\n",
        "print('ë‹¨ì–´ ì§‘í•©ì˜ í¬ê¸°: {}'.format((vocab_size)))\n",
        "print()\n",
        "print('ê° ë‹¨ì–´ì— ëŒ€í•œ ë“±ì¥ ë¹ˆë„ìˆ˜: ', tokenizer.word_counts.items())\n",
        "\n",
        "\n",
        "# í† í°í™”ëœ ë‹¨ì–´ ë¶„ì„\n",
        "threshold = 2\n",
        "total_cnt = len(word_to_index) # ë‹¨ì–´ì˜ ìˆ˜\n",
        "rare_cnt = 0 # ë“±ì¥ ë¹ˆë„ìˆ˜ê°€ thresholdë³´ë‹¤ ì‘ì€ ë‹¨ì–´ì˜ ê°œìˆ˜ë¥¼ ì¹´ìš´íŠ¸\n",
        "total_freq = 0 # í›ˆë ¨ ë°ì´í„°ì˜ ì „ì²´ ë‹¨ì–´ ë¹ˆë„ìˆ˜ ì´ í•©\n",
        "rare_freq = 0 # ë“±ì¥ ë¹ˆë„ìˆ˜ê°€ thresholdë³´ë‹¤ ì‘ì€ ë‹¨ì–´ì˜ ë“±ì¥ ë¹ˆë„ìˆ˜ì˜ ì´ í•©\n",
        "\n",
        "# ë‹¨ì–´ì™€ ë¹ˆë„ìˆ˜ì˜ ìŒ(pair)ì„ keyì™€ valueë¡œ ë°›ëŠ”ë‹¤.\n",
        "for key, value in tokenizer.word_counts.items():\n",
        "    total_freq = total_freq + value\n",
        "\n",
        "    # ë‹¨ì–´ì˜ ë“±ì¥ ë¹ˆë„ìˆ˜ê°€ thresholdë³´ë‹¤ ì‘ìœ¼ë©´\n",
        "    if(value < threshold):\n",
        "        rare_cnt = rare_cnt + 1\n",
        "        rare_freq = rare_freq + value\n",
        "\n",
        "print('ë“±ì¥ ë¹ˆë„ê°€ %së²ˆ ì´í•˜ì¸ í¬ê·€ ë‹¨ì–´ì˜ ìˆ˜: %s'%(threshold - 1, rare_cnt))\n",
        "print(\"ë‹¨ì–´ ì§‘í•©(vocabulary)ì—ì„œ í¬ê·€ ë‹¨ì–´ì˜ ë¹„ìœ¨:\", (rare_cnt / total_cnt)*100)\n",
        "print(\"ì „ì²´ ë“±ì¥ ë¹ˆë„ì—ì„œ í¬ê·€ ë‹¨ì–´ ë“±ì¥ ë¹ˆë„ ë¹„ìœ¨:\", (rare_freq / total_freq)*100)\n",
        "print('ë©”ì¼ì˜ ìµœëŒ€ ê¸¸ì´ : %d' % max(len(sample) for sample in X_train_encoded))\n",
        "print('ë©”ì¼ì˜ í‰ê·  ê¸¸ì´ : %f' % (sum(map(len, X_train_encoded))/len(X_train_encoded)))\n",
        "plt.hist([len(sample) for sample in X_data], bins=50)\n",
        "plt.xlabel('length of samples')\n",
        "plt.ylabel('number of samples')\n",
        "plt.show()\n",
        "# ê°€ì¥ ê¸´ ë©”ì¼ì˜ ê¸¸ì´ëŠ” 189ì´ë©°, ì „ì²´ ë°ì´í„°ì˜ ê¸¸ì´ ë¶„í¬ëŠ” ëŒ€ì²´ì ìœ¼ë¡œ ì•½ 50ì´í•˜ì˜ ê¸¸ì´ë¥¼ ê°€ì§‘ë‹ˆë‹¤\n",
        "\n",
        "\n",
        "max_len = 189\n",
        "X_train_padded = pad_sequences(X_train_encoded, maxlen = max_len)\n",
        "print(\"í›ˆë ¨ ë°ì´í„°ì˜ í¬ê¸°(shape):\", X_train_padded.shape)\n",
        "# 189ë³´ë‹¤ ê¸¸ì´ê°€ ì§§ì€ ë©”ì¼ ìƒ˜í”Œì€ ì „ë¶€ ìˆ«ì 0ì´ íŒ¨ë”©ë˜ì–´ 189ì˜ ê¸¸ì´ë¥¼ ê°€ì§€ë„ë¡ ë§Œë“¦"
      ],
      "metadata": {
        "id": "9uUxhOYBSlcr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **2.RNNìœ¼ë¡œ ìŠ¤íŒ¸ ë©”ì¼ ë¶„ë¥˜í•˜ê¸°**"
      ],
      "metadata": {
        "id": "t24aQmxFQZ8G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import SimpleRNN, Embedding, Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "\n",
        "# ëª¨ë¸ êµ¬ì„±\n",
        "embedding_dim = 32\n",
        "hidden_units = 32\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, embedding_dim))\n",
        "model.add(SimpleRNN(hidden_units))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
        "history = model.fit(X_train_padded, y_train, epochs=4, batch_size=64, validation_split=0.2)\n",
        "\n",
        "\n",
        "# í…ŒìŠ¤íŠ¸ ë°ì´í„° ì „ì²˜ë¦¬\n",
        "X_test_encoded = tokenizer.texts_to_sequences(X_test)\n",
        "X_test_padded = pad_sequences(X_test_encoded, maxlen = max_len)\n",
        "print(\"\\ní…ŒìŠ¤íŠ¸ ì •í™•ë„: %.4f\" % (model.evaluate(X_test_padded, y_test)[1]))\n"
      ],
      "metadata": {
        "id": "c9hiTkxtQcpi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========== í•µì‹¬ ì˜ˆì¸¡ ë° ë¹„êµ ì½”ë“œ ==========\n",
        "from sklearn.metrics import classification_report\n",
        "import numpy as np\n",
        "\n",
        "# 1. ì˜ˆì¸¡ê°’ ìƒì„±\n",
        "y_pred_proba = model.predict(X_test_padded)  # í™•ë¥ ê°’ (0~1)\n",
        "y_pred = (y_pred_proba > 0.5).astype(int).flatten()  # ì´ì§„ ë¶„ë¥˜ê°’ (0 or 1)\n",
        "\n",
        "# 2. ê²°ê³¼ ë¹„êµ\n",
        "print(\"\\n=== ì˜ˆì¸¡ ê²°ê³¼ ===\")\n",
        "print(f\"ì‹¤ì œê°’: {y_test[:10]}\")\n",
        "print(f\"ì˜ˆì¸¡ê°’: {y_pred[:10]}\")\n",
        "print(f\"ì˜ˆì¸¡ í™•ë¥ : {y_pred_proba[:10].flatten()}\")\n",
        "\n",
        "# 3. ì„±ëŠ¥ ì§€í‘œ\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "print(f\"\\nì •í™•ë„: {accuracy:.4f}\")\n",
        "print(f\"í˜¼ë™ í–‰ë ¬:\\n{cm}\")\n",
        "print(\"\\në¶„ë¥˜ ë¦¬í¬íŠ¸:\")\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "pOC_v37vW5vb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========== ìƒˆë¡œìš´ ë©”ì¼ í…ŒìŠ¤íŠ¸ ì½”ë“œ ==========\n",
        "def predict_spam(text_list, model, tokenizer, max_len=189):\n",
        "    \"\"\"\n",
        "    ìƒˆë¡œìš´ ë©”ì¼ í…ìŠ¤íŠ¸ë“¤ì´ ìŠ¤íŒ¸ì¸ì§€ ì˜ˆì¸¡í•˜ëŠ” í•¨ìˆ˜\n",
        "\n",
        "    Parameters:\n",
        "    - text_list: ì˜ˆì¸¡í•  ë©”ì¼ í…ìŠ¤íŠ¸ë“¤ì˜ ë¦¬ìŠ¤íŠ¸\n",
        "    - model: í•™ìŠµëœ RNN ëª¨ë¸\n",
        "    - tokenizer: í•™ìŠµì— ì‚¬ìš©ëœ í† í¬ë‚˜ì´ì €\n",
        "    - max_len: íŒ¨ë”©í•  ìµœëŒ€ ê¸¸ì´\n",
        "\n",
        "    Returns:\n",
        "    - predictions: ê° í…ìŠ¤íŠ¸ì— ëŒ€í•œ ì˜ˆì¸¡ ê²°ê³¼\n",
        "    \"\"\"\n",
        "    # í…ìŠ¤íŠ¸ë¥¼ ì‹œí€€ìŠ¤ë¡œ ë³€í™˜\n",
        "    sequences = tokenizer.texts_to_sequences(text_list)\n",
        "    # íŒ¨ë”© ì ìš©\n",
        "    padded = pad_sequences(sequences, maxlen=max_len)\n",
        "    # ì˜ˆì¸¡ ìˆ˜í–‰\n",
        "    predictions = model.predict(padded)\n",
        "\n",
        "    return predictions\n",
        "\n",
        "# í…ŒìŠ¤íŠ¸í•  ìƒˆë¡œìš´ ë©”ì¼ ì˜ˆì‹œ 3ê°œ\n",
        "test_emails = [\n",
        "    # ì˜ˆì‹œ 1: ëª…ë°±í•œ ìŠ¤íŒ¸ ë©”ì¼\n",
        "    \"URGENT! You have won $1000000! Click here now to claim your prize! Limited time offer! Call now!\",\n",
        "\n",
        "    # ì˜ˆì‹œ 2: ì •ìƒ ë©”ì¼\n",
        "    \"Hi John, hope you're doing well. Let's meet for coffee tomorrow at 3pm. Looking forward to catching up!\",\n",
        "\n",
        "    # ì˜ˆì‹œ 3: ì• ë§¤í•œ ê²½ê³„ì„  ë©”ì¼ (í”„ë¡œëª¨ì…˜ì„±ì´ì§€ë§Œ ìŠ¤íŒ¸ì€ ì•„ë‹ ìˆ˜ ìˆìŒ)\n",
        "    \"Special discount on our premium products. Save 30% this week only. Visit our website for more details.\"\n",
        "]\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ìƒˆë¡œìš´ ë©”ì¼ ìŠ¤íŒ¸ ë¶„ë¥˜ í…ŒìŠ¤íŠ¸\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "\n",
        "# ì˜ˆì¸¡ ìˆ˜í–‰\n",
        "predictions = predict_spam(test_emails, model, tokenizer, max_len)\n",
        "\n",
        "# ê²°ê³¼ ì¶œë ¥\n",
        "for i, (email, pred) in enumerate(zip(test_emails, predictions)):\n",
        "    prob = pred[0]  # í™•ë¥ ê°’ ì¶”ì¶œ\n",
        "    is_spam = \"ìŠ¤íŒ¸ ë©”ì¼\" if prob > 0.5 else \"ì •ìƒ ë©”ì¼\"\n",
        "    confidence = prob if prob > 0.5 else 1 - prob\n",
        "\n",
        "    print(f\"\\nğŸ“§ í…ŒìŠ¤íŠ¸ ë©”ì¼ {i+1}:\")\n",
        "    print(f\"ë‚´ìš©: {email}\")\n",
        "    print(f\"ì˜ˆì¸¡ ê²°ê³¼: {is_spam}\")\n",
        "    print(f\"ìŠ¤íŒ¸ í™•ë¥ : {prob:.4f}\")\n",
        "    print(f\"ì‹ ë¢°ë„: {confidence:.4f} ({confidence*100:.1f}%)\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "# ========== ëŒ€í™”í˜• í…ŒìŠ¤íŠ¸ í•¨ìˆ˜ ==========\n",
        "def interactive_spam_test():\n",
        "    \"\"\"\n",
        "    ì‚¬ìš©ìê°€ ì§ì ‘ ë©”ì¼ ë‚´ìš©ì„ ì…ë ¥í•˜ì—¬ í…ŒìŠ¤íŠ¸í•  ìˆ˜ ìˆëŠ” í•¨ìˆ˜\n",
        "    \"\"\"\n",
        "    print(\"\\nğŸ” ì§ì ‘ ë©”ì¼ í…ŒìŠ¤íŠ¸í•´ë³´ê¸°\")\n",
        "    print(\"ë©”ì¼ ë‚´ìš©ì„ ì…ë ¥í•˜ì„¸ìš” (ì¢…ë£Œí•˜ë ¤ë©´ 'quit' ì…ë ¥):\")\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"\\në©”ì¼ ë‚´ìš©: \")\n",
        "\n",
        "        if user_input.lower() == 'quit':\n",
        "            print(\"í…ŒìŠ¤íŠ¸ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.\")\n",
        "            break\n",
        "\n",
        "        if user_input.strip() == '':\n",
        "            print(\"ë©”ì¼ ë‚´ìš©ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.\")\n",
        "            continue\n",
        "\n",
        "        # ì˜ˆì¸¡ ìˆ˜í–‰\n",
        "        prediction = predict_spam([user_input], model, tokenizer, max_len)\n",
        "        prob = prediction[0][0]\n",
        "        is_spam = \"ìŠ¤íŒ¸ ë©”ì¼\" if prob > 0.5 else \"ì •ìƒ ë©”ì¼\"\n",
        "        confidence = prob if prob > 0.5 else 1 - prob\n",
        "\n",
        "        print(f\"ğŸ“Š ë¶„ì„ ê²°ê³¼:\")\n",
        "        print(f\"   ì˜ˆì¸¡: {is_spam}\")\n",
        "        print(f\"   ìŠ¤íŒ¸ í™•ë¥ : {prob:.4f}\")\n",
        "        print(f\"   ì‹ ë¢°ë„: {confidence:.4f} ({confidence*100:.1f}%)\")\n",
        "\n",
        "# ëŒ€í™”í˜• í…ŒìŠ¤íŠ¸ ì‹¤í–‰ (ì£¼ì„ í•´ì œí•˜ì—¬ ì‚¬ìš©)\n",
        "# interactive_spam_test()\n",
        "\n",
        "# ========== ì¶”ê°€ ë¶„ì„: ë‹¨ì–´ ì¤‘ìš”ë„ í™•ì¸ ==========\n",
        "def analyze_email_tokens(email_text, tokenizer, max_len=189):\n",
        "    \"\"\"\n",
        "    ì´ë©”ì¼ì˜ í† í°í™” ê³¼ì •ì„ ë¶„ì„í•˜ëŠ” í•¨ìˆ˜\n",
        "    \"\"\"\n",
        "    print(f\"\\nğŸ” '{email_text[:50]}...' ë¶„ì„\")\n",
        "\n",
        "    # í† í°í™”\n",
        "    sequence = tokenizer.texts_to_sequences([email_text])\n",
        "    padded = pad_sequences(sequence, maxlen=max_len)\n",
        "\n",
        "    # ì›ë³¸ ë‹¨ì–´ë“¤ í™•ì¸\n",
        "    words = email_text.split()\n",
        "    print(f\"ì›ë³¸ ë‹¨ì–´ë“¤: {words}\")\n",
        "\n",
        "    # í† í°í™”ëœ ê²°ê³¼\n",
        "    tokens = sequence[0]\n",
        "    print(f\"í† í°í™” ê²°ê³¼: {tokens}\")\n",
        "\n",
        "    # ì—­í† í°í™” (ìˆ«ì -> ë‹¨ì–´)\n",
        "    reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n",
        "    decoded_words = [reverse_word_map.get(token, '<UNK>') for token in tokens]\n",
        "    print(f\"í† í° -> ë‹¨ì–´: {decoded_words}\")\n",
        "\n",
        "    return padded\n",
        "\n",
        "# ê° í…ŒìŠ¤íŠ¸ ë©”ì¼ì— ëŒ€í•´ í† í° ë¶„ì„\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"í† í°í™” ë¶„ì„\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for i, email in enumerate(test_emails):\n",
        "    analyze_email_tokens(email, tokenizer, max_len)\n",
        "    print(\"-\" * 40)"
      ],
      "metadata": {
        "id": "4Uhhkh1-j7kh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **LSTM(Long Short-Term Memory)**\n",
        "\n",
        "- LSTMì€ **ì¥ë‹¨ê¸° ë©”ëª¨ë¦¬ ë„¤íŠ¸ì›Œí¬**ë¡œ, RNN(ìˆœí™˜ì‹ ê²½ë§)ì˜ í•œ ì¢…ë¥˜\n",
        "- ê¸°ì¡´ RNNì˜ ê¸°ìš¸ê¸° ì†Œì‹¤ ë¬¸ì œ, **ì¥ê¸° ì˜ì¡´ì„± ë¬¸ì œ(the problem of Long-Term Dependencies)ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ê°œë°œëœ ê³ ê¸‰ ì‹ ê²½ë§ êµ¬ì¡°**\n",
        "- LSTMì€ ì€ë‹‰ì¸µì˜ ë©”ëª¨ë¦¬ ì…€ì— ì…ë ¥ ê²Œì´íŠ¸, ë§ê° ê²Œì´íŠ¸, ì¶œë ¥ ê²Œì´íŠ¸ë¥¼ ì¶”ê°€í•˜ì—¬ ë¶ˆí•„ìš”í•œ ê¸°ì–µì„ ì§€ìš°ê³ , ê¸°ì–µí•´ì•¼í•  ê²ƒë“¤ì„ ì •í•¨\n",
        "- - ì•„í‚¤í…ì³ (ì°¸ê³ : https://wikidocs.net/22888)"
      ],
      "metadata": {
        "id": "3VWAwcAVlSjv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![RNN](https://wikidocs.net/images/page/22888/vanilla_rnn_ver2.PNG \"RNN\")\n"
      ],
      "metadata": {
        "id": "kG2aAdlcp0Ap"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![LSTM](https://wikidocs.net/images/page/22888/vaniila_rnn_and_different_lstm_ver2.PNG \"LSTM\")"
      ],
      "metadata": {
        "id": "e_MDGDGyp1Zl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ì˜ˆì œ: **í•œê¸€ ìŠ¤íŒ¸ë©”ì¼ ë¶„ë¥˜ê¸°**"
      ],
      "metadata": {
        "id": "x8r9nH0VnhES"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import re\n",
        "import pickle\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# í•œê¸€ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ í•¨ìˆ˜\n",
        "def preprocess_korean_text(text):\n",
        "    \"\"\"\n",
        "    í•œê¸€ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ í•¨ìˆ˜\n",
        "    - íŠ¹ìˆ˜ë¬¸ì ì œê±°\n",
        "    - ë¶ˆí•„ìš”í•œ ê³µë°± ì œê±°\n",
        "    - ì†Œë¬¸ì ë³€í™˜ì€ í•œê¸€ì—ì„œ ë¶ˆí•„ìš”\n",
        "    \"\"\"\n",
        "    # í•œê¸€, ì˜ë¬¸, ìˆ«ì, ê³µë°±ë§Œ ë‚¨ê¸°ê¸°\n",
        "    text = re.sub(r'[^ê°€-í£a-zA-Z0-9\\s]', ' ', text)\n",
        "    # ì—°ì†ëœ ê³µë°±ì„ í•˜ë‚˜ë¡œ ë³€ê²½\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    # ì•ë’¤ ê³µë°± ì œê±°\n",
        "    text = text.strip()\n",
        "    return text\n",
        "\n",
        "# í•œê¸€ ìŠ¤íŒ¸/ì •ìƒ ë©”ì¼ ëª¨ì˜ ë°ì´í„°ì…‹ ìƒì„±\n",
        "def create_korean_email_dataset():\n",
        "    \"\"\"\n",
        "    í•œê¸€ ìŠ¤íŒ¸/ì •ìƒ ë©”ì¼ ëª¨ì˜ ë°ì´í„°ì…‹ ìƒì„±\n",
        "    ì‹¤ì œ í”„ë¡œì íŠ¸ì—ì„œëŠ” ê³µê°œ ë°ì´í„°ì…‹ì´ë‚˜ ìˆ˜ì§‘ëœ ë°ì´í„°ë¥¼ ì‚¬ìš©\n",
        "    \"\"\"\n",
        "\n",
        "    # ì •ìƒ ë©”ì¼ ì˜ˆì‹œ (500ê°œ ìƒì„±ì„ ìœ„í•œ ê¸°ë³¸ íŒ¨í„´)\n",
        "    normal_patterns = [\n",
        "        \"ì•ˆë…•í•˜ì„¸ìš” íšŒì˜ ì¼ì •ì„ ì•Œë ¤ë“œë¦½ë‹ˆë‹¤\",\n",
        "        \"í”„ë¡œì íŠ¸ ì§„í–‰ ìƒí™© ë³´ê³ ë“œë¦½ë‹ˆë‹¤\",\n",
        "        \"ì²¨ë¶€íŒŒì¼ í™•ì¸ ë¶€íƒë“œë¦½ë‹ˆë‹¤\",\n",
        "        \"ë‚´ì¼ ì˜¤í›„ ë¯¸íŒ… ì°¸ì„ ê°€ëŠ¥í•˜ì‹ ê°€ìš”\",\n",
        "        \"ì—…ë¬´ ê´€ë ¨ ë¬¸ì˜ì‚¬í•­ì´ ìˆìŠµë‹ˆë‹¤\",\n",
        "        \"ë³´ê³ ì„œ ì‘ì„± ì™„ë£Œí–ˆìŠµë‹ˆë‹¤\",\n",
        "        \"ì ì‹¬ì‹œê°„ì— ì ê¹ ì´ì•¼ê¸°í•  ìˆ˜ ìˆì„ê¹Œìš”\",\n",
        "        \"ê³ ê°ì‚¬ ë¯¸íŒ… ê²°ê³¼ ê³µìœ ë“œë¦½ë‹ˆë‹¤\",\n",
        "        \"ë‹¤ìŒ ì£¼ ì¼ì • ì¡°ìœ¨ ë¶€íƒë“œë¦½ë‹ˆë‹¤\",\n",
        "        \"êµìœ¡ ìë£Œ ì „ë‹¬ë“œë¦½ë‹ˆë‹¤\",\n",
        "        \"íšŒì˜ì‹¤ ì˜ˆì•½ í™•ì¸ ë¶€íƒë“œë¦½ë‹ˆë‹¤\",\n",
        "        \"ì¶œì¥ ê³„íšì„œ ê²€í†  ìš”ì²­ë“œë¦½ë‹ˆë‹¤\",\n",
        "        \"ì›”ê°„ ì„±ê³¼ ë³´ê³ ì„œ ì œì¶œí•©ë‹ˆë‹¤\",\n",
        "        \"íŒ€ ë¹Œë”© í–‰ì‚¬ ì°¸ì„ ì˜ì‚¬ í™•ì¸\",\n",
        "        \"ìƒˆ í”„ë¡œì íŠ¸ ì œì•ˆì„œ ê²€í†  ë°”ëë‹ˆë‹¤\"\n",
        "    ]\n",
        "\n",
        "    # ìŠ¤íŒ¸ ë©”ì¼ ì˜ˆì‹œ (500ê°œ ìƒì„±ì„ ìœ„í•œ ê¸°ë³¸ íŒ¨í„´)\n",
        "    spam_patterns = [\n",
        "        \"ì¶•í•˜í•©ë‹ˆë‹¤ 1ì–µì› ë‹¹ì²¨ë˜ì…¨ìŠµë‹ˆë‹¤ ì§€ê¸ˆ í™•ì¸í•˜ì„¸ìš”\",\n",
        "        \"ê¸´ê¸‰ ëŒ€ì¶œ ê°€ëŠ¥ ë¬´ì‹¬ì‚¬ ë‹¹ì¼ ìŠ¹ì¸\",\n",
        "        \"í´ë¦­ë§Œìœ¼ë¡œ ì›” 300ë§Œì› ìˆ˜ìµ ë³´ì¥\",\n",
        "        \"ë¬´ë£Œ ìƒí’ˆê¶Œ ì¦ì • ì§€ê¸ˆ ì‹ ì²­í•˜ì„¸ìš”\",\n",
        "        \"ë‹¤ì´ì–´íŠ¸ ë³´ì¡°ì œ íŠ¹ê°€ íŒë§¤ íš¨ê³¼ 100í¼ì„¼íŠ¸\",\n",
        "        \"íˆ¬ì ê¶Œìœ  ê³ ìˆ˜ìµ ë³´ì¥ ìœ„í—˜ ë¶€ë‹´ ì—†ìŒ\",\n",
        "        \"ì„±ì¸ìš©í’ˆ í• ì¸ íŒë§¤ ë¹„ë°€ ë°°ì†¡\",\n",
        "        \"ì¹´ì§€ë…¸ ê²Œì„ ì²« ê°€ì… ë³´ë„ˆìŠ¤ ì§€ê¸‰\",\n",
        "        \"ì£¼ì‹ ì •ë³´ ì œê³µ ìˆ˜ìµë¥  200í¼ì„¼íŠ¸\",\n",
        "        \"ì•„ë¥´ë°”ì´íŠ¸ ëª¨ì§‘ í•˜ë£¨ 10ë§Œì› ë³´ì¥\",\n",
        "        \"ì˜¨ë¼ì¸ ë„ë°• ì‚¬ì´íŠ¸ ê°€ì… ì¦‰ì‹œ ë³´ë„ˆìŠ¤\",\n",
        "        \"ì‹ ìš©ì¹´ë“œ í˜„ê¸ˆ ì„œë¹„ìŠ¤ ì¦‰ì‹œ ìŠ¹ì¸\",\n",
        "        \"ë¶ˆë²• ë³µì œí’ˆ íŒë§¤ ì •í’ˆ ë³´ì¥\",\n",
        "        \"í”¼ë¼ë¯¸ë“œ íŒë§¤ ì¡°ì§ ê°€ì… ê¶Œìœ \",\n",
        "        \"ê°€ìƒí™”í íˆ¬ì ì‚¬ê¸° ì˜í˜¹ ìƒí’ˆ\"\n",
        "    ]\n",
        "\n",
        "    # ë°ì´í„°ì…‹ í™•ì¥ ìƒì„±\n",
        "    emails = []\n",
        "    labels = []\n",
        "\n",
        "    # ì •ìƒ ë©”ì¼ 500ê°œ ìƒì„±\n",
        "    for i in range(500):\n",
        "        base_pattern = normal_patterns[i % len(normal_patterns)]\n",
        "        # íŒ¨í„´ì— ë³€í™”ë¥¼ ì£¼ì–´ ë‹¤ì–‘ì„± ì¦ê°€\n",
        "        variations = [\n",
        "            base_pattern,\n",
        "            base_pattern + f\" {i+1}ë²ˆì§¸ ê±´ì…ë‹ˆë‹¤\",\n",
        "            base_pattern + \" í™•ì¸ í›„ íšŒì‹  ë°”ëë‹ˆë‹¤\",\n",
        "            base_pattern + \" ê´€ë ¨í•˜ì—¬ ë…¼ì˜ê°€ í•„ìš”í•©ë‹ˆë‹¤\",\n",
        "            \"ì—…ë¬´: \" + base_pattern\n",
        "        ]\n",
        "        emails.append(variations[i % len(variations)])\n",
        "        labels.append(0)  # ì •ìƒ ë©”ì¼\n",
        "\n",
        "    # ìŠ¤íŒ¸ ë©”ì¼ 500ê°œ ìƒì„±\n",
        "    for i in range(500):\n",
        "        base_pattern = spam_patterns[i % len(spam_patterns)]\n",
        "        # ìŠ¤íŒ¸ì˜ íŠ¹ì§•ì„ ê°•í™”í•œ ë³€í™”\n",
        "        variations = [\n",
        "            base_pattern,\n",
        "            \"ğŸ‰\" + base_pattern + \"ğŸ‰\",\n",
        "            base_pattern + \" ì§€ê¸ˆ ì¦‰ì‹œ í´ë¦­í•˜ì„¸ìš”!\",\n",
        "            \"â˜…â˜…â˜… \" + base_pattern + \" â˜…â˜…â˜…\",\n",
        "            base_pattern + \" ë†“ì¹˜ë©´ í›„íšŒí•©ë‹ˆë‹¤!\"\n",
        "        ]\n",
        "        emails.append(variations[i % len(variations)])\n",
        "        labels.append(1)  # ìŠ¤íŒ¸ ë©”ì¼\n",
        "\n",
        "    return emails, labels\n",
        "\n",
        "# ë°ì´í„°ì…‹ ìƒì„± ë° ë¡œë“œ\n",
        "print(\"=\"*60)\n",
        "print(\"í•œê¸€ ìŠ¤íŒ¸ ë©”ì¼ ë¶„ë¥˜ê¸° ìƒì„± ì¤‘...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "emails, labels = create_korean_email_dataset()\n",
        "\n",
        "# ë°ì´í„°í”„ë ˆì„ ìƒì„±\n",
        "df = pd.DataFrame({\n",
        "    'email': emails,\n",
        "    'label': labels\n",
        "})\n",
        "\n",
        "# numpy ë°°ì—´ë¡œ ë³€í™˜í•˜ì—¬ ì•ˆì •ì„± í–¥ìƒ\n",
        "labels_array = np.array(labels)\n",
        "\n",
        "print(f\"ì´ ë°ì´í„° ìˆ˜: {len(df)}\")\n",
        "print(f\"ì •ìƒ ë©”ì¼ ìˆ˜: {np.sum(labels_array == 0)}\")\n",
        "print(f\"ìŠ¤íŒ¸ ë©”ì¼ ìˆ˜: {np.sum(labels_array == 1)}\")\n",
        "print(f\"ì •ìƒ ë©”ì¼ ë¹„ìœ¨: {np.sum(labels_array == 0)/len(labels_array)*100:.1f}%\")\n",
        "print(f\"ìŠ¤íŒ¸ ë©”ì¼ ë¹„ìœ¨: {np.sum(labels_array == 1)/len(labels_array)*100:.1f}%\")\n",
        "\n",
        "# ë°ì´í„° ì „ì²˜ë¦¬\n",
        "df['email_processed'] = df['email'].apply(preprocess_korean_text)\n",
        "\n",
        "# ë°ì´í„° ë¶„í• \n",
        "X = df['email_processed'].values\n",
        "y = df['label'].values\n",
        "\n",
        "# ë°ì´í„° íƒ€ì… í™•ì¸ ë° ë³€í™˜\n",
        "X = np.array(X, dtype=str)\n",
        "y = np.array(y, dtype=int)\n",
        "\n",
        "print(f\"ë°ì´í„° íƒ€ì… - X: {X.dtype}, y: {y.dtype}\")\n",
        "print(f\"ë°ì´í„° í˜•íƒœ - X: {X.shape}, y: {y.shape}\")\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(f\"\\ní›ˆë ¨ ë°ì´í„°: {len(X_train)}\")\n",
        "print(f\"í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(X_test)}\")\n",
        "\n",
        "# í† í¬ë‚˜ì´ì € ì„¤ì • (í•œê¸€ íŠ¹ì„± ê³ ë ¤)\n",
        "MAX_FEATURES = 10000  # ì–´íœ˜ í¬ê¸°\n",
        "MAX_LEN = 100  # ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´\n",
        "\n",
        "tokenizer = Tokenizer(\n",
        "    num_words=MAX_FEATURES,\n",
        "    oov_token='<OOV>',  # Out-of-vocabulary í† í°\n",
        "    filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'  # ì œê±°í•  ë¬¸ì\n",
        ")\n",
        "\n",
        "# í›ˆë ¨ ë°ì´í„°ë¡œ í† í¬ë‚˜ì´ì € í•™ìŠµ\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "# í…ìŠ¤íŠ¸ë¥¼ ì‹œí€€ìŠ¤ë¡œ ë³€í™˜\n",
        "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
        "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "# íŒ¨ë”© ì ìš©\n",
        "X_train_pad = pad_sequences(X_train_seq, maxlen=MAX_LEN, padding='post')\n",
        "X_test_pad = pad_sequences(X_test_seq, maxlen=MAX_LEN, padding='post')\n",
        "\n",
        "print(f\"\\nì–´íœ˜ í¬ê¸°: {len(tokenizer.word_index)}\")\n",
        "print(f\"ì‹œí€€ìŠ¤ ìµœëŒ€ ê¸¸ì´: {MAX_LEN}\")\n",
        "\n",
        "# ê°œì„ ëœ ëª¨ë¸ êµ¬ì¶• (Bidirectional LSTM ì‚¬ìš©)\n",
        "def create_enhanced_model():\n",
        "    model = Sequential([\n",
        "        Embedding(input_dim=MAX_FEATURES,\n",
        "                 output_dim=128,\n",
        "                 input_length=MAX_LEN,\n",
        "                 name='embedding'),\n",
        "        Bidirectional(LSTM(64, dropout=0.3, recurrent_dropout=0.3), name='bilstm'),\n",
        "        Dense(32, activation='relu', name='dense1'),\n",
        "        Dropout(0.5, name='dropout'),\n",
        "        Dense(1, activation='sigmoid', name='output')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# ëª¨ë¸ ìƒì„± ë° ì»´íŒŒì¼\n",
        "model = create_enhanced_model()\n",
        "model.compile(\n",
        "    optimizer=Adam(learning_rate=0.001),\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "print(\"\\nëª¨ë¸ êµ¬ì¡°:\")\n",
        "model.summary()\n",
        "\n",
        "# ì½œë°± ì„¤ì •\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=3,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(\n",
        "    monitor='val_loss',\n",
        "    factor=0.5,\n",
        "    patience=2,\n",
        "    min_lr=0.0001\n",
        ")\n",
        "\n",
        "# ëª¨ë¸ í›ˆë ¨\n",
        "print(\"\\nëª¨ë¸ í›ˆë ¨ ì‹œì‘...\")\n",
        "history = model.fit(\n",
        "    X_train_pad, y_train,\n",
        "    epochs=10,\n",
        "    batch_size=32,\n",
        "    validation_split=0.2,\n",
        "    callbacks=[early_stopping, reduce_lr],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# í…ŒìŠ¤íŠ¸ ì„±ëŠ¥ í‰ê°€\n",
        "test_loss, test_accuracy = model.evaluate(X_test_pad, y_test, verbose=0)\n",
        "print(f\"\\ní…ŒìŠ¤íŠ¸ ì •í™•ë„: {test_accuracy:.4f}\")\n",
        "\n",
        "# ì˜ˆì¸¡ ë° ìƒì„¸ í‰ê°€\n",
        "y_pred_proba = model.predict(X_test_pad)\n",
        "y_pred = (y_pred_proba > 0.5).astype(int).flatten()\n",
        "\n",
        "print(\"\\në¶„ë¥˜ ë¦¬í¬íŠ¸:\")\n",
        "print(classification_report(y_test, y_pred, target_names=['ì •ìƒ', 'ìŠ¤íŒ¸']))\n",
        "\n",
        "print(\"\\ní˜¼ë™ í–‰ë ¬:\")\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(cm)\n",
        "\n",
        "# ìŠ¤íŒ¸ ë¶„ë¥˜ ì˜ˆì¸¡ í•¨ìˆ˜\n",
        "def predict_spam_korean(text, model, tokenizer, max_len=MAX_LEN):\n",
        "    \"\"\"\n",
        "    í•œê¸€ í…ìŠ¤íŠ¸ê°€ ìŠ¤íŒ¸ì¸ì§€ ì˜ˆì¸¡í•˜ëŠ” í•¨ìˆ˜\n",
        "    \"\"\"\n",
        "    # ì „ì²˜ë¦¬\n",
        "    processed_text = preprocess_korean_text(text)\n",
        "\n",
        "    # í† í¬ë‚˜ì´ì§• ë° íŒ¨ë”©\n",
        "    sequence = tokenizer.texts_to_sequences([processed_text])\n",
        "    padded = pad_sequences(sequence, maxlen=max_len, padding='post')\n",
        "\n",
        "    # ì˜ˆì¸¡\n",
        "    prediction = model.predict(padded, verbose=0)[0][0]\n",
        "\n",
        "    return prediction\n",
        "\n",
        "# í…ŒìŠ¤íŠ¸ìš© í•œê¸€ ë©”ì¼ ì˜ˆì‹œ\n",
        "test_korean_emails = [\n",
        "    \"ì¶•í•˜í•©ë‹ˆë‹¤! ë³µê¶Œì— ë‹¹ì²¨ë˜ì…¨ìŠµë‹ˆë‹¤. 1ì–µì›ì„ ë°›ìœ¼ì‹œë ¤ë©´ ì§€ê¸ˆ ì¦‰ì‹œ ë§í¬ë¥¼ í´ë¦­í•˜ì„¸ìš”!\",\n",
        "    \"ë‚´ì¼ ì˜¤í›„ 3ì‹œ íšŒì˜ì‹¤ì—ì„œ í”„ë¡œì íŠ¸ ì§„í–‰ ìƒí™©ì„ ë…¼ì˜í•˜ê² ìŠµë‹ˆë‹¤. ì°¸ì„ ë¶€íƒë“œë¦½ë‹ˆë‹¤.\",\n",
        "    \"ë¬´ë£Œ ë‹¤ì´ì–´íŠ¸ ì•½í’ˆ ì¦ì •! íš¨ê³¼ 100% ë³´ì¥! ì§€ê¸ˆ ì‹ ì²­í•˜ë©´ íŠ¹ë³„ í• ì¸!\",\n",
        "    \"ë³´ê³ ì„œ ì‘ì„±ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ì²¨ë¶€íŒŒì¼ì„ í™•ì¸í•´ì£¼ì‹œê¸° ë°”ëë‹ˆë‹¤.\",\n",
        "    \"ê¸´ê¸‰! ëŒ€ì¶œ ìŠ¹ì¸ ì™„ë£Œ! ë¬´ì‹¬ì‚¬ ë‹¹ì¼ ì§€ê¸‰! ì—°ë½ ë°”ëë‹ˆë‹¤!\",\n",
        "    \"êµìœ¡ ìë£Œë¥¼ ê³µìœ ë“œë¦½ë‹ˆë‹¤. ê²€í†  í›„ ì˜ê²¬ ë¶€íƒë“œë¦½ë‹ˆë‹¤.\"\n",
        "]\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"í•œê¸€ ë©”ì¼ ìŠ¤íŒ¸ ë¶„ë¥˜ í…ŒìŠ¤íŠ¸\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for i, email in enumerate(test_korean_emails, 1):\n",
        "    prob = predict_spam_korean(email, model, tokenizer)\n",
        "    is_spam = \"ìŠ¤íŒ¸ ë©”ì¼\" if prob > 0.5 else \"ì •ìƒ ë©”ì¼\"\n",
        "    confidence = prob if prob > 0.5 else 1 - prob\n",
        "\n",
        "    print(f\"\\nğŸ“§ í…ŒìŠ¤íŠ¸ {i}:\")\n",
        "    print(f\"ë‚´ìš©: {email}\")\n",
        "    print(f\"ì˜ˆì¸¡: {is_spam}\")\n",
        "    print(f\"ìŠ¤íŒ¸ í™•ë¥ : {prob:.4f}\")\n",
        "    print(f\"ì‹ ë¢°ë„: {confidence:.4f} ({confidence*100:.1f}%)\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "# ëŒ€í™”í˜• í…ŒìŠ¤íŠ¸ í•¨ìˆ˜\n",
        "def interactive_korean_spam_test():\n",
        "    \"\"\"\n",
        "    ì‚¬ìš©ìê°€ ì§ì ‘ í•œê¸€ ë©”ì¼ì„ ì…ë ¥í•˜ì—¬ í…ŒìŠ¤íŠ¸í•˜ëŠ” í•¨ìˆ˜\n",
        "    \"\"\"\n",
        "    print(\"\\nğŸ” í•œê¸€ ë©”ì¼ ìŠ¤íŒ¸ ë¶„ë¥˜ í…ŒìŠ¤íŠ¸\")\n",
        "    print(\"ë©”ì¼ ë‚´ìš©ì„ ì…ë ¥í•˜ì„¸ìš” (ì¢…ë£Œ: 'quit'):\")\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"\\nâœ‰ï¸ ë©”ì¼ ë‚´ìš©: \")\n",
        "\n",
        "        if user_input.lower() in ['quit', 'ì¢…ë£Œ', 'q']:\n",
        "            print(\"í…ŒìŠ¤íŠ¸ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.\")\n",
        "            break\n",
        "\n",
        "        if user_input.strip() == '':\n",
        "            print(\"ë©”ì¼ ë‚´ìš©ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.\")\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            prob = predict_spam_korean(user_input, model, tokenizer)\n",
        "            is_spam = \"ìŠ¤íŒ¸ ë©”ì¼\" if prob > 0.5 else \"ì •ìƒ ë©”ì¼\"\n",
        "            confidence = prob if prob > 0.5 else 1 - prob\n",
        "\n",
        "            print(f\"\\nğŸ“Š ë¶„ì„ ê²°ê³¼:\")\n",
        "            print(f\"   ğŸ·ï¸  ë¶„ë¥˜: {is_spam}\")\n",
        "            print(f\"   ğŸ“ˆ ìŠ¤íŒ¸ í™•ë¥ : {prob:.4f}\")\n",
        "            print(f\"   âœ… ì‹ ë¢°ë„: {confidence:.4f} ({confidence*100:.1f}%)\")\n",
        "\n",
        "            # ìŠ¤íŒ¸ì¼ ê°€ëŠ¥ì„±ì´ ë†’ì€ ê²½ìš° ê²½ê³  í‘œì‹œ\n",
        "            if prob > 0.8:\n",
        "                print(f\"   âš ï¸  ì£¼ì˜: ìŠ¤íŒ¸ì¼ ê°€ëŠ¥ì„±ì´ ë§¤ìš° ë†’ìŠµë‹ˆë‹¤!\")\n",
        "            elif prob < 0.2:\n",
        "                print(f\"   âœ… ì•ˆì „: ì •ìƒ ë©”ì¼ì¼ ê°€ëŠ¥ì„±ì´ ë§¤ìš° ë†’ìŠµë‹ˆë‹¤.\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}\")\n",
        "\n",
        "# ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ì €ì¥ í•¨ìˆ˜\n",
        "def save_model_and_tokenizer(model, tokenizer, model_path='korean_spam_model.h5', tokenizer_path='korean_tokenizer.pickle'):\n",
        "    \"\"\"\n",
        "    í•™ìŠµëœ ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ì €ì¥í•˜ëŠ” í•¨ìˆ˜\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # ëª¨ë¸ ì €ì¥\n",
        "        model.save(model_path)\n",
        "        print(f\"âœ… ëª¨ë¸ì´ {model_path}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
        "\n",
        "        # í† í¬ë‚˜ì´ì € ì €ì¥\n",
        "        with open(tokenizer_path, 'wb') as handle:\n",
        "            pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "        print(f\"âœ… í† í¬ë‚˜ì´ì €ê°€ {tokenizer_path}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
        "        print(\"í˜„ì¬ ì„¸ì…˜ì—ì„œë§Œ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì„¸ìš”.\")\n",
        "\n",
        "# ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ì €ì¥ (ì„ íƒì‚¬í•­)\n",
        "try:\n",
        "    save_model_and_tokenizer(model, tokenizer)\n",
        "except:\n",
        "    print(\"âš ï¸ íŒŒì¼ ì €ì¥ì„ ê±´ë„ˆëœë‹ˆë‹¤. í˜„ì¬ ì„¸ì…˜ì—ì„œë§Œ ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤.\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"í•œê¸€ ìŠ¤íŒ¸ ë©”ì¼ ë¶„ë¥˜ê¸° ì™„ì„±!\")\n",
        "print(\"ì•„ë˜ í•¨ìˆ˜ë¥¼ ì‹¤í–‰í•˜ì—¬ ì§ì ‘ í…ŒìŠ¤íŠ¸í•´ë³´ì„¸ìš”:\")\n",
        "print(\"interactive_korean_spam_test()\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# ëŒ€í™”í˜• í…ŒìŠ¤íŠ¸ ì‹¤í–‰ (ì£¼ì„ í•´ì œí•˜ì—¬ ì‚¬ìš©)\n",
        "# interactive_korean_spam_test()"
      ],
      "metadata": {
        "id": "7LUl3Od6lR34"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ëŒ€í™”í˜• í…ŒìŠ¤íŠ¸ ì‹¤í–‰ (ì£¼ì„ í•´ì œí•˜ì—¬ ì‚¬ìš©)\n",
        "interactive_korean_spam_test()"
      ],
      "metadata": {
        "id": "A5Zmt7yLme8g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Seq2Seq (ì‹œí€€ìŠ¤-íˆ¬-ì‹œí€€ìŠ¤(Sequence-to-Sequence, seq2seq)\n",
        "\n",
        "- **ì…ë ¥ ì‹œí€€ìŠ¤ë¥¼ ë°›ì•„ì„œ ë‹¤ë¥¸ ì‹œí€€ìŠ¤ë¡œ ë³€í™˜í•˜ëŠ” ëª¨ë¸**\n",
        "- ë§ˆì¹˜ ë²ˆì—­ê¸°ì²˜ëŸ¼ í•œ ì–¸ì–´ì˜ ë¬¸ì¥ì„ ë‹¤ë¥¸ ì–¸ì–´ë¡œ ë°”ê¾¸ëŠ” ê²ƒê³¼ ê°™ìŒ\n",
        "- ì—­ì‚¬ì  ë°°ê²½\n",
        "    - 2014ë…„: Googleì´ ê¸°ê³„ë²ˆì—­ìš©ìœ¼ë¡œ ì œì•ˆ\n",
        "    - ë°°ê²½: ê¸°ì¡´ ë°©ë²•ë“¤ì€ ê³ ì • ê¸¸ì´ ì…ë ¥ë§Œ ì²˜ë¦¬ ê°€ëŠ¥í–ˆìŒ\n",
        "    - í˜ì‹ : ê°€ë³€ ê¸¸ì´ ì…ë ¥ì„ ê°€ë³€ ê¸¸ì´ ì¶œë ¥ìœ¼ë¡œ ë³€í™˜ ê°€ëŠ¥\n",
        "- ê¸°ë³¸ êµ¬ì¡°\n",
        "\n",
        "|êµ¬ì„± ìš”ì†Œ|ì—­í• |ë¹„ìœ |\n",
        "|---|---|---|\n",
        "|Encoder|ì…ë ¥ ì‹œí€€ìŠ¤ë¥¼ ì´í•´í•˜ê³  ì••ì¶•|ì±…ì„ ì½ê³  ìš”ì•½í•˜ëŠ” ì‚¬ëŒ|\n",
        "|Decoder|ì••ì¶•ëœ ì •ë³´ë¡œ ì¶œë ¥ ì‹œí€€ìŠ¤ ìƒì„±|ìš”ì•½ë³¸ì„ ë³´ê³  ë‹¤ë¥¸ ì–¸ì–´ë¡œ ì“°ëŠ” ì‚¬ëŒ|\n",
        "|Context Vector|ì¸ì½”ë”ì™€ ë””ì½”ë”ë¥¼ ì—°ê²°í•˜ëŠ” ì •ë³´|ë‘ ì‚¬ëŒ ì‚¬ì´ì˜ ë©”ëª¨|\n"
      ],
      "metadata": {
        "id": "nvxSncXMywPP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ì‘ë™ ê³¼ì •\n",
        "    - ì…ë ¥: \"I love you\"\n",
        "    -        â†“ (Encoder)\n",
        "    - Context Vector (ì••ì¶•ëœ ì˜ë¯¸)\n",
        "    -        â†“ (Decoder)  \n",
        "    - ì¶œë ¥: \"ë‚˜ëŠ” ë„ˆë¥¼ ì‚¬ë‘í•´\""
      ],
      "metadata": {
        "id": "asTdfE8bzp2q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ì•„í‚¤í…ì²˜ (ì°¸ê³  https://wikidocs.net/24996)\n"
      ],
      "metadata": {
        "id": "8Q3Wp85W0PDG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Seq2seq](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdna%2FLUwms%2FbtszM0Eg9wB%2FAAAAAAAAAAAAAAAAAAAAAHUSsygDhXBD9OsONcPC84p1qhBQHqdlxVNImS4aFdRi%2Fimg.jpg%3Fcredential%3DyqXZFxpELC7KVnFOS48ylbz2pIh7yKj8%26expires%3D1759244399%26allow_ip%3D%26allow_referer%3D%26signature%3DhFLLizlqkuD3zD3pRxfQeu4%252Be7g%253D \"Seq2seq\")"
      ],
      "metadata": {
        "id": "uuCGQ9vt8ol_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ì˜ˆì œ1 : ê¸°ë³¸ Seq2Seq êµ¬ì¡°"
      ],
      "metadata": {
        "id": "N5J3KSgJ2CRp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.lstm = nn.LSTM(embed_size, hidden_size, batch_first=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        output, (hidden, cell) = self.lstm(embedded)\n",
        "        return hidden, cell  # ë§ˆì§€ë§‰ ìƒíƒœë§Œ ë°˜í™˜\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.lstm = nn.LSTM(embed_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, x, hidden, cell):\n",
        "        embedded = self.embedding(x)\n",
        "        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n",
        "        prediction = self.fc(output)\n",
        "        return prediction, hidden, cell\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def forward(self, source, target):\n",
        "        # ì¸ì½”ë”©: ì…ë ¥ ì‹œí€€ìŠ¤ë¥¼ context vectorë¡œ ì••ì¶•\n",
        "        hidden, cell = self.encoder(source)\n",
        "\n",
        "        # ë””ì½”ë”©: context vectorë¡œë¶€í„° ì¶œë ¥ ì‹œí€€ìŠ¤ ìƒì„±\n",
        "        outputs = []\n",
        "        input_token = target[:, 0:1]  # ì‹œì‘ í† í°\n",
        "\n",
        "        for i in range(1, target.size(1)):\n",
        "            output, hidden, cell = self.decoder(input_token, hidden, cell)\n",
        "            outputs.append(output)\n",
        "            input_token = target[:, i:i+1]  # ë‹¤ìŒ ì…ë ¥\n",
        "\n",
        "        return torch.cat(outputs, dim=1)\n",
        "\n",
        "# ëª¨ë¸ ìƒì„±\n",
        "vocab_size = 1000\n",
        "embed_size = 256\n",
        "hidden_size = 512\n",
        "\n",
        "encoder = Encoder(vocab_size, embed_size, hidden_size)\n",
        "decoder = Decoder(vocab_size, embed_size, hidden_size)\n",
        "model = Seq2Seq(encoder, decoder)\n",
        "\n",
        "print(\"Seq2Seq ëª¨ë¸ ìƒì„± ì™„ë£Œ!\")\n",
        "print(f\"Encoder: {encoder}\")\n",
        "print(f\"Decoder: {decoder}\")\n"
      ],
      "metadata": {
        "id": "nwb6s0XU2E2q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mNIgtqgEnQ8"
      },
      "source": [
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Attention ë©”ì»¤ë‹ˆì¦˜**"
      ],
      "metadata": {
        "id": "6RRXAC1HeE0p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **ì–´ë–¤ ë¶€ë¶„ì— ì§‘ì¤‘í• ì§€ ê²°ì •í•˜ëŠ” ë©”ì»¤ë‹ˆì¦˜**(ì–´í…ì…˜ì€ ê´€ë ¨ì„±ì´ ë†’ì€ ì •ë³´ì— ì§‘ì¤‘í•˜ëŠ” ë©”ì»¤ë‹ˆì¦˜)\n",
        "- ë§ˆì¹˜ ì±…ì„ ì½ì„ ë•Œ ì¤‘ìš”í•œ ë¬¸ì¥ì— í˜•ê´‘íœì„ ì¹˜ëŠ” ê²ƒê³¼ ê°™ë‹¤\n",
        "\n"
      ],
      "metadata": {
        "id": "5EN82gyCeMKz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- êµ¬ì„±ìš”ì†Œ\n",
        "\n",
        "|êµ¬ì„± ìš”ì†Œ|ì—­í• |ë¹„ìœ |\n",
        "|---|---|---|\n",
        "|Query (Q)|ì§ˆë¬¸, ì°¾ê³  ìˆëŠ” ê²ƒ|\"ë²ˆì—­í•˜ë ¤ëŠ” í˜„ì¬ ë‹¨ì–´\"|\n",
        "|Key (K)|ì°¸ì¡°í•  ì •ë³´ë“¤ì˜ ì¸ë±ìŠ¤|\"ì‚¬ì „ì˜ ì°¾ê¸° ëª©ë¡\"|\n",
        "|Value (V)|ì‹¤ì œ ì •ë³´ ë‚´ìš©|\"ì‚¬ì „ì˜ ì‹¤ì œ ëœ» ì„¤ëª…\"|\n",
        "\n",
        "- Attention ì¢…ë¥˜\n",
        "\n",
        "|ì–´í…ì…˜ íƒ€ì…|ìš©ë„|Query|Key|Value|\n",
        "|---|---|---|---|---|\n",
        "|Self-Attention|ë¬¸ë§¥ ì´í•´|ê°™ì€ ë¬¸ì¥|ê°™ì€ ë¬¸ì¥|ê°™ì€ ë¬¸ì¥|\n",
        "|Cross-Attention|ë²ˆì—­, ìš”ì•½|íƒ€ê²Ÿ ì–¸ì–´|ì†ŒìŠ¤ ì–¸ì–´|ì†ŒìŠ¤ ì–¸ì–´|\n",
        "|Visual Attention|ì´ë¯¸ì§€ ìº¡ì…˜|ìƒì„±í•  ë‹¨ì–´|ì´ë¯¸ì§€ ì˜ì—­|ì´ë¯¸ì§€ íŠ¹ì§•|\n",
        "\n",
        "\n",
        "- ì‘ë™ ì›ë¦¬\n",
        "    - Queryë¡œ ì–´ë–¤ ì •ë³´ë¥¼ ì°¾ì„ì§€ ì •ì˜\n",
        "    - ëª¨ë“  Keyë“¤ê³¼ ìœ ì‚¬ë„ ê³„ì‚° (ì ìˆ˜ ë§¤ê¸°ê¸°)\n",
        "    - ì ìˆ˜ê°€ ë†’ì€ Valueë“¤ì— ë” ì§‘ì¤‘\n",
        "    - ê°€ì¤‘í•©ìœ¼ë¡œ ìµœì¢… ê²°ê³¼ ìƒì„±\n",
        "\n",
        "- ì•„í‚¤í…ì²˜ (ì°¸ê³  https://denev6.tistory.com/entry/Attention-Mechanism?category=1039051)"
      ],
      "metadata": {
        "id": "u3AybMBDttxF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Attention](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdna%2FcX4CSD%2FbtsGcN8fNML%2FAAAAAAAAAAAAAAAAAAAAAPaDbvrD7FwLowmxlMTPy2qzgCdlrNUXriNwMrY3_fn9%2Fimg.png%3Fcredential%3DyqXZFxpELC7KVnFOS48ylbz2pIh7yKj8%26expires%3D1759244399%26allow_ip%3D%26allow_referer%3D%26signature%3DeYx75sEAKFiOWcrO7ISyOMDj3VM%253D \"Attention\")\n",
        "\n"
      ],
      "metadata": {
        "id": "FNTEhsL2s8up"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Self-Attention**:\n",
        "    - \"í•œ ì‹œí€€ìŠ¤ ë‚´ì˜ ëª¨ë“  ìœ„ì¹˜ê°€ ì„œë¡œ ì§ì ‘ì ìœ¼ë¡œ ìƒí˜¸ì‘ìš©í•˜ì—¬ ê° ìœ„ì¹˜ê°€ ë‹¤ë¥¸ ëª¨ë“  ìœ„ì¹˜ë“¤ë¡œë¶€í„° ì–¼ë§ˆë‚˜ ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ì§€ ê²°ì •í•˜ëŠ” ë©”ì»¤ë‹ˆì¦˜\"\n",
        "    - ê¸°ë³¸ ì•„ì´ë””ì–´\n",
        "        - Query, Key, Valueê°€ ëª¨ë‘ ê°™ì€ ì…ë ¥ì—ì„œ ë‚˜ì˜´\n",
        "        - ë¬¸ì¥ì˜ ê° ë‹¨ì–´ê°€ ê°™ì€ ë¬¸ì¥ì˜ ë‹¤ë¥¸ ëª¨ë“  ë‹¨ì–´ë“¤ê³¼ ê´€ê³„ë¥¼ ê³„ì‚°\n",
        "    - ì‘ë™ ê³¼ì •\n",
        "        > ì…ë ¥ ë¬¸ì¥: \"The cat sat on the mat\"\n",
        "\n",
        "        >ê° ë‹¨ì–´ê°€ ì§ˆë¬¸: \"ë‚˜ì™€ ê´€ë ¨ìˆëŠ” ë‹¨ì–´ëŠ”?\"\n",
        "        >- \"cat\" â†’ \"The\"(0.1), \"cat\"(0.8), \"sat\"(0.6), \"on\"(0.1), \"the\"(0.1), \"mat\"(0.3)\n",
        "        >- \"sat\" â†’ \"The\"(0.1), \"cat\"(0.7), \"sat\"(0.9), \"on\"(0.4), \"the\"(0.1), \"mat\"(0.2)\n"
      ],
      "metadata": {
        "id": "1BZsiD2MDNWK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Scaled Dot-Product Attention\n",
        "    - Queryì™€ Keyì˜ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•˜ê³  âˆšd_kë¡œ ìŠ¤ì¼€ì¼ë§í•œ í›„, ì†Œí”„íŠ¸ë§¥ìŠ¤ë¥¼ ì ìš©í•´ Valueì— ê°€ì¤‘ì¹˜ë¥¼ ë¶€ì—¬í•˜ëŠ” ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜\n",
        "    - Attention(Q,K,V) = softmax(QK^T/âˆšd_k)V\n",
        "        - QK^T: Queryì™€ Key ê°„ ìœ ì‚¬ë„ ê³„ì‚°\n",
        "        - âˆšd_k: ì°¨ì› í¬ê¸°ë¡œ ë‚˜ëˆ ì„œ ê¸°ìš¸ê¸° ì•ˆì •í™”\n",
        "        - softmax: í™•ë¥  ë¶„í¬ë¡œ ë³€í™˜\n",
        "        - V: ì‹¤ì œ ì •ë³´ì— ê°€ì¤‘ì¹˜ ì ìš©\n",
        "    - ë¹„ìœ  : ë¹„ìœ : \"ì‹œí—˜ ë³¼ ë•Œ ì¤‘ìš”í•œ ë¶€ë¶„ì— í˜•ê´‘íœ ì¹˜ëŠ” ê²ƒ\" - ê´€ë ¨ì„± ë†’ì€ ì •ë³´ì— ë” ì§‘ì¤‘!"
      ],
      "metadata": {
        "id": "YHuPulCM-gWD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **ì˜ˆì œ1 : ê°€ì¥ ê¸°ë³¸ì ì¸ Attention**"
      ],
      "metadata": {
        "id": "tYVCIKaLqk5_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "def basic_attention(query, key, value):\n",
        "    \"\"\"ê°€ì¥ ê¸°ë³¸ì ì¸ ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜\"\"\"\n",
        "    # 1. ì ìˆ˜ ê³„ì‚° (Queryì™€ Keyì˜ ìœ ì‚¬ë„)\n",
        "    scores = torch.matmul(query, key.transpose(-2, -1))\n",
        "    print(f\"ì ìˆ˜ ê³„ì‚° ê²°ê³¼: {scores}\")\n",
        "\n",
        "    # 2. ì†Œí”„íŠ¸ë§¥ìŠ¤ë¡œ ê°€ì¤‘ì¹˜ ë³€í™˜\n",
        "    attention_weights = F.softmax(scores, dim=-1)\n",
        "    print(f\"ì–´í…ì…˜ ê°€ì¤‘ì¹˜: {attention_weights}\")\n",
        "\n",
        "    # 3. Valueì— ê°€ì¤‘ì¹˜ ì ìš©\n",
        "    output = torch.matmul(attention_weights, value)\n",
        "\n",
        "    return output, attention_weights\n",
        "\n",
        "# ì˜ˆì œ: 3ê°œ ë‹¨ì–´, 4ì°¨ì› ì„ë² ë”©\n",
        "query = torch.tensor([[1.0, 0.5, 0.2, 0.1]])  # ì°¾ê³ ì í•˜ëŠ” ê²ƒ\n",
        "key = torch.tensor([\n",
        "    [1.0, 0.3, 0.1, 0.0],  # ì²« ë²ˆì§¸ ë‹¨ì–´\n",
        "    [0.2, 1.0, 0.5, 0.3],  # ë‘ ë²ˆì§¸ ë‹¨ì–´\n",
        "    [0.1, 0.4, 1.0, 0.8]   # ì„¸ ë²ˆì§¸ ë‹¨ì–´\n",
        "])\n",
        "value = torch.tensor([\n",
        "    [2.0, 1.0, 0.5, 0.2],  # ì²« ë²ˆì§¸ ë‹¨ì–´ì˜ ì‹¤ì œ ì •ë³´\n",
        "    [1.5, 2.0, 1.0, 0.8],  # ë‘ ë²ˆì§¸ ë‹¨ì–´ì˜ ì‹¤ì œ ì •ë³´\n",
        "    [0.8, 1.2, 2.0, 1.5]   # ì„¸ ë²ˆì§¸ ë‹¨ì–´ì˜ ì‹¤ì œ ì •ë³´\n",
        "])\n",
        "\n",
        "print(\"=== ê¸°ë³¸ Attention ì˜ˆì œ ===\")\n",
        "output, weights = basic_attention(query, key, value)\n",
        "print(f\"ìµœì¢… ì¶œë ¥: {output}\")\n",
        "print(f\"ì–´ëŠ ë‹¨ì–´ì— ì§‘ì¤‘í–ˆë‚˜: {weights.squeeze().numpy()}\")"
      ],
      "metadata": {
        "id": "dHH_pLP2qjyw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **ì˜ˆì œ2 : Scaled Dot-Product Attention** (ì‹¤ì œ ì‚¬ìš©ë˜ëŠ” ë°©ì‹)"
      ],
      "metadata": {
        "id": "Nf1jtCg2rQyr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "def scaled_dot_product_attention(Q, K, V, mask=None):\n",
        "    \"\"\"ì‹¤ì œ Transformerì—ì„œ ì‚¬ìš©í•˜ëŠ” ì–´í…ì…˜\"\"\"\n",
        "    d_k = Q.size(-1)  # Key ì°¨ì›\n",
        "\n",
        "    # 1. Qì™€ Kì˜ ì ê³± í›„ ìŠ¤ì¼€ì¼ë§\n",
        "    scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)\n",
        "\n",
        "    # 2. ë§ˆìŠ¤í‚¹ ì ìš© (í•„ìš”í•œ ê²½ìš°)\n",
        "    if mask is not None:\n",
        "        scores = scores.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "    # 3. ì†Œí”„íŠ¸ë§¥ìŠ¤ë¡œ í™•ë¥  ë³€í™˜\n",
        "    attention_weights = F.softmax(scores, dim=-1)\n",
        "\n",
        "    # 4. Valueì— ê°€ì¤‘ì¹˜ ì ìš©\n",
        "    output = torch.matmul(attention_weights, V)\n",
        "\n",
        "    return output, attention_weights\n",
        "\n",
        "# ì˜ˆì œ: ë¬¸ì¥ \"I love AI\" (3ê°œ ë‹¨ì–´)\n",
        "print(\"\\n=== Scaled Dot-Product Attention ì˜ˆì œ ===\")\n",
        "\n",
        "# ê° ë‹¨ì–´ì˜ ì„ë² ë”© (ê°„ë‹¨í•œ ì˜ˆì‹œ)\n",
        "sentence_embeddings = torch.tensor([\n",
        "    [1.0, 0.2, 0.3, 0.1],  # \"I\"\n",
        "    [0.3, 1.0, 0.8, 0.2],  # \"love\"\n",
        "    [0.1, 0.5, 1.0, 0.9]   # \"AI\"\n",
        "], dtype=torch.float32)\n",
        "\n",
        "# Q, K, Vê°€ ëª¨ë‘ ê°™ì€ ê²½ìš° (Self-Attention)\n",
        "Q = K = V = sentence_embeddings\n",
        "\n",
        "output, attention_weights = scaled_dot_product_attention(Q, K, V)\n",
        "\n",
        "print(\"ì…ë ¥ ë¬¸ì¥ ì„ë² ë”©:\")\n",
        "print(sentence_embeddings)\n",
        "print(\"\\nì–´í…ì…˜ ê°€ì¤‘ì¹˜ (ë‹¨ì–´ë³„ ì§‘ì¤‘ë„):\")\n",
        "print(attention_weights.numpy())\n",
        "print(\"\\nì–´í…ì…˜ ì ìš© í›„ ì¶œë ¥:\")\n",
        "print(output.numpy())\n",
        "\n",
        "# ê° ë‹¨ì–´ê°€ ì–´ë””ì— ì§‘ì¤‘í–ˆëŠ”ì§€ í•´ì„\n",
        "words = [\"I\", \"love\", \"AI\"]\n",
        "print(\"\\n=== í•´ì„ ===\")\n",
        "for i, word in enumerate(words):\n",
        "    weights = attention_weights[i].numpy()\n",
        "    print(f\"'{word}'ê°€ ì§‘ì¤‘í•œ ë‹¨ì–´ë“¤:\")\n",
        "    for j, w in enumerate(words):\n",
        "        print(f\"  {w}: {weights[j]:.3f}\")\n",
        "    print()"
      ],
      "metadata": {
        "id": "a4q2Jl7trUAV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **ì˜ˆì œ3 : ì‹¤ì œ ë²ˆì—­ ì˜ˆì œë¡œ ì´í•´í•˜ê¸°**"
      ],
      "metadata": {
        "id": "nu9V7WLLrUc0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def attention_translation_example():\n",
        "    \"\"\"ë²ˆì—­ì—ì„œ ì–´í…ì…˜ì´ ì–´ë–»ê²Œ ì‘ë™í•˜ëŠ”ì§€ ë³´ì—¬ì£¼ëŠ” ì˜ˆì œ\"\"\"\n",
        "\n",
        "    # ì˜ì–´ ë¬¸ì¥: \"I love you\"\n",
        "    english_words = [\"I\", \"love\", \"you\"]\n",
        "    # í•œêµ­ì–´ ë²ˆì—­: \"ë‚˜ëŠ” ë„ˆë¥¼ ì‚¬ë‘í•´\"\n",
        "    korean_words = [\"ë‚˜ëŠ”\", \"ë„ˆë¥¼\", \"ì‚¬ë‘í•´\"]\n",
        "\n",
        "    # ê°„ë‹¨í•œ ì„ë² ë”© (ì‹¤ì œë¡œëŠ” ë” ë³µì¡)\n",
        "    english_embeddings = torch.tensor([\n",
        "        [1.0, 0.1, 0.2],  # I\n",
        "        [0.2, 1.0, 0.8],  # love\n",
        "        [0.1, 0.2, 1.0]   # you\n",
        "    ], dtype=torch.float32)\n",
        "\n",
        "    korean_embeddings = torch.tensor([\n",
        "        [0.9, 0.1, 0.1],  # ë‚˜ëŠ”\n",
        "        [0.1, 0.1, 0.9],  # ë„ˆë¥¼\n",
        "        [0.1, 0.9, 0.2]   # ì‚¬ë‘í•´\n",
        "    ], dtype=torch.float32)\n",
        "\n",
        "    print(\"=== ë²ˆì—­ ì–´í…ì…˜ ì˜ˆì œ ===\")\n",
        "    print(\"ì˜ì–´:\", english_words)\n",
        "    print(\"í•œêµ­ì–´:\", korean_words)\n",
        "\n",
        "    # ê° í•œêµ­ì–´ ë‹¨ì–´ê°€ ì˜ì–´ ë‹¨ì–´ë“¤ì— ì–¼ë§ˆë‚˜ ì§‘ì¤‘í•˜ëŠ”ì§€\n",
        "    for i, korean_word in enumerate(korean_words):\n",
        "        query = korean_embeddings[i:i+1]  # í˜„ì¬ í•œêµ­ì–´ ë‹¨ì–´\n",
        "        key = english_embeddings          # ëª¨ë“  ì˜ì–´ ë‹¨ì–´ë“¤\n",
        "        value = english_embeddings\n",
        "\n",
        "        output, weights = scaled_dot_product_attention(query, key, value)\n",
        "\n",
        "        print(f\"\\n'{korean_word}' ë²ˆì—­ì‹œ ì˜ì–´ ë‹¨ì–´ ì§‘ì¤‘ë„:\")\n",
        "        for j, eng_word in enumerate(english_words):\n",
        "            print(f\"  {eng_word}: {weights[0][j].item():.3f}\")\n",
        "\n",
        "attention_translation_example()"
      ],
      "metadata": {
        "id": "nkUNNPH-rUrd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **ì˜ˆì œ4 : ì‹œê°ì  ì–´í…ì…˜** (ì´ë¯¸ì§€ì—ì„œ í…ìŠ¤íŠ¸ ìƒì„±)"
      ],
      "metadata": {
        "id": "lWzMtrxjr7S2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def visual_attention_example():\n",
        "    \"\"\"ì´ë¯¸ì§€ ìº¡ì…˜ ìƒì„±ì—ì„œì˜ ì–´í…ì…˜ ì˜ˆì œ\"\"\"\n",
        "\n",
        "    # ì´ë¯¸ì§€ì˜ ë‹¤ë¥¸ ì˜ì—­ë“¤ (5ê°œ ì˜ì—­)\n",
        "    image_regions = [\"í•˜ëŠ˜\", \"ë‚˜ë¬´\", \"ì‚¬ëŒ\", \"ê°•ì•„ì§€\", \"ì”ë””\"]\n",
        "\n",
        "    # ê° ì˜ì—­ì˜ íŠ¹ì§• ë²¡í„° (ê°„ë‹¨íˆ 3ì°¨ì›)\n",
        "    region_features = torch.tensor([\n",
        "        [0.8, 0.2, 0.1],  # í•˜ëŠ˜ (íŒŒë€ìƒ‰ ìœ„ì£¼)\n",
        "        [0.2, 0.8, 0.3],  # ë‚˜ë¬´ (ì´ˆë¡ìƒ‰ ìœ„ì£¼)\n",
        "        [0.6, 0.4, 0.5],  # ì‚¬ëŒ (ë³µí•©ì )\n",
        "        [0.7, 0.3, 0.2],  # ê°•ì•„ì§€ (ê°ˆìƒ‰ ìœ„ì£¼)\n",
        "        [0.1, 0.9, 0.2]   # ì”ë”” (ì´ˆë¡ìƒ‰ ìœ„ì£¼)\n",
        "    ], dtype=torch.float32)\n",
        "\n",
        "    # ìƒì„±í•˜ë ¤ëŠ” ë‹¨ì–´ë“¤\n",
        "    words_to_generate = [\"ì‚¬ëŒì´\", \"ê°•ì•„ì§€ì™€\", \"í•¨ê»˜\"]\n",
        "    word_queries = torch.tensor([\n",
        "        [0.6, 0.4, 0.5],  # \"ì‚¬ëŒì´\" - ì‚¬ëŒ ì˜ì—­ì— ì§‘ì¤‘í•´ì•¼ í•¨\n",
        "        [0.7, 0.3, 0.2],  # \"ê°•ì•„ì§€ì™€\" - ê°•ì•„ì§€ ì˜ì—­ì— ì§‘ì¤‘\n",
        "        [0.4, 0.4, 0.4]   # \"í•¨ê»˜\" - ì „ì²´ì ìœ¼ë¡œ ë´ì•¼ í•¨\n",
        "    ], dtype=torch.float32)\n",
        "\n",
        "    print(\"=== ì‹œê°ì  ì–´í…ì…˜ ì˜ˆì œ ===\")\n",
        "    print(\"ì´ë¯¸ì§€ ì˜ì—­ë“¤:\", image_regions)\n",
        "\n",
        "    for i, word in enumerate(words_to_generate):\n",
        "        query = word_queries[i:i+1]\n",
        "        key = value = region_features\n",
        "\n",
        "        output, weights = scaled_dot_product_attention(query, key, value)\n",
        "\n",
        "        print(f\"\\n'{word}' ìƒì„±ì‹œ ì´ë¯¸ì§€ ì˜ì—­ë³„ ì§‘ì¤‘ë„:\")\n",
        "        for j, region in enumerate(image_regions):\n",
        "            attention_score = weights[0][j].item()\n",
        "            bar = \"â–ˆ\" * int(attention_score * 20)  # ë§‰ëŒ€ ê·¸ë˜í”„ë¡œ í‘œì‹œ\n",
        "            print(f\"  {region:4s}: {attention_score:.3f} {bar}\")\n",
        "\n",
        "visual_attention_example()"
      ],
      "metadata": {
        "id": "B99OetApr9uu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **ì˜ˆì œ5 : Self-Attentionìœ¼ë¡œ ë¬¸ë§¥ ì´í•´í•˜ê¸°**"
      ],
      "metadata": {
        "id": "Z6tNtp9PsRJv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def self_attention_context():\n",
        "    \"\"\"Self-Attentionìœ¼ë¡œ ë¬¸ë§¥ì„ ì–´ë–»ê²Œ ì´í•´í•˜ëŠ”ì§€ ë³´ì—¬ì£¼ëŠ” ì˜ˆì œ\"\"\"\n",
        "\n",
        "    # ë¬¸ì¥: \"The bank can guarantee deposits will eventually cover future tuition costs\"\n",
        "    # \"bank\"ê°€ ê¸ˆìœµê¸°ê´€ì¸ì§€ ê°•ë‘‘ì¸ì§€ ë¬¸ë§¥ìœ¼ë¡œ íŒë‹¨í•´ì•¼ í•¨\n",
        "\n",
        "    sentence = [\"The\", \"bank\", \"can\", \"guarantee\", \"deposits\"]\n",
        "\n",
        "    # ë‹¨ì–´ë³„ ê°„ë‹¨í•œ ì„ë² ë”© (ì‹¤ì œë¡œëŠ” í›¨ì”¬ ë³µì¡)\n",
        "    embeddings = torch.tensor([\n",
        "        [0.1, 0.2, 0.3, 0.1],  # The\n",
        "        [0.5, 0.3, 0.2, 0.4],  # bank (ì• ë§¤í•œ ì˜ë¯¸)\n",
        "        [0.2, 0.4, 0.8, 0.1],  # can\n",
        "        [0.3, 0.6, 0.4, 0.7],  # guarantee (ë³´ì¥í•˜ë‹¤ - ê¸ˆìœµ ê´€ë ¨)\n",
        "        [0.8, 0.2, 0.3, 0.9]   # deposits (ì˜ˆê¸ˆ - ê¸ˆìœµ ê´€ë ¨)\n",
        "    ], dtype=torch.float32)\n",
        "\n",
        "    print(\"=== Self-Attention ë¬¸ë§¥ ì´í•´ ì˜ˆì œ ===\")\n",
        "    print(\"ë¬¸ì¥:\", \" \".join(sentence))\n",
        "\n",
        "    # Self-attention ê³„ì‚°\n",
        "    output, attention_weights = scaled_dot_product_attention(embeddings, embeddings, embeddings)\n",
        "\n",
        "    # \"bank\" ë‹¨ì–´(ì¸ë±ìŠ¤ 1)ê°€ ë‹¤ë¥¸ ë‹¨ì–´ë“¤ì— ì–¼ë§ˆë‚˜ ì§‘ì¤‘í•˜ëŠ”ì§€ í™•ì¸\n",
        "    bank_attention = attention_weights[1]\n",
        "\n",
        "    print(f\"\\n'bank'ê°€ ê° ë‹¨ì–´ì— ì§‘ì¤‘í•˜ëŠ” ì •ë„:\")\n",
        "    for i, word in enumerate(sentence):\n",
        "        attention_score = bank_attention[i].item()\n",
        "        print(f\"  {word:9s}: {attention_score:.3f}\")\n",
        "\n",
        "    print(f\"\\ní•´ì„: 'bank'ê°€ 'guarantee'({bank_attention[3]:.3f})ì™€ 'deposits'({bank_attention[4]:.3f})ì—\")\n",
        "    print(\"ë†’ì€ ì§‘ì¤‘ë„ë¥¼ ë³´ì´ë¯€ë¡œ, ê¸ˆìœµê¸°ê´€ì˜ ì˜ë¯¸ë¡œ ì´í•´ë¨!\")\n",
        "\n",
        "self_attention_context()"
      ],
      "metadata": {
        "id": "2vAnOCXhsRiV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Transformer ë©”ì»¤ë‹ˆì¦˜**"
      ],
      "metadata": {
        "id": "bmAOXjcPtd9C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 2017ë…„ \"Attention Is All You Need\" ë…¼ë¬¸ ë°œí‘œ (Google)\n",
        "- https://arxiv.org/abs/1706.03762\n",
        "- ë°°ê²½: RNNì˜ ìˆœì°¨ ì²˜ë¦¬ í•œê³„ ê·¹ë³µ\n",
        "- í˜ì‹ : ë³‘ë ¬ ì²˜ë¦¬ + Self-Attentionìœ¼ë¡œ ì„±ëŠ¥ ëŒ€í­ í–¥ìƒ\n",
        "-\n",
        "- í•µì‹¬ êµ¬ì¡°\n",
        "|êµ¬ì„± ìš”ì†Œ|ê¸°ëŠ¥|í•µì‹¬ íŠ¹ì§•|\n",
        "|---|---|---|\n",
        "|Self-Attention|ë¬¸ì¥ ë‚´ ë‹¨ì–´ë“¤ ê°„ì˜ ê´€ê³„ íŒŒì•…|ëª¨ë“  ìœ„ì¹˜ë¥¼ ë™ì‹œì— ì°¸ì¡°|\n",
        "|Multi-Head Attention|ì—¬ëŸ¬ ê´€ì ì—ì„œ ë™ì‹œì— ì§‘ì¤‘|8ê°œ í—¤ë“œë¡œ ë‹¤ì–‘í•œ íŒ¨í„´ í•™ìŠµ|\n",
        "|Positional Encoding|ë‹¨ì–´ ìˆœì„œ ì •ë³´ ì œê³µ|ì‚¼ê°í•¨ìˆ˜ë¡œ ìœ„ì¹˜ ì¸ì½”ë”©|\n",
        "|Feed Forward Network|ë¹„ì„ í˜• ë³€í™˜|2ì¸µ ì™„ì „ì—°ê²°ì¸µ|\n",
        "|Layer Normalization|í•™ìŠµ ì•ˆì •í™”|ê° ì¸µë§ˆë‹¤ ì •ê·œí™”|"
      ],
      "metadata": {
        "id": "KL-57hMvt5GV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ë¹„ìœ \n",
        "    - RNN: ì±…ì„ í•œ ì¤„ì”© ì½ê¸° (ëŠë¦¬ì§€ë§Œ ìˆœì„œ ì¤‘ìš”)\n",
        "    - Transformer: ì±… ì „ì²´ë¥¼ í•œë²ˆì— ìŠ¤ìº” í›„ ì¤‘ìš” ë¶€ë¶„ì— ì§‘ì¤‘ (ë¹ ë¥´ê³  ì •í™•)\n",
        "        - \"ëª¨ë“  ë‹¨ì–´ê°€ ëª¨ë“  ë‹¨ì–´ì™€ ì§ì ‘ ëŒ€í™”í•  ìˆ˜ ìˆê²Œ í•˜ì!\"\n",
        "- ì•„í‚¤í…ì²˜ (ì°¸ê³  https://wikidocs.net/31379)"
      ],
      "metadata": {
        "id": "2GK5EpOnv4mO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<!-- ![Transformer](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdna%2FbaUw7E%2FbtsGwRgo7Gh%2FAAAAAAAAAAAAAAAAAAAAADsw8eQNDsJIV5siCEHkZ47STihuV8H4bDHsi0Pn9TWk%2Fimg.png%3Fcredential%3DyqXZFxpELC7KVnFOS48ylbz2pIh7yKj8%26expires%3D1759244399%26allow_ip%3D%26allow_referer%3D%26signature%3D2JkSSaqR3ITrU1zIsxbLgcxJomQ%253D \"Transformer\") -->\n",
        "\n",
        "\n",
        "![Transformer](https://wikidocs.net/images/page/236193/%ED%8A%B8%EB%9E%9C%EC%8A%A4%ED%8F%AC%EB%A8%B8.png \"Transformer\")"
      ],
      "metadata": {
        "id": "uVwA9W381S4t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **ì˜ˆì œ 1: ê°„ë‹¨í•œ Attention ê³„ì‚°**"
      ],
      "metadata": {
        "id": "uYPn-39xukWq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "def simple_attention(query, key, value):\n",
        "    \"\"\"ê°„ë‹¨í•œ ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜\"\"\"\n",
        "    # 1. Queryì™€ Keyì˜ ìœ ì‚¬ë„ ê³„ì‚°\n",
        "    scores = torch.matmul(query, key.transpose(-2, -1))\n",
        "\n",
        "    # 2. ì†Œí”„íŠ¸ë§¥ìŠ¤ë¡œ ê°€ì¤‘ì¹˜ ê³„ì‚°\n",
        "    attention_weights = F.softmax(scores, dim=-1)\n",
        "\n",
        "    # 3. Valueì— ê°€ì¤‘ì¹˜ ì ìš©\n",
        "    output = torch.matmul(attention_weights, value)\n",
        "\n",
        "    return output, attention_weights\n",
        "\n",
        "# ì˜ˆì œ ë°ì´í„° (ë¬¸ì¥ ê¸¸ì´=4, ì„ë² ë”© ì°¨ì›=6)\n",
        "seq_len, d_model = 4, 6\n",
        "query = torch.randn(1, seq_len, d_model)\n",
        "key = torch.randn(1, seq_len, d_model)\n",
        "value = torch.randn(1, seq_len, d_model)\n",
        "\n",
        "# ì–´í…ì…˜ ê³„ì‚°\n",
        "output, weights = simple_attention(query, key, value)\n",
        "\n",
        "print(\"ì…ë ¥ shape:\", query.shape)\n",
        "print(\"ì¶œë ¥ shape:\", output.shape)\n",
        "print(\"ì–´í…ì…˜ ê°€ì¤‘ì¹˜ shape:\", weights.shape)\n",
        "print(\"\\nì–´í…ì…˜ ê°€ì¤‘ì¹˜ (ì–´ë””ì— ì§‘ì¤‘í–ˆëŠ”ì§€):\")\n",
        "print(weights[0].detach().numpy())"
      ],
      "metadata": {
        "id": "kYj-Ex4ZusGl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **ì˜ˆì œ 2: Multi-Head Attention**"
      ],
      "metadata": {
        "id": "tmBAvUQhustY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_model // num_heads\n",
        "\n",
        "        # ì„ í˜• ë³€í™˜ì¸µë“¤\n",
        "        self.W_q = nn.Linear(d_model, d_model)\n",
        "        self.W_k = nn.Linear(d_model, d_model)\n",
        "        self.W_v = nn.Linear(d_model, d_model)\n",
        "        self.W_o = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_len, d_model = x.size()\n",
        "\n",
        "        # 1. Q, K, V ìƒì„±\n",
        "        Q = self.W_q(x)\n",
        "        K = self.W_k(x)\n",
        "        V = self.W_v(x)\n",
        "\n",
        "        # 2. ë©€í‹°í—¤ë“œë¡œ ë¶„í• \n",
        "        Q = Q.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        K = K.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        V = V.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        # 3. ì–´í…ì…˜ ê³„ì‚°\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
        "        attention_weights = F.softmax(scores, dim=-1)\n",
        "        attention_output = torch.matmul(attention_weights, V)\n",
        "\n",
        "        # 4. í—¤ë“œë“¤ í•©ì¹˜ê¸°\n",
        "        attention_output = attention_output.transpose(1, 2).contiguous().view(\n",
        "            batch_size, seq_len, d_model)\n",
        "\n",
        "        # 5. ìµœì¢… ì„ í˜• ë³€í™˜\n",
        "        output = self.W_o(attention_output)\n",
        "\n",
        "        return output\n",
        "\n",
        "# ì‚¬ìš© ì˜ˆì‹œ\n",
        "d_model, num_heads = 512, 8\n",
        "mha = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "# ì…ë ¥: (ë°°ì¹˜=2, ì‹œí€€ìŠ¤=10, íŠ¹ì„±=512)\n",
        "x = torch.randn(2, 10, d_model)\n",
        "output = mha(x)\n",
        "\n",
        "print(f\"ì…ë ¥ shape: {x.shape}\")\n",
        "print(f\"ì¶œë ¥ shape: {output.shape}\")\n",
        "print(\"Multi-Head Attention ì™„ë£Œ!\")"
      ],
      "metadata": {
        "id": "aFus3Q5nus6R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **ì˜ˆì œ 3: ê°„ë‹¨í•œ Transformer ë¸”ë¡**"
      ],
      "metadata": {
        "id": "nfTUKYQ7utF9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# (ê°€ì •) MultiHeadAttentionì€ ì•„ë˜ì™€ ê°™ì€ ì‹œê·¸ë‹ˆì²˜/ë™ì‘ì„ ê°€ì§„ë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤.\n",
        "# - ì…ë ¥: x (B, T, d_model)\n",
        "# - ì¶œë ¥: (B, T, d_model)  â† ê° í—¤ë“œì—ì„œ ì‚°ì¶œëœ ì–´í…ì…˜ ê²°ê³¼ë¥¼ concat + ì„ í˜• ë³€í™˜ í›„ ë°˜í™˜\n",
        "# - (í™•ì¥) ì‹¤ì œ êµ¬í˜„ì€ ë§ˆìŠ¤í¬, key_padding_mask ë“±ì„ ë°›ì„ ìˆ˜ ìˆìŒ. ì—¬ê¸°ì„œëŠ” ë‹¨ìˆœí™”.\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super().__init__()\n",
        "        assert d_model % num_heads == 0, \"d_modelì€ num_headsë¡œ ë‚˜ëˆ„ì–´ ë–¨ì–´ì ¸ì•¼ í•©ë‹ˆë‹¤.\"\n",
        "        self.attn = nn.MultiheadAttention(embed_dim=d_model, num_heads=num_heads, batch_first=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # ìê¸°ì–´í…ì…˜(Self-Attention): Q=K=V=x\n",
        "        # MultiheadAttentionì€ (B, T, C) ì…ë ¥ì„ ì§€ì›(batch_first=True)í•˜ë¯€ë¡œ ê·¸ëŒ€ë¡œ ì‚¬ìš© ê°€ëŠ¥\n",
        "        out, _ = self.attn(x, x, x, need_weights=False)\n",
        "        return out\n",
        "\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "        \"\"\"\n",
        "        í•˜ë‚˜ì˜ Transformer ì¸ì½”ë” ë¸”ë¡(ì› ë…¼ë¬¸ì˜ Post-LN êµ¬ì„±ê³¼ ë™ì¼í•œ í˜•íƒœ):\n",
        "        - ì„œë¸Œì¸µ 1: ë©€í‹°í—¤ë“œ ìê¸°ì–´í…ì…˜(Multi-Head Self-Attention)\n",
        "        - ì„œë¸Œì¸µ 2: í¬ì§€ì…˜-ì™€ì´ì¦ˆ í”¼ë“œí¬ì›Œë“œ ë„¤íŠ¸ì›Œí¬(FFN)\n",
        "        - ê° ì„œë¸Œì¸µ ë’¤ì— ì”ì°¨ì—°ê²°(Residual) + LayerNorm\n",
        "        - ë“œë¡­ì•„ì›ƒìœ¼ë¡œ ê³¼ì í•© ë°©ì§€\n",
        "\n",
        "        Args:\n",
        "            d_model (int): ì„ë² ë”© ì°¨ì› (ëª¨ë“  ì„œë¸Œì¸µì˜ ì…ì¶œë ¥ ì±„ë„ ìˆ˜)\n",
        "            num_heads (int): ë©€í‹°í—¤ë“œ ê°œìˆ˜ (d_model % num_heads == 0 ê¶Œì¥/í•„ìˆ˜)\n",
        "            d_ff (int): FFN ë‚´ë¶€ í™•ì¥ ì°¨ì› (ì¼ë°˜ì ìœ¼ë¡œ 4 * d_model)\n",
        "            dropout (float): ë“œë¡­ì•„ì›ƒ ë¹„ìœ¨ (í•™ìŠµ ì‹œë§Œ ì ìš©, eval()ì—ì„œëŠ” ë¹„í™œì„±)\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # 1) Multi-Head Attention ì„œë¸Œì¸µ\n",
        "        #    ì…ë ¥ê³¼ ë™ì¼ ì°¨ì›ì˜ ì¶œë ¥ì„ ë‚´ëŠ” ìê¸°ì–´í…ì…˜(Self-Attention) ëª¨ë“ˆ\n",
        "        self.attention = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "        # 2) Position-wise Feed-Forward Network (FFN)\n",
        "        #    ê° ì‹œì (í† í° ìœ„ì¹˜)ë³„ë¡œ ë™ì¼í•œ ë‘ ê°œì˜ ì„ í˜•ë³€í™˜ì„ ì ìš©:\n",
        "        #      d_model â†’ d_ff (í™•ì¥) â†’ í™œì„±í™”(ReLU) â†’ d_ff â†’ d_model (ì¶•ì†Œ)\n",
        "        #    ReLU ëŒ€ì‹  GeLUë¥¼ ì“°ëŠ” ë³€í˜•ë„ ë§ìŒ(BERT/Transformers).\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(d_model, d_ff),  # í™•ì¥\n",
        "            nn.ReLU(),                 # ë¹„ì„ í˜•ì„± (í•™ìŠµí‘œí˜„ë ¥ í–¥ìƒ)\n",
        "            nn.Linear(d_ff, d_model)   # ì›ë˜ ì°¨ì›ìœ¼ë¡œ ì¶•ì†Œ\n",
        "        )\n",
        "\n",
        "        # 3) Layer Normalization (Post-LN)\n",
        "        #    ê° ì„œë¸Œì¸µì˜ ì¶œë ¥ + ì”ì°¨ë¥¼ ë”í•œ ë’¤ ì •ê·œí™”.\n",
        "        #    ì› ë…¼ë¬¸(2017)ì€ Post-LN, ìµœê·¼ì—” Pre-LN(ì„œë¸Œì¸µ ì•ì— LN)ë„ ìì£¼ ì‚¬ìš©ë¨.\n",
        "        self.norm1 = nn.LayerNorm(d_model)  # ì–´í…ì…˜ ì„œë¸Œì¸µ ë’¤\n",
        "        self.norm2 = nn.LayerNorm(d_model)  # FFN ì„œë¸Œì¸µ ë’¤\n",
        "\n",
        "        # 4) Dropout\n",
        "        #    ì„œë¸Œì¸µ ì¶œë ¥ì— ë“œë¡­ì•„ì›ƒì„ ì ìš©í•˜ì—¬ ê³¼ì í•©ì„ ì¤„ì„.\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: ì…ë ¥ ì‹œí€€ìŠ¤ ì„ë² ë”© í…ì„œ (B, T, d_model)\n",
        "        Returns:\n",
        "            (B, T, d_model): ì…ë ¥ê³¼ ë™ì¼í•œ shape. (Residualë¡œ â€œì •ë³´ ë³´ì¡´ + ë³€í™˜â€)\n",
        "        \"\"\"\n",
        "        # --- ì„œë¸Œì¸µ 1: Self-Attention + Residual + LayerNorm ---\n",
        "        # attention_output: (B, T, d_model)\n",
        "        attention_output = self.attention(x)\n",
        "\n",
        "        # ë“œë¡­ì•„ì›ƒ í›„, ì…ë ¥ xì— Residual ì—°ê²°ë¡œ ë”í•¨ (ì •ë³´ ê²½ë¡œ ë³´ì¡´ & ê¸°ìš¸ê¸° íë¦„ ì•ˆì •í™”)\n",
        "        # ê·¸ ë‹¤ìŒ LayerNormìœ¼ë¡œ ë¶„í¬ë¥¼ ì •ê·œí™”í•˜ì—¬ í•™ìŠµì„ ì•ˆì •í™” (Post-LN íŒ¨í„´)\n",
        "        x = self.norm1(x + self.dropout(attention_output))\n",
        "\n",
        "        # --- ì„œë¸Œì¸µ 2: Feed-Forward + Residual + LayerNorm ---\n",
        "        # ffn_output: (B, T, d_model)  â† position-wiseë¡œ ë…ë¦½ì  ì ìš© (Të§ˆë‹¤ ê°™ì€ íŒŒë¼ë¯¸í„°)\n",
        "        ffn_output = self.ffn(x)\n",
        "\n",
        "        # ë‹¤ì‹œ ë“œë¡­ì•„ì›ƒ â†’ Residual â†’ LayerNorm (Post-LN)\n",
        "        x = self.norm2(x + self.dropout(ffn_output))\n",
        "\n",
        "        # ì¶œë ¥ shapeëŠ” ì…ë ¥ê³¼ ë™ì¼ (B, T, d_model)\n",
        "        return x\n",
        "\n",
        "\n",
        "# =========================\n",
        "# Transformer ë¸”ë¡ í…ŒìŠ¤íŠ¸\n",
        "# =========================\n",
        "\n",
        "# í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì • ì˜ˆ:\n",
        "# - d_model=512: í† í° ì„ë² ë”©/ì±„ë„ ìˆ˜\n",
        "# - num_heads=8: 512/8=64 â†’ ê° í—¤ë“œì˜ ì°¨ì› d_k=64 (ì •ìˆ˜ë¡œ ë‚˜ëˆ ë–¨ì–´ì§)\n",
        "# - d_ff=2048: FFN ë‚´ë¶€ í™•ì¥(=4 * d_model) â†’ ì› ë…¼ë¬¸ ê¸°ë³¸ ê¶Œì¥ ì„¤ì •\n",
        "transformer_block = TransformerBlock(d_model=512, num_heads=8, d_ff=2048)\n",
        "\n",
        "# ë”ë¯¸ ì…ë ¥: ë°°ì¹˜ í¬ê¸° B=2, ì‹œí€€ìŠ¤ ê¸¸ì´ T=10, ì±„ë„ C=d_model=512\n",
        "x = torch.randn(2, 10, 512)  # (B, T, C)\n",
        "\n",
        "# ìˆœì „íŒŒ: ì¶œë ¥ë„ (B, T, C) í˜•íƒœë¥¼ ìœ ì§€ (Residual ë•ë¶„ì— ì°¨ì› ë³´ì¡´)\n",
        "output = transformer_block(x)\n",
        "\n",
        "print(f\"Transformer ë¸”ë¡ ì…ë ¥:  {x.shape}\")     # torch.Size([2, 10, 512])\n",
        "print(f\"Transformer ë¸”ë¡ ì¶œë ¥: {output.shape}\") # torch.Size([2, 10, 512])\n",
        "print(\"Transformer ë¸”ë¡ ì²˜ë¦¬ ì™„ë£Œ!\")\n"
      ],
      "metadata": {
        "id": "Ef6XeDwYutOD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "F__Q1Jh7Gaq7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **BERT**\n"
      ],
      "metadata": {
        "id": "RXLkoXSVGdOW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **BERT(Bidirectional Encoder Representations from Transformers)**\n",
        "- 2018ë…„ êµ¬ê¸€ì—ì„œ ë°œí‘œí•œ í˜ì‹ ì ì¸ ìì—°ì–´ì²˜ë¦¬ ëª¨ë¸\n",
        "- ê¸°ì¡´ì˜ ì¼ë°©í–¥ ì–¸ì–´ëª¨ë¸ê³¼ ë‹¬ë¦¬ ì–‘ë°©í–¥ìœ¼ë¡œ ë¬¸ë§¥ì„ ì´í•´í•  ìˆ˜ ìˆì–´ íšê¸°ì ì¸ ì„±ëŠ¥ í–¥ìƒì„ ë³´ì„\n",
        "í•œêµ­ì–´ BERT ëª¨ë¸ë“¤ì€ ì´ëŸ¬í•œ BERT ì•„í‚¤í…ì²˜ë¥¼ í•œêµ­ì–´ì— ë§ê²Œ ì ìš©í•œ ê²ƒìœ¼ë¡œ:\n"
      ],
      "metadata": {
        "id": "PksYoKg3Gu9X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### í•œêµ­ì–´ BERT ëª¨ë¸\n",
        "- 2019ë…„: SKTì—ì„œ KoBERT ê³µê°œ\n",
        "- 2020ë…„: Beomië‹˜ì´ KcBERT ê³µê°œ\n",
        "- 2020ë…„: KETIì—ì„œ KR-BERT ê³µê°œ"
      ],
      "metadata": {
        "id": "9s9HGlq8Gw0o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ì£¼ì˜ì‚¬í•­\n",
        "    - **ë©”ëª¨ë¦¬ ê´€ë¦¬**: ì½”ë©ì—ì„œëŠ” GPU ë©”ëª¨ë¦¬ ì œí•œì´ ìˆìœ¼ë¯€ë¡œ ë°°ì¹˜í¬ê¸° ì¡°ì ˆ í•„ìš”\n",
        "    - **ëª¨ë¸ ì„ íƒ**: KoBERT, KcBERT, KR-BERT ì¤‘ íƒœìŠ¤í¬ì— ë§ëŠ” ëª¨ë¸ ì„ íƒ\n",
        "    - **ë°ì´í„° ì „ì²˜ë¦¬**: í•œêµ­ì–´ íŠ¹ì„±ì„ ê³ ë ¤í•œ ì „ì²˜ë¦¬ (ë„ì–´ì“°ê¸°, íŠ¹ìˆ˜ë¬¸ì ë“±)\n",
        "    - **í‰ê°€ ì§€í‘œ**: ì •í™•ë„ ì™¸ì— F1-score, Precision, Recall ë“± ë‹¤ì–‘í•œ ì§€í‘œ í™œìš©"
      ],
      "metadata": {
        "id": "3j3T10z4H8t6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **ì˜ˆì œ1: ê¸°ë³¸ ì„¤ì¹˜ ë° ì„¤ì •**"
      ],
      "metadata": {
        "id": "QXn9w4rGHDbn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜\n",
        "!pip install transformers torch datasets sentencepiece\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification\n",
        "from transformers import TrainingArguments, Trainer\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# GPU ì‚¬ìš© í™•ì¸\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}\")"
      ],
      "metadata": {
        "id": "7EE82MMmHH2I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **ì˜ˆì œ2: KoBERT ê¸°ë³¸ ì‚¬ìš©ë²•**"
      ],
      "metadata": {
        "id": "YUtoeVayHH9o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# KoBERT ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
        "model_name = \"skt/kobert-base-v1\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModel.from_pretrained(model_name)\n",
        "\n",
        "# í…ìŠ¤íŠ¸ ì¸ì½”ë”© ì˜ˆì œ\n",
        "texts = [\n",
        "    \"ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì •ë§ ì¢‹ë„¤ìš”!\",\n",
        "    \"ì´ ì˜í™”ëŠ” ë„ˆë¬´ ì¬ë¯¸ì—†ì–´ìš”.\",\n",
        "    \"íŒŒì´ì¬ í”„ë¡œê·¸ë˜ë°ì„ ë°°ìš°ê³  ì‹¶ìŠµë‹ˆë‹¤.\"\n",
        "]\n",
        "\n",
        "# í† í°í™” ë° ì¸ì½”ë”©\n",
        "encoded = tokenizer(texts, padding=True, truncation=True, return_tensors='pt')\n",
        "print(\"í† í°í™” ê²°ê³¼:\")\n",
        "for i, text in enumerate(texts):\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    print(f\"'{text}' -> {tokens}\")\n",
        "\n",
        "# ëª¨ë¸ ì¶”ë¡ \n",
        "with torch.no_grad():\n",
        "    outputs = model(**encoded)\n",
        "    last_hidden_states = outputs.last_hidden_state\n",
        "\n",
        "print(f\"\\nì„ë² ë”© ì°¨ì›: {last_hidden_states.shape}\")"
      ],
      "metadata": {
        "id": "kQwlgaAFHIEF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **ì˜ˆì œ3: ê°ì •ë¶„ì„ ì‹¤ìŠµ (íŒŒì¸íŠœë‹)**"
      ],
      "metadata": {
        "id": "3gg5MnWjHIJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ìƒ˜í”Œ ë°ì´í„°ì…‹ ìƒì„± (ì‹¤ì œë¡œëŠ” ë” í° ë°ì´í„°ì…‹ ì‚¬ìš©)\n",
        "data = {\n",
        "    'text': [\n",
        "        \"ì´ ì œí’ˆ ì •ë§ ì¢‹ì•„ìš”! ê°•ì¶”í•©ë‹ˆë‹¤.\",\n",
        "        \"ë°°ì†¡ì´ ë„ˆë¬´ ëŠ¦ì–´ì„œ ì§œì¦ë‚˜ë„¤ìš”.\",\n",
        "        \"ê°€ê²© ëŒ€ë¹„ ê´œì°®ì€ ê²ƒ ê°™ì•„ìš”.\",\n",
        "        \"í’ˆì§ˆì´ ìƒê°ë³´ë‹¤ ì¢‹ì§€ ì•Šë„¤ìš”.\",\n",
        "        \"ì„œë¹„ìŠ¤ê°€ ì¹œì ˆí•˜ê³  ë§Œì¡±ìŠ¤ëŸ¬ì›Œìš”.\",\n",
        "        \"ë‹¤ì‹œëŠ” ì´ìš©í•˜ê³  ì‹¶ì§€ ì•ŠìŠµë‹ˆë‹¤.\",\n",
        "        \"ë³´í†µ ìˆ˜ì¤€ì¸ ê²ƒ ê°™ì•„ìš”.\",\n",
        "        \"ì •ë§ ì‹¤ë§ìŠ¤ëŸ¬ìš´ ê²½í—˜ì´ì—ˆìŠµë‹ˆë‹¤.\"\n",
        "    ],\n",
        "    'label': [1, 0, 1, 0, 1, 0, 1, 0]  # 1: ê¸ì •, 0: ë¶€ì •\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "\n",
        "# ë°ì´í„°ì…‹ í´ë˜ìŠ¤ ì •ì˜\n",
        "class SentimentDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts.iloc[idx])\n",
        "        label = self.labels.iloc[idx]\n",
        "\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=self.max_length,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# ë°ì´í„°ì…‹ ìƒì„±\n",
        "train_dataset = SentimentDataset(\n",
        "    train_df['text'], train_df['label'], tokenizer\n",
        ")\n",
        "val_dataset = SentimentDataset(\n",
        "    val_df['text'], val_df['label'], tokenizer\n",
        ")\n",
        "\n",
        "# ë¶„ë¥˜ ëª¨ë¸ ë¡œë“œ\n",
        "classification_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_name, num_labels=2\n",
        ")\n",
        "\n",
        "# í›ˆë ¨ ì„¤ì •\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        ")\n",
        "\n",
        "# í‰ê°€ í•¨ìˆ˜\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    return {'accuracy': accuracy_score(labels, predictions)}\n",
        "\n",
        "# íŠ¸ë ˆì´ë„ˆ ì„¤ì • ë° í›ˆë ¨\n",
        "trainer = Trainer(\n",
        "    model=classification_model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "# ëª¨ë¸ í›ˆë ¨ (ì£¼ì„ ì²˜ë¦¬ - ì‹¤ì œ ì‚¬ìš©ì‹œ í™œì„±í™”)\n",
        "# trainer.train()\n",
        "\n",
        "# ì˜ˆì¸¡ í•¨ìˆ˜\n",
        "def predict_sentiment(text):\n",
        "    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n",
        "    with torch.no_grad():\n",
        "        outputs = classification_model(**inputs)\n",
        "        predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "        predicted_class = torch.argmax(predictions, dim=-1).item()\n",
        "        confidence = predictions[0][predicted_class].item()\n",
        "\n",
        "    sentiment = \"ê¸ì •\" if predicted_class == 1 else \"ë¶€ì •\"\n",
        "    return sentiment, confidence\n",
        "\n",
        "# í…ŒìŠ¤íŠ¸\n",
        "test_texts = [\n",
        "    \"ì´ ê°•ì˜ëŠ” ì •ë§ ìœ ìµí•˜ê³  ì¬ë¯¸ìˆì–´ìš”!\",\n",
        "    \"ì„¤ëª…ì´ ë„ˆë¬´ ì–´ë ¤ì›Œì„œ ì´í•´í•˜ê¸° í˜ë“¤ì–´ìš”.\",\n",
        "    \"ì ë‹¹í•œ ìˆ˜ì¤€ì¸ ê²ƒ ê°™ìŠµë‹ˆë‹¤.\"\n",
        "]\n",
        "\n",
        "for text in test_texts:\n",
        "    sentiment, confidence = predict_sentiment(text)\n",
        "    print(f\"'{text}' -> {sentiment} (í™•ì‹ ë„: {confidence:.2f})\")"
      ],
      "metadata": {
        "id": "irqRcnc-HIQo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **ì˜ˆì œ4: í•œêµ­ì–´ ë‹¨ì–´ ìœ ì‚¬ë„ ì¸¡ì •**"
      ],
      "metadata": {
        "id": "4J4_MmyFHbS0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def get_word_embedding(word):\n",
        "    \"\"\"ë‹¨ì–´ì˜ ì„ë² ë”© ë²¡í„°ë¥¼ êµ¬í•˜ëŠ” í•¨ìˆ˜\"\"\"\n",
        "    inputs = tokenizer(word, return_tensors='pt')\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        # [CLS] í† í°ì˜ ì„ë² ë”© ì‚¬ìš©\n",
        "        embedding = outputs.last_hidden_state[0, 0, :].numpy()\n",
        "    return embedding\n",
        "\n",
        "# ë‹¨ì–´ë“¤ì˜ ìœ ì‚¬ë„ ì¸¡ì •\n",
        "words = [\"ì‚¬ë‘\", \"ì¢‹ì•„\", \"í–‰ë³µ\", \"ìŠ¬í””\", \"í™”ë‚¨\", \"ê¸°ì¨\"]\n",
        "embeddings = []\n",
        "\n",
        "for word in words:\n",
        "    embedding = get_word_embedding(word)\n",
        "    embeddings.append(embedding)\n",
        "\n",
        "# ìœ ì‚¬ë„ í–‰ë ¬ ê³„ì‚°\n",
        "similarity_matrix = cosine_similarity(embeddings)\n",
        "\n",
        "print(\"ë‹¨ì–´ ìœ ì‚¬ë„ ë§¤íŠ¸ë¦­ìŠ¤:\")\n",
        "print(\"\\t\", \"\\t\".join(words))\n",
        "for i, word in enumerate(words):\n",
        "    similarities = [f\"{sim:.3f}\" for sim in similarity_matrix[i]]\n",
        "    print(f\"{word}\\t\" + \"\\t\".join(similarities))"
      ],
      "metadata": {
        "id": "GGsOXOMpHmXb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **ì˜ˆì œ5: ë¬¸ì¥ ì™„ì„± ê²Œì„**"
      ],
      "metadata": {
        "id": "KN6l87K2HqId"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# ë§ˆìŠ¤í¬ ì–¸ì–´ ëª¨ë¸ íŒŒì´í”„ë¼ì¸ ìƒì„±\n",
        "fill_mask = pipeline(\"fill-mask\", model=model_name, tokenizer=tokenizer)\n",
        "\n",
        "def sentence_completion_game(sentence_with_mask):\n",
        "    \"\"\"ë§ˆìŠ¤í¬ëœ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ê²Œì„\"\"\"\n",
        "    results = fill_mask(sentence_with_mask)\n",
        "\n",
        "    print(f\"ì›ë¬¸: {sentence_with_mask}\")\n",
        "    print(\"ì˜ˆì¸¡ ê²°ê³¼:\")\n",
        "    for i, result in enumerate(results[:5], 1):\n",
        "        filled_sentence = result['sequence']\n",
        "        score = result['score']\n",
        "        print(f\"{i}. {filled_sentence} (í™•ë¥ : {score:.3f})\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "# ê²Œì„ ì˜ˆì œë“¤\n",
        "game_sentences = [\n",
        "    \"ì˜¤ëŠ˜ [MASK]ê°€ ì •ë§ ì¢‹ë„¤ìš”.\",\n",
        "    \"íŒŒì´ì¬ì€ [MASK] í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì…ë‹ˆë‹¤.\",\n",
        "    \"AIëŠ” ë¯¸ë˜ì˜ [MASK]ë¥¼ ë°”ê¿€ ê²ƒì…ë‹ˆë‹¤.\"\n",
        "]\n",
        "\n",
        "for sentence in game_sentences:\n",
        "    sentence_completion_game(sentence)"
      ],
      "metadata": {
        "id": "Lp1WF74PHqRi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **ì˜ˆì œ6: í…ìŠ¤íŠ¸ ë¶„ë¥˜ ë†€ì´**"
      ],
      "metadata": {
        "id": "-jWxlIrnHqZO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def classify_text_topic(text):\n",
        "    \"\"\"í…ìŠ¤íŠ¸ì˜ ì£¼ì œë¥¼ ë¶„ë¥˜í•˜ëŠ” ê°„ë‹¨í•œ ì˜ˆì œ\"\"\"\n",
        "    # í‚¤ì›Œë“œ ê¸°ë°˜ ê°„ë‹¨ ë¶„ë¥˜ (ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ ëª¨ë¸ í•„ìš”)\n",
        "    tech_keywords = [\"í”„ë¡œê·¸ë˜ë°\", \"ì½”ë”©\", \"AI\", \"ì»´í“¨í„°\", \"ì†Œí”„íŠ¸ì›¨ì–´\"]\n",
        "    food_keywords = [\"ìŒì‹\", \"ë§›ìˆ\", \"ìš”ë¦¬\", \"ì‹ë‹¹\", \"ë©”ë‰´\"]\n",
        "    movie_keywords = [\"ì˜í™”\", \"ë°°ìš°\", \"ê°ë…\", \"ìŠ¤í† ë¦¬\", \"ìƒì˜\"]\n",
        "\n",
        "    tech_score = sum(1 for keyword in tech_keywords if keyword in text)\n",
        "    food_score = sum(1 for keyword in food_keywords if keyword in text)\n",
        "    movie_score = sum(1 for keyword in movie_keywords if keyword in text)\n",
        "\n",
        "    scores = {\"ê¸°ìˆ \": tech_score, \"ìŒì‹\": food_score, \"ì˜í™”\": movie_score}\n",
        "    predicted_topic = max(scores, key=scores.get)\n",
        "\n",
        "    return predicted_topic, scores\n",
        "\n",
        "# í…ŒìŠ¤íŠ¸ ë¬¸ì¥ë“¤\n",
        "test_sentences = [\n",
        "    \"íŒŒì´ì¬ìœ¼ë¡œ AI í”„ë¡œê·¸ë˜ë°ì„ ë°°ìš°ê³  ìˆì–´ìš”.\",\n",
        "    \"ì–´ì œ ë¨¹ì€ íŒŒìŠ¤íƒ€ê°€ ì •ë§ ë§›ìˆì—ˆì–´ìš”.\",\n",
        "    \"ì´ë²ˆ ì£¼ë§ì— ê°œë´‰í•œ ì˜í™”ë¥¼ ë³´ëŸ¬ ê°ˆ ì˜ˆì •ì…ë‹ˆë‹¤.\"\n",
        "]\n",
        "\n",
        "print(\"í…ìŠ¤íŠ¸ ì£¼ì œ ë¶„ë¥˜ ê²°ê³¼:\")\n",
        "for text in test_sentences:\n",
        "    topic, scores = classify_text_topic(text)\n",
        "    print(f\"'{text}' -> ì£¼ì œ: {topic}, ì ìˆ˜: {scores}\")"
      ],
      "metadata": {
        "id": "9V72EK28Hqhx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "HTH8dVxiIXxl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **GPT**"
      ],
      "metadata": {
        "id": "FataFDpgIZMS"
      }
    }
  ]
}