{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanghyun-ai/ktcloud_genai/blob/main/%EC%8B%A4%EC%8A%B5%EC%BD%94%EB%93%9C/106_LLM_2010%EB%85%84%EB%8C%80_%EC%9D%B4%ED%9B%84_%EC%96%B8%EC%96%B4AI%ED%9D%90%EB%A6%84_%EC%99%84%EC%84%B1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2010년대 이후 언어 AI흐름**"
      ],
      "metadata": {
        "id": "xAPO58qbDYi8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "NAmo5uhXQ46c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 💡**코드 내용**\n",
        "    - 언어 AI의 2010년 이후의 대략적인 흐름"
      ],
      "metadata": {
        "id": "KHIcfXn8RArA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "K-OX9xcYMp_f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **QuickTour**"
      ],
      "metadata": {
        "id": "CTlhVOfCa3jo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **예제 1: 딥러닝 이전의 방식 - 단어 빈도로 감성 분석하기 (TF-IDF)**\n",
        "\n",
        "- 2010년대 이전, 단어의 출현 빈도를 중요한 특징(Feature)으로 사용했던 고전적인 방식\n",
        "- scikit-learn 라이브러리를 사용\n",
        "- **TfidfVectorizer**를 사용해 사람이 직접 **'단어의 빈도수'라는 특징을 추출** -->  '특징 공학'의 예시\n",
        "- 딥러닝 모델처럼 단어의 의미나 문맥을 스스로 학습하는 것이 아니라 **통계에 기반**한다"
      ],
      "metadata": {
        "id": "1TCZmmIJs4D-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 필요한 라이브러리 설치\n",
        "# pip install scikit-learn\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "print(\"----------- 예제 1: 딥러닝 이전의 고전적인 감성 분석 -----------\")\n",
        "\n",
        "# 훈련 데이터: 영화 리뷰와 긍정(1)/부정(0) 레이블\n",
        "train_text = [\n",
        "    \"이 영화 정말 재미있어요. 배우들 연기도 최고!\",\n",
        "    \"시간 가는 줄 모르고 봤네요. 강력 추천합니다.\",\n",
        "    \"기대했는데 너무 실망했어요. 스토리가 지루해요.\",\n",
        "    \"돈이 아까운 영화. 다시는 안 볼래요.\"\n",
        "]\n",
        "train_labels = [1, 1, 0, 0] # 1: 긍정, 0: 부정\n",
        "\n",
        "# 1. 특징 공학(Feature Engineering): TF-IDF로 문장을 숫자 벡터로 변환\n",
        "# TF-IDF: 단어의 중요도를 계산하는 통계적 방법\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "X_train = tfidf_vectorizer.fit_transform(train_text)\n",
        "\n",
        "# 단어 사전과 변환된 벡터 확인\n",
        "print(\"단어 사전:\", tfidf_vectorizer.get_feature_names_out())\n",
        "print(\"TF-IDF 벡터 (첫 번째 문장):\", X_train[0].toarray())\n",
        "\n",
        "\n",
        "# 2. 모델 학습: 로지스틱 회귀 모델로 학습\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, train_labels)\n",
        "\n",
        "# 3. 새로운 데이터로 예측\n",
        "test_text = [\"배우들 연기가 아쉬웠지만 스토리는 흥미로웠어요.\"]\n",
        "X_test = tfidf_vectorizer.transform(test_text)\n",
        "prediction = model.predict(X_test)\n",
        "\n",
        "print(f\"\\n테스트 문장: '{test_text[0]}'\")\n",
        "print(\"예측 결과:\", \"긍정 😀\" if prediction[0] == 1 else \"부정 😞\")"
      ],
      "metadata": {
        "id": "6S4m0IoMIgDJ",
        "outputId": "6467c71a-38d7-495f-d55c-ab6862bf8154",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------- 예제 1: 딥러닝 이전의 고전적인 감성 분석 -----------\n",
            "단어 사전: ['가는' '강력' '기대했는데' '너무' '다시는' '돈이' '모르고' '배우들' '볼래요' '봤네요' '스토리가' '시간'\n",
            " '실망했어요' '아까운' '연기도' '영화' '재미있어요' '정말' '지루해요' '최고' '추천합니다']\n",
            "TF-IDF 벡터 (첫 번째 문장): [[0.         0.         0.         0.         0.         0.\n",
            "  0.         0.42176478 0.         0.         0.         0.\n",
            "  0.         0.         0.42176478 0.3325242  0.42176478 0.42176478\n",
            "  0.         0.42176478 0.        ]]\n",
            "\n",
            "테스트 문장: '배우들 연기가 아쉬웠지만 스토리는 흥미로웠어요.'\n",
            "예측 결과: 긍정 😀\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **예제 2: 단어의 의미를 벡터로! - 분산 표현 (Word2Vec)**\n",
        "\n",
        "- **gensim** : 문서 유사도 분석 등 여러 NLP 알고리즘을 구현한 라이브러리"
      ],
      "metadata": {
        "id": "vpepkUF0JGEz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 필요한 라이브러리 설치\n",
        "!pip install gensim"
      ],
      "metadata": {
        "id": "YZPkLgNvJcXc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56a3d7dc-ac3e-4008-c0a6-79dae4c47642"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.3.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Collecting numpy<2.0,>=1.18.5 (from gensim)\n",
            "  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
            "  Downloading scipy-1.13.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.3.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open>=1.8.1->gensim) (1.17.3)\n",
            "Downloading gensim-4.3.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.6/26.6 MB\u001b[0m \u001b[31m41.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m69.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.13.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.2/38.2 MB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, scipy, gensim\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.16.2\n",
            "    Uninstalling scipy-1.16.2:\n",
            "      Successfully uninstalled scipy-1.16.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "print(\"\\n\\n----------- 예제 2: Word2Vec으로 단어의 분산 표현 학습하기 -----------\")\n",
        "\n",
        "# Word2Vec 학습을 위한 샘플 문장 (토크나이징 된 형태)\n",
        "sentences = [\n",
        "    ['왕', '남자', '강하다'],\n",
        "    ['여왕', '여자', '아름답다'],\n",
        "    ['남자', '힘', '일'],\n",
        "    ['여자', '지혜', '사랑'],\n",
        "    ['왕', '권력', '지배'],\n",
        "    ['여왕', '권력', '우아함']\n",
        "]\n",
        "\n",
        "# 1. Word2Vec 모델 학습 (분산 표현 학습)\n",
        "# vector_size: 단어를 표현할 벡터의 차원\n",
        "# window: 주변 단어를 몇 개까지 볼 것인지\n",
        "# min_count: 최소 등장 횟수\n",
        "# sg=1: Skip-Gram 방식 사용 (주변 단어 예측)\n",
        "model = Word2Vec(sentences, vector_size=100, window=2, min_count=1, sg=1)\n",
        "print(\"\\n✅ Word2Vec 모델 학습 완료!\")\n",
        "\n",
        "\n",
        "# 2. 학습된 단어 벡터 확인\n",
        "king_vector = model.wv['왕']\n",
        "print(f\"\\n✅ '왕'의 벡터 (일부): {king_vector[:5]}...\")\n",
        "\n",
        "\n",
        "# 3. 단어 간의 유사도 계산\n",
        "# '왕'과 가장 유사한 단어는 무엇일까요?\n",
        "similar_words = model.wv.most_similar('왕')\n",
        "print(\"\\n✅ '왕'과 가장 유사한 단어:\", similar_words)\n",
        "\n",
        "\n",
        "# 4. 재미있는 단어 산술 연산!\n",
        "# '왕' - '남자' + '여자' = ?\n",
        "try:\n",
        "    result = model.wv.most_similar(positive=['여왕', '남자'], negative=['여자'])\n",
        "    print(\"\\n✅ '여왕' + '남자' - '여자' ≈\", result)\n",
        "\n",
        "    result = model.wv.most_similar(positive=['왕', '여자'], negative=['남자'])\n",
        "    print(\"\\n✅ '왕' - '남자' + '여자' ≈\", result)\n",
        "except KeyError as e:\n",
        "    print(f\"\\n✅ 아쉽게도 '{e.args[0]}' 단어가 사전에 없어 계산할 수 없어요. 더 많은 데이터로 학습하면 가능해집니다!\")\n"
      ],
      "metadata": {
        "id": "geQkVMOBIgIq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 필요한 라이브러리 설치\n",
        "# pip install gensim\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "print(\"----------- '여왕'이 반드시 정답으로 나오도록 강제 학습시킨 모델 -----------\")\n",
        "\n",
        "# 1. 데이터 재구성: 관계 강제 학습을 위한 초고반복/단순화 데이터\n",
        "# '남자'와 '여자'의 벡터 차이가 '성별'임을 명확히 암기시키기 위해\n",
        "# 동일한 구조의 문장을 여러 단어 쌍에 걸쳐 극단적으로 반복합니다.\n",
        "\n",
        "force_learning_sentences = [\n",
        "    # --- 핵심 평행 구조 (왕/여왕) 반복 ---\n",
        "    ['왕', '남자', '군주'],\n",
        "    ['여왕', '여자', '군주'],\n",
        "    ['왕', '남자', '왕족'],\n",
        "    ['여왕', '여자', '왕족'],\n",
        "    ['왕', '남자', '리더'],\n",
        "    ['여왕', '여자', '리더'],\n",
        "\n",
        "    # --- 다른 단어 쌍으로 동일한 구조 반복 (성별 관계 일반화) ---\n",
        "    ['왕자', '남자', '후계자'],\n",
        "    ['공주', '여자', '후계자'],\n",
        "    ['아버지', '남자', '부모'],\n",
        "    ['어머니', '여자', '부모'],\n",
        "    ['아들', '남자', '자식'],\n",
        "    ['딸', '여자', '자식'],\n",
        "    ['남편', '남자', '배우자'],\n",
        "    ['아내', '여자', '배우자'],\n",
        "    ['삼촌', '남자', '친척'],\n",
        "    ['이모', '여자', '친척'],\n",
        "    ['남배우', '남자', '배우'],\n",
        "    ['여배우', '여자', '배우'],\n",
        "\n",
        "    # --- 추가적인 문맥을 최소화하여 혼란 방지 ---\n",
        "    ['남자', '강인함'],\n",
        "    ['여자', '섬세함'],\n",
        "    ['왕', '권위'],\n",
        "    ['여왕', '품위']\n",
        "]\n",
        "\n",
        "\n",
        "# 2. 모델 학습 (epochs를 늘려 작은 데이터를 반복 학습)\n",
        "model = Word2Vec(\n",
        "    sentences=force_learning_sentences,\n",
        "    vector_size=50,     # 작은 데이터셋이므로 벡터 차원을 줄여 집중 학습\n",
        "    window=2,           # 주변 단어 범위\n",
        "    min_count=1,        # 최소 단어 빈도\n",
        "    sg=1,               # Skip-Gram 모델 사용\n",
        "    epochs=1000,         # !! 작은 데이터를 500번 반복 학습하여 관계를 각인시킴\n",
        "    seed=42             # 결과 재현을 위한 시드 값 고정\n",
        ")\n",
        "print(\"Word2Vec 모델 학습 완료!\")\n",
        "\n",
        "\n",
        "# 3. 결과 확인: '왕 - 남자 + 여자' 연산\n",
        "try:\n",
        "    # positive: 더할 벡터들, negative: 뺄 벡터\n",
        "    result = model.wv.most_similar(positive=['왕', '여자'], negative=['남자'], topn=3)\n",
        "    print(f\"\\n'왕' - '남자' + '여자' ≈ {result}\")\n",
        "\n",
        "except KeyError as e:\n",
        "    print(f\"\\n계산에 필요한 단어 '{e.args[0]}'가 학습되지 않았습니다.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "SDkRMI_9MODY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **예제 3: End-to-End 모델 수행하기**\n",
        "\n",
        "- 원본 텍스트를 입력하면 바로 결과가 나오는 End-to-End 모델\n",
        "- Hugging Face의 transformers 라이브러리를 사용"
      ],
      "metadata": {
        "id": "swxXfWZNOpm1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 필요한 라이브러리 설치\n",
        "# pip install transformers\n",
        "# pip install torch # 또는 tensorflow\n",
        "\n",
        "from transformers import pipeline\n",
        "\n",
        "print(\"\\n\\n----------- 예제 3: End-to-End 모델로 즉시 감성 분석하기 -----------\")\n",
        "\n",
        "# 1. 미리 학습된 End-to-End 모델 로드\n",
        "# Hugging Face 라이브러리는 수많은 최신 AI 모델을 몇 줄의 코드로 불러올 수 있게 해줍니다.\n",
        "# 모델이 알아서 토크나이징, 임베딩, 추론까지 모든 과정을 처리합니다.\n",
        "sentiment_analyzer = pipeline(\n",
        "    'sentiment-analysis',\n",
        "    model='matthewburke/korean_sentiment' # 한국어 감성 분석에 특화된 모델\n",
        ")\n",
        "\n",
        "# 2. 문장을 입력하고 바로 결과 확인\n",
        "text1 = \"이 강의는 정말 유익하고 미래에 큰 도움이 될 것 같아요!\"\n",
        "text2 = \"내용이 너무 어려워서 하나도 이해가 안돼요...\"\n",
        "\n",
        "results = sentiment_analyzer([text1, text2])\n",
        "\n",
        "# 3. 결과 출력\n",
        "for text, result in zip([text1, text2], results):\n",
        "    label = \"긍정 😀\" if result['label'] == 'positive' else \"부정 😞\"\n",
        "    score = result['score'] * 100\n",
        "    print(f\"\\n문장: '{text}'\")\n",
        "    print(f\"결과: {label} ({score:.2f}% 확률)\")"
      ],
      "metadata": {
        "id": "ogYxl9SZIgSK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "Ej3JWUxwIguu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "0g36MizoMox8"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNkuxQaDMBt5"
      },
      "source": [
        "# **NLP 딥러닝 모델**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vCdKDfUM9Lw"
      },
      "source": [
        "---\n",
        "- 💡 **NOTE**\n",
        "    - NLP(자연어처리) 발전과정에 기여한 딥러닝 모델 소개\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPcm74RoEnQ2"
      },
      "source": [
        "## **RNN(Recurrent Neural Network)**\n",
        "- **메모리를 가진 신경망**\n",
        "-  마치 사람이 책을 읽을 때 앞 문장의 내용을 기억하면서 다음 문장을 이해하는 것과 유사\n",
        "- 역사적 배경\n",
        "    - 1986년: David Rumelhart가 순환 신경망 개념 제안\n",
        "    - 배경: 기존 피드포워드 신경망은 순서가 있는 데이터(시계열, 자연어)를 처리하기 어려웠음\n",
        "    - 혁신점: 시간적 의존성을 모델링할 수 있게 됨\n",
        "- 아키텍쳐 (참고: https://wikidocs.net/22886)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![RNN](https://wikidocs.net/images/page/22886/rnn_image2_ver3.PNG \"RNN\")"
      ],
      "metadata": {
        "id": "zl5uXcJ9QQ8-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 예제 : 스펨메일 분류기\n",
        "\n",
        "- **1.학습용 데이터 전처리**"
      ],
      "metadata": {
        "id": "3ovGt97LQSez"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import urllib.request\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# 스펨메일 데이터셋 다운로드\n",
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/ukairia777/tensorflow-nlp-tutorial/main/10.%20RNN%20Text%20Classification/dataset/spam.csv\", filename=\"spam.csv\")\n",
        "data = pd.read_csv('spam.csv', encoding='latin1')\n",
        "print('총 샘플의 수 :',len(data))\n",
        "\n",
        "# 불필요한 컬럼 삭제\n",
        "del data['Unnamed: 2']\n",
        "del data['Unnamed: 3']\n",
        "del data['Unnamed: 4']\n",
        "data['v1'] = data['v1'].replace(['ham','spam'],[0,1])\n",
        "\n",
        "\n",
        "# 데이터 확인\n",
        "print('결측값 여부 :',data.isnull().values.any())\n",
        "print('v2열의 유니크한 값 :',data['v2'].nunique())\n",
        "\n",
        "# v2 열에서 중복인 내용이 있다면 중복 제거\n",
        "data.drop_duplicates(subset=['v2'], inplace=True)\n",
        "print('총 샘플의 수 :',len(data))\n",
        "print('정상 메일과 스팸 메일의 개수')\n",
        "print(data.groupby('v1').size().reset_index(name='count'))\n",
        "print('-' * 30)\n",
        "print(f'정상 메일의 비율 = {round(data[\"v1\"].value_counts()[0]/len(data) * 100,3)}%')\n",
        "print(f'스팸 메일의 비율 = {round(data[\"v1\"].value_counts()[1]/len(data) * 100,3)}%')\n",
        "\n",
        "\n",
        "# 훈련 데이터 & 테스트 데이터 분리\n",
        "X_data = data['v2']\n",
        "y_data = data['v1']\n",
        "print('메일 본문의 개수: {}'.format(len(X_data)))\n",
        "print('레이블의 개수: {}'.format(len(y_data)))\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data,\n",
        "                                                    test_size=0.2, random_state=0, stratify=y_data)\n",
        "\n",
        "print('--------훈련 데이터의 비율-----------')\n",
        "print(f'정상 메일 = {round(y_train.value_counts()[0]/len(y_train) * 100,3)}%')\n",
        "print(f'스팸 메일 = {round(y_train.value_counts()[1]/len(y_train) * 100,3)}%')\n",
        "\n",
        "print('--------테스트 데이터의 비율-----------')\n",
        "print(f'정상 메일 = {round(y_test.value_counts()[0]/len(y_test) * 100,3)}%')\n",
        "print(f'스팸 메일 = {round(y_test.value_counts()[1]/len(y_test) * 100,3)}%')"
      ],
      "metadata": {
        "id": "u90YliAPQRMp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 문장 토큰화\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "X_train_encoded = tokenizer.texts_to_sequences(X_train)\n",
        "print('메일의 토큰화 결과(5개):\\n', X_train_encoded[:5])\n",
        "print()\n",
        "word_to_index = tokenizer.word_index\n",
        "print('토큰에 부여된 정수:\\n', word_to_index)\n",
        "print()\n",
        "vocab_size = len(word_to_index) + 1\n",
        "print('단어 집합의 크기: {}'.format((vocab_size)))\n",
        "print()\n",
        "print('각 단어에 대한 등장 빈도수: ', tokenizer.word_counts.items())\n",
        "\n",
        "\n",
        "# 토큰화된 단어 분석\n",
        "threshold = 2\n",
        "total_cnt = len(word_to_index) # 단어의 수\n",
        "rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
        "total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
        "rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n",
        "\n",
        "# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n",
        "for key, value in tokenizer.word_counts.items():\n",
        "    total_freq = total_freq + value\n",
        "\n",
        "    # 단어의 등장 빈도수가 threshold보다 작으면\n",
        "    if(value < threshold):\n",
        "        rare_cnt = rare_cnt + 1\n",
        "        rare_freq = rare_freq + value\n",
        "\n",
        "print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n",
        "print(\"단어 집합(vocabulary)에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n",
        "print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)\n",
        "print('메일의 최대 길이 : %d' % max(len(sample) for sample in X_train_encoded))\n",
        "print('메일의 평균 길이 : %f' % (sum(map(len, X_train_encoded))/len(X_train_encoded)))\n",
        "plt.hist([len(sample) for sample in X_data], bins=50)\n",
        "plt.xlabel('length of samples')\n",
        "plt.ylabel('number of samples')\n",
        "plt.show()\n",
        "# 가장 긴 메일의 길이는 189이며, 전체 데이터의 길이 분포는 대체적으로 약 50이하의 길이를 가집니다\n",
        "\n",
        "\n",
        "max_len = 189\n",
        "X_train_padded = pad_sequences(X_train_encoded, maxlen = max_len)\n",
        "print(\"훈련 데이터의 크기(shape):\", X_train_padded.shape)\n",
        "# 189보다 길이가 짧은 메일 샘플은 전부 숫자 0이 패딩되어 189의 길이를 가지도록 만듦"
      ],
      "metadata": {
        "id": "9uUxhOYBSlcr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **2.RNN으로 스팸 메일 분류하기**"
      ],
      "metadata": {
        "id": "t24aQmxFQZ8G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import SimpleRNN, Embedding, Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "\n",
        "# 모델 구성\n",
        "embedding_dim = 32\n",
        "hidden_units = 32\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, embedding_dim))\n",
        "model.add(SimpleRNN(hidden_units))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
        "history = model.fit(X_train_padded, y_train, epochs=4, batch_size=64, validation_split=0.2)\n",
        "\n",
        "\n",
        "# 테스트 데이터 전처리\n",
        "X_test_encoded = tokenizer.texts_to_sequences(X_test)\n",
        "X_test_padded = pad_sequences(X_test_encoded, maxlen = max_len)\n",
        "print(\"\\n테스트 정확도: %.4f\" % (model.evaluate(X_test_padded, y_test)[1]))\n"
      ],
      "metadata": {
        "id": "c9hiTkxtQcpi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========== 핵심 예측 및 비교 코드 ==========\n",
        "from sklearn.metrics import classification_report\n",
        "import numpy as np\n",
        "\n",
        "# 1. 예측값 생성\n",
        "y_pred_proba = model.predict(X_test_padded)  # 확률값 (0~1)\n",
        "y_pred = (y_pred_proba > 0.5).astype(int).flatten()  # 이진 분류값 (0 or 1)\n",
        "\n",
        "# 2. 결과 비교\n",
        "print(\"\\n=== 예측 결과 ===\")\n",
        "print(f\"실제값: {y_test[:10]}\")\n",
        "print(f\"예측값: {y_pred[:10]}\")\n",
        "print(f\"예측 확률: {y_pred_proba[:10].flatten()}\")\n",
        "\n",
        "# 3. 성능 지표\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "print(f\"\\n정확도: {accuracy:.4f}\")\n",
        "print(f\"혼동 행렬:\\n{cm}\")\n",
        "print(\"\\n분류 리포트:\")\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "pOC_v37vW5vb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========== 새로운 메일 테스트 코드 ==========\n",
        "def predict_spam(text_list, model, tokenizer, max_len=189):\n",
        "    \"\"\"\n",
        "    새로운 메일 텍스트들이 스팸인지 예측하는 함수\n",
        "\n",
        "    Parameters:\n",
        "    - text_list: 예측할 메일 텍스트들의 리스트\n",
        "    - model: 학습된 RNN 모델\n",
        "    - tokenizer: 학습에 사용된 토크나이저\n",
        "    - max_len: 패딩할 최대 길이\n",
        "\n",
        "    Returns:\n",
        "    - predictions: 각 텍스트에 대한 예측 결과\n",
        "    \"\"\"\n",
        "    # 텍스트를 시퀀스로 변환\n",
        "    sequences = tokenizer.texts_to_sequences(text_list)\n",
        "    # 패딩 적용\n",
        "    padded = pad_sequences(sequences, maxlen=max_len)\n",
        "    # 예측 수행\n",
        "    predictions = model.predict(padded)\n",
        "\n",
        "    return predictions\n",
        "\n",
        "# 테스트할 새로운 메일 예시 3개\n",
        "test_emails = [\n",
        "    # 예시 1: 명백한 스팸 메일\n",
        "    \"URGENT! You have won $1000000! Click here now to claim your prize! Limited time offer! Call now!\",\n",
        "\n",
        "    # 예시 2: 정상 메일\n",
        "    \"Hi John, hope you're doing well. Let's meet for coffee tomorrow at 3pm. Looking forward to catching up!\",\n",
        "\n",
        "    # 예시 3: 애매한 경계선 메일 (프로모션성이지만 스팸은 아닐 수 있음)\n",
        "    \"Special discount on our premium products. Save 30% this week only. Visit our website for more details.\"\n",
        "]\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"새로운 메일 스팸 분류 테스트\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "\n",
        "# 예측 수행\n",
        "predictions = predict_spam(test_emails, model, tokenizer, max_len)\n",
        "\n",
        "# 결과 출력\n",
        "for i, (email, pred) in enumerate(zip(test_emails, predictions)):\n",
        "    prob = pred[0]  # 확률값 추출\n",
        "    is_spam = \"스팸 메일\" if prob > 0.5 else \"정상 메일\"\n",
        "    confidence = prob if prob > 0.5 else 1 - prob\n",
        "\n",
        "    print(f\"\\n📧 테스트 메일 {i+1}:\")\n",
        "    print(f\"내용: {email}\")\n",
        "    print(f\"예측 결과: {is_spam}\")\n",
        "    print(f\"스팸 확률: {prob:.4f}\")\n",
        "    print(f\"신뢰도: {confidence:.4f} ({confidence*100:.1f}%)\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "# ========== 대화형 테스트 함수 ==========\n",
        "def interactive_spam_test():\n",
        "    \"\"\"\n",
        "    사용자가 직접 메일 내용을 입력하여 테스트할 수 있는 함수\n",
        "    \"\"\"\n",
        "    print(\"\\n🔍 직접 메일 테스트해보기\")\n",
        "    print(\"메일 내용을 입력하세요 (종료하려면 'quit' 입력):\")\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"\\n메일 내용: \")\n",
        "\n",
        "        if user_input.lower() == 'quit':\n",
        "            print(\"테스트를 종료합니다.\")\n",
        "            break\n",
        "\n",
        "        if user_input.strip() == '':\n",
        "            print(\"메일 내용을 입력해주세요.\")\n",
        "            continue\n",
        "\n",
        "        # 예측 수행\n",
        "        prediction = predict_spam([user_input], model, tokenizer, max_len)\n",
        "        prob = prediction[0][0]\n",
        "        is_spam = \"스팸 메일\" if prob > 0.5 else \"정상 메일\"\n",
        "        confidence = prob if prob > 0.5 else 1 - prob\n",
        "\n",
        "        print(f\"📊 분석 결과:\")\n",
        "        print(f\"   예측: {is_spam}\")\n",
        "        print(f\"   스팸 확률: {prob:.4f}\")\n",
        "        print(f\"   신뢰도: {confidence:.4f} ({confidence*100:.1f}%)\")\n",
        "\n",
        "# 대화형 테스트 실행 (주석 해제하여 사용)\n",
        "# interactive_spam_test()\n",
        "\n",
        "# ========== 추가 분석: 단어 중요도 확인 ==========\n",
        "def analyze_email_tokens(email_text, tokenizer, max_len=189):\n",
        "    \"\"\"\n",
        "    이메일의 토큰화 과정을 분석하는 함수\n",
        "    \"\"\"\n",
        "    print(f\"\\n🔍 '{email_text[:50]}...' 분석\")\n",
        "\n",
        "    # 토큰화\n",
        "    sequence = tokenizer.texts_to_sequences([email_text])\n",
        "    padded = pad_sequences(sequence, maxlen=max_len)\n",
        "\n",
        "    # 원본 단어들 확인\n",
        "    words = email_text.split()\n",
        "    print(f\"원본 단어들: {words}\")\n",
        "\n",
        "    # 토큰화된 결과\n",
        "    tokens = sequence[0]\n",
        "    print(f\"토큰화 결과: {tokens}\")\n",
        "\n",
        "    # 역토큰화 (숫자 -> 단어)\n",
        "    reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n",
        "    decoded_words = [reverse_word_map.get(token, '<UNK>') for token in tokens]\n",
        "    print(f\"토큰 -> 단어: {decoded_words}\")\n",
        "\n",
        "    return padded\n",
        "\n",
        "# 각 테스트 메일에 대해 토큰 분석\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"토큰화 분석\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for i, email in enumerate(test_emails):\n",
        "    analyze_email_tokens(email, tokenizer, max_len)\n",
        "    print(\"-\" * 40)"
      ],
      "metadata": {
        "id": "4Uhhkh1-j7kh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **LSTM(Long Short-Term Memory)**\n",
        "\n",
        "- LSTM은 **장단기 메모리 네트워크**로, RNN(순환신경망)의 한 종류\n",
        "- 기존 RNN의 기울기 소실 문제, **장기 의존성 문제(the problem of Long-Term Dependencies)를 해결하기 위해 개발된 고급 신경망 구조**\n",
        "- LSTM은 은닉층의 메모리 셀에 입력 게이트, 망각 게이트, 출력 게이트를 추가하여 불필요한 기억을 지우고, 기억해야할 것들을 정함\n",
        "- - 아키텍쳐 (참고: https://wikidocs.net/22888)"
      ],
      "metadata": {
        "id": "3VWAwcAVlSjv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![RNN](https://wikidocs.net/images/page/22888/vanilla_rnn_ver2.PNG \"RNN\")\n"
      ],
      "metadata": {
        "id": "kG2aAdlcp0Ap"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![LSTM](https://wikidocs.net/images/page/22888/vaniila_rnn_and_different_lstm_ver2.PNG \"LSTM\")"
      ],
      "metadata": {
        "id": "e_MDGDGyp1Zl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 예제: **한글 스팸메일 분류기**"
      ],
      "metadata": {
        "id": "x8r9nH0VnhES"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import re\n",
        "import pickle\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# 한글 텍스트 전처리 함수\n",
        "def preprocess_korean_text(text):\n",
        "    \"\"\"\n",
        "    한글 텍스트 전처리 함수\n",
        "    - 특수문자 제거\n",
        "    - 불필요한 공백 제거\n",
        "    - 소문자 변환은 한글에서 불필요\n",
        "    \"\"\"\n",
        "    # 한글, 영문, 숫자, 공백만 남기기\n",
        "    text = re.sub(r'[^가-힣a-zA-Z0-9\\s]', ' ', text)\n",
        "    # 연속된 공백을 하나로 변경\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    # 앞뒤 공백 제거\n",
        "    text = text.strip()\n",
        "    return text\n",
        "\n",
        "# 한글 스팸/정상 메일 모의 데이터셋 생성\n",
        "def create_korean_email_dataset():\n",
        "    \"\"\"\n",
        "    한글 스팸/정상 메일 모의 데이터셋 생성\n",
        "    실제 프로젝트에서는 공개 데이터셋이나 수집된 데이터를 사용\n",
        "    \"\"\"\n",
        "\n",
        "    # 정상 메일 예시 (500개 생성을 위한 기본 패턴)\n",
        "    normal_patterns = [\n",
        "        \"안녕하세요 회의 일정을 알려드립니다\",\n",
        "        \"프로젝트 진행 상황 보고드립니다\",\n",
        "        \"첨부파일 확인 부탁드립니다\",\n",
        "        \"내일 오후 미팅 참석 가능하신가요\",\n",
        "        \"업무 관련 문의사항이 있습니다\",\n",
        "        \"보고서 작성 완료했습니다\",\n",
        "        \"점심시간에 잠깐 이야기할 수 있을까요\",\n",
        "        \"고객사 미팅 결과 공유드립니다\",\n",
        "        \"다음 주 일정 조율 부탁드립니다\",\n",
        "        \"교육 자료 전달드립니다\",\n",
        "        \"회의실 예약 확인 부탁드립니다\",\n",
        "        \"출장 계획서 검토 요청드립니다\",\n",
        "        \"월간 성과 보고서 제출합니다\",\n",
        "        \"팀 빌딩 행사 참석 의사 확인\",\n",
        "        \"새 프로젝트 제안서 검토 바랍니다\"\n",
        "    ]\n",
        "\n",
        "    # 스팸 메일 예시 (500개 생성을 위한 기본 패턴)\n",
        "    spam_patterns = [\n",
        "        \"축하합니다 1억원 당첨되셨습니다 지금 확인하세요\",\n",
        "        \"긴급 대출 가능 무심사 당일 승인\",\n",
        "        \"클릭만으로 월 300만원 수익 보장\",\n",
        "        \"무료 상품권 증정 지금 신청하세요\",\n",
        "        \"다이어트 보조제 특가 판매 효과 100퍼센트\",\n",
        "        \"투자 권유 고수익 보장 위험 부담 없음\",\n",
        "        \"성인용품 할인 판매 비밀 배송\",\n",
        "        \"카지노 게임 첫 가입 보너스 지급\",\n",
        "        \"주식 정보 제공 수익률 200퍼센트\",\n",
        "        \"아르바이트 모집 하루 10만원 보장\",\n",
        "        \"온라인 도박 사이트 가입 즉시 보너스\",\n",
        "        \"신용카드 현금 서비스 즉시 승인\",\n",
        "        \"불법 복제품 판매 정품 보장\",\n",
        "        \"피라미드 판매 조직 가입 권유\",\n",
        "        \"가상화폐 투자 사기 의혹 상품\"\n",
        "    ]\n",
        "\n",
        "    # 데이터셋 확장 생성\n",
        "    emails = []\n",
        "    labels = []\n",
        "\n",
        "    # 정상 메일 500개 생성\n",
        "    for i in range(500):\n",
        "        base_pattern = normal_patterns[i % len(normal_patterns)]\n",
        "        # 패턴에 변화를 주어 다양성 증가\n",
        "        variations = [\n",
        "            base_pattern,\n",
        "            base_pattern + f\" {i+1}번째 건입니다\",\n",
        "            base_pattern + \" 확인 후 회신 바랍니다\",\n",
        "            base_pattern + \" 관련하여 논의가 필요합니다\",\n",
        "            \"업무: \" + base_pattern\n",
        "        ]\n",
        "        emails.append(variations[i % len(variations)])\n",
        "        labels.append(0)  # 정상 메일\n",
        "\n",
        "    # 스팸 메일 500개 생성\n",
        "    for i in range(500):\n",
        "        base_pattern = spam_patterns[i % len(spam_patterns)]\n",
        "        # 스팸의 특징을 강화한 변화\n",
        "        variations = [\n",
        "            base_pattern,\n",
        "            \"🎉\" + base_pattern + \"🎉\",\n",
        "            base_pattern + \" 지금 즉시 클릭하세요!\",\n",
        "            \"★★★ \" + base_pattern + \" ★★★\",\n",
        "            base_pattern + \" 놓치면 후회합니다!\"\n",
        "        ]\n",
        "        emails.append(variations[i % len(variations)])\n",
        "        labels.append(1)  # 스팸 메일\n",
        "\n",
        "    return emails, labels\n",
        "\n",
        "# 데이터셋 생성 및 로드\n",
        "print(\"=\"*60)\n",
        "print(\"한글 스팸 메일 분류기 생성 중...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "emails, labels = create_korean_email_dataset()\n",
        "\n",
        "# 데이터프레임 생성\n",
        "df = pd.DataFrame({\n",
        "    'email': emails,\n",
        "    'label': labels\n",
        "})\n",
        "\n",
        "# numpy 배열로 변환하여 안정성 향상\n",
        "labels_array = np.array(labels)\n",
        "\n",
        "print(f\"총 데이터 수: {len(df)}\")\n",
        "print(f\"정상 메일 수: {np.sum(labels_array == 0)}\")\n",
        "print(f\"스팸 메일 수: {np.sum(labels_array == 1)}\")\n",
        "print(f\"정상 메일 비율: {np.sum(labels_array == 0)/len(labels_array)*100:.1f}%\")\n",
        "print(f\"스팸 메일 비율: {np.sum(labels_array == 1)/len(labels_array)*100:.1f}%\")\n",
        "\n",
        "# 데이터 전처리\n",
        "df['email_processed'] = df['email'].apply(preprocess_korean_text)\n",
        "\n",
        "# 데이터 분할\n",
        "X = df['email_processed'].values\n",
        "y = df['label'].values\n",
        "\n",
        "# 데이터 타입 확인 및 변환\n",
        "X = np.array(X, dtype=str)\n",
        "y = np.array(y, dtype=int)\n",
        "\n",
        "print(f\"데이터 타입 - X: {X.dtype}, y: {y.dtype}\")\n",
        "print(f\"데이터 형태 - X: {X.shape}, y: {y.shape}\")\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(f\"\\n훈련 데이터: {len(X_train)}\")\n",
        "print(f\"테스트 데이터: {len(X_test)}\")\n",
        "\n",
        "# 토크나이저 설정 (한글 특성 고려)\n",
        "MAX_FEATURES = 10000  # 어휘 크기\n",
        "MAX_LEN = 100  # 최대 시퀀스 길이\n",
        "\n",
        "tokenizer = Tokenizer(\n",
        "    num_words=MAX_FEATURES,\n",
        "    oov_token='<OOV>',  # Out-of-vocabulary 토큰\n",
        "    filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'  # 제거할 문자\n",
        ")\n",
        "\n",
        "# 훈련 데이터로 토크나이저 학습\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "# 텍스트를 시퀀스로 변환\n",
        "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
        "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "# 패딩 적용\n",
        "X_train_pad = pad_sequences(X_train_seq, maxlen=MAX_LEN, padding='post')\n",
        "X_test_pad = pad_sequences(X_test_seq, maxlen=MAX_LEN, padding='post')\n",
        "\n",
        "print(f\"\\n어휘 크기: {len(tokenizer.word_index)}\")\n",
        "print(f\"시퀀스 최대 길이: {MAX_LEN}\")\n",
        "\n",
        "# 개선된 모델 구축 (Bidirectional LSTM 사용)\n",
        "def create_enhanced_model():\n",
        "    model = Sequential([\n",
        "        Embedding(input_dim=MAX_FEATURES,\n",
        "                 output_dim=128,\n",
        "                 input_length=MAX_LEN,\n",
        "                 name='embedding'),\n",
        "        Bidirectional(LSTM(64, dropout=0.3, recurrent_dropout=0.3), name='bilstm'),\n",
        "        Dense(32, activation='relu', name='dense1'),\n",
        "        Dropout(0.5, name='dropout'),\n",
        "        Dense(1, activation='sigmoid', name='output')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# 모델 생성 및 컴파일\n",
        "model = create_enhanced_model()\n",
        "model.compile(\n",
        "    optimizer=Adam(learning_rate=0.001),\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "print(\"\\n모델 구조:\")\n",
        "model.summary()\n",
        "\n",
        "# 콜백 설정\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=3,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(\n",
        "    monitor='val_loss',\n",
        "    factor=0.5,\n",
        "    patience=2,\n",
        "    min_lr=0.0001\n",
        ")\n",
        "\n",
        "# 모델 훈련\n",
        "print(\"\\n모델 훈련 시작...\")\n",
        "history = model.fit(\n",
        "    X_train_pad, y_train,\n",
        "    epochs=10,\n",
        "    batch_size=32,\n",
        "    validation_split=0.2,\n",
        "    callbacks=[early_stopping, reduce_lr],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# 테스트 성능 평가\n",
        "test_loss, test_accuracy = model.evaluate(X_test_pad, y_test, verbose=0)\n",
        "print(f\"\\n테스트 정확도: {test_accuracy:.4f}\")\n",
        "\n",
        "# 예측 및 상세 평가\n",
        "y_pred_proba = model.predict(X_test_pad)\n",
        "y_pred = (y_pred_proba > 0.5).astype(int).flatten()\n",
        "\n",
        "print(\"\\n분류 리포트:\")\n",
        "print(classification_report(y_test, y_pred, target_names=['정상', '스팸']))\n",
        "\n",
        "print(\"\\n혼동 행렬:\")\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(cm)\n",
        "\n",
        "# 스팸 분류 예측 함수\n",
        "def predict_spam_korean(text, model, tokenizer, max_len=MAX_LEN):\n",
        "    \"\"\"\n",
        "    한글 텍스트가 스팸인지 예측하는 함수\n",
        "    \"\"\"\n",
        "    # 전처리\n",
        "    processed_text = preprocess_korean_text(text)\n",
        "\n",
        "    # 토크나이징 및 패딩\n",
        "    sequence = tokenizer.texts_to_sequences([processed_text])\n",
        "    padded = pad_sequences(sequence, maxlen=max_len, padding='post')\n",
        "\n",
        "    # 예측\n",
        "    prediction = model.predict(padded, verbose=0)[0][0]\n",
        "\n",
        "    return prediction\n",
        "\n",
        "# 테스트용 한글 메일 예시\n",
        "test_korean_emails = [\n",
        "    \"축하합니다! 복권에 당첨되셨습니다. 1억원을 받으시려면 지금 즉시 링크를 클릭하세요!\",\n",
        "    \"내일 오후 3시 회의실에서 프로젝트 진행 상황을 논의하겠습니다. 참석 부탁드립니다.\",\n",
        "    \"무료 다이어트 약품 증정! 효과 100% 보장! 지금 신청하면 특별 할인!\",\n",
        "    \"보고서 작성이 완료되었습니다. 첨부파일을 확인해주시기 바랍니다.\",\n",
        "    \"긴급! 대출 승인 완료! 무심사 당일 지급! 연락 바랍니다!\",\n",
        "    \"교육 자료를 공유드립니다. 검토 후 의견 부탁드립니다.\"\n",
        "]\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"한글 메일 스팸 분류 테스트\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for i, email in enumerate(test_korean_emails, 1):\n",
        "    prob = predict_spam_korean(email, model, tokenizer)\n",
        "    is_spam = \"스팸 메일\" if prob > 0.5 else \"정상 메일\"\n",
        "    confidence = prob if prob > 0.5 else 1 - prob\n",
        "\n",
        "    print(f\"\\n📧 테스트 {i}:\")\n",
        "    print(f\"내용: {email}\")\n",
        "    print(f\"예측: {is_spam}\")\n",
        "    print(f\"스팸 확률: {prob:.4f}\")\n",
        "    print(f\"신뢰도: {confidence:.4f} ({confidence*100:.1f}%)\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "# 대화형 테스트 함수\n",
        "def interactive_korean_spam_test():\n",
        "    \"\"\"\n",
        "    사용자가 직접 한글 메일을 입력하여 테스트하는 함수\n",
        "    \"\"\"\n",
        "    print(\"\\n🔍 한글 메일 스팸 분류 테스트\")\n",
        "    print(\"메일 내용을 입력하세요 (종료: 'quit'):\")\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"\\n✉️ 메일 내용: \")\n",
        "\n",
        "        if user_input.lower() in ['quit', '종료', 'q']:\n",
        "            print(\"테스트를 종료합니다.\")\n",
        "            break\n",
        "\n",
        "        if user_input.strip() == '':\n",
        "            print(\"메일 내용을 입력해주세요.\")\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            prob = predict_spam_korean(user_input, model, tokenizer)\n",
        "            is_spam = \"스팸 메일\" if prob > 0.5 else \"정상 메일\"\n",
        "            confidence = prob if prob > 0.5 else 1 - prob\n",
        "\n",
        "            print(f\"\\n📊 분석 결과:\")\n",
        "            print(f\"   🏷️  분류: {is_spam}\")\n",
        "            print(f\"   📈 스팸 확률: {prob:.4f}\")\n",
        "            print(f\"   ✅ 신뢰도: {confidence:.4f} ({confidence*100:.1f}%)\")\n",
        "\n",
        "            # 스팸일 가능성이 높은 경우 경고 표시\n",
        "            if prob > 0.8:\n",
        "                print(f\"   ⚠️  주의: 스팸일 가능성이 매우 높습니다!\")\n",
        "            elif prob < 0.2:\n",
        "                print(f\"   ✅ 안전: 정상 메일일 가능성이 매우 높습니다.\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"오류가 발생했습니다: {e}\")\n",
        "\n",
        "# 모델과 토크나이저 저장 함수\n",
        "def save_model_and_tokenizer(model, tokenizer, model_path='korean_spam_model.h5', tokenizer_path='korean_tokenizer.pickle'):\n",
        "    \"\"\"\n",
        "    학습된 모델과 토크나이저를 저장하는 함수\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # 모델 저장\n",
        "        model.save(model_path)\n",
        "        print(f\"✅ 모델이 {model_path}에 저장되었습니다.\")\n",
        "\n",
        "        # 토크나이저 저장\n",
        "        with open(tokenizer_path, 'wb') as handle:\n",
        "            pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "        print(f\"✅ 토크나이저가 {tokenizer_path}에 저장되었습니다.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ 저장 중 오류 발생: {e}\")\n",
        "        print(\"현재 세션에서만 모델을 사용하세요.\")\n",
        "\n",
        "# 모델 및 토크나이저 저장 (선택사항)\n",
        "try:\n",
        "    save_model_and_tokenizer(model, tokenizer)\n",
        "except:\n",
        "    print(\"⚠️ 파일 저장을 건너뜁니다. 현재 세션에서만 사용 가능합니다.\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"한글 스팸 메일 분류기 완성!\")\n",
        "print(\"아래 함수를 실행하여 직접 테스트해보세요:\")\n",
        "print(\"interactive_korean_spam_test()\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# 대화형 테스트 실행 (주석 해제하여 사용)\n",
        "# interactive_korean_spam_test()"
      ],
      "metadata": {
        "id": "7LUl3Od6lR34"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 대화형 테스트 실행 (주석 해제하여 사용)\n",
        "interactive_korean_spam_test()"
      ],
      "metadata": {
        "id": "A5Zmt7yLme8g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Seq2Seq (시퀀스-투-시퀀스(Sequence-to-Sequence, seq2seq)\n",
        "\n",
        "- **입력 시퀀스를 받아서 다른 시퀀스로 변환하는 모델**\n",
        "- 마치 번역기처럼 한 언어의 문장을 다른 언어로 바꾸는 것과 같음\n",
        "- 역사적 배경\n",
        "    - 2014년: Google이 기계번역용으로 제안\n",
        "    - 배경: 기존 방법들은 고정 길이 입력만 처리 가능했음\n",
        "    - 혁신: 가변 길이 입력을 가변 길이 출력으로 변환 가능\n",
        "- 기본 구조\n",
        "\n",
        "|구성 요소|역할|비유|\n",
        "|---|---|---|\n",
        "|Encoder|입력 시퀀스를 이해하고 압축|책을 읽고 요약하는 사람|\n",
        "|Decoder|압축된 정보로 출력 시퀀스 생성|요약본을 보고 다른 언어로 쓰는 사람|\n",
        "|Context Vector|인코더와 디코더를 연결하는 정보|두 사람 사이의 메모|\n"
      ],
      "metadata": {
        "id": "nvxSncXMywPP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 작동 과정\n",
        "    - 입력: \"I love you\"\n",
        "    -        ↓ (Encoder)\n",
        "    - Context Vector (압축된 의미)\n",
        "    -        ↓ (Decoder)  \n",
        "    - 출력: \"나는 너를 사랑해\""
      ],
      "metadata": {
        "id": "asTdfE8bzp2q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 아키텍처 (참고 https://wikidocs.net/24996)\n"
      ],
      "metadata": {
        "id": "8Q3Wp85W0PDG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Seq2seq](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdna%2FLUwms%2FbtszM0Eg9wB%2FAAAAAAAAAAAAAAAAAAAAAHUSsygDhXBD9OsONcPC84p1qhBQHqdlxVNImS4aFdRi%2Fimg.jpg%3Fcredential%3DyqXZFxpELC7KVnFOS48ylbz2pIh7yKj8%26expires%3D1759244399%26allow_ip%3D%26allow_referer%3D%26signature%3DhFLLizlqkuD3zD3pRxfQeu4%252Be7g%253D \"Seq2seq\")"
      ],
      "metadata": {
        "id": "uuCGQ9vt8ol_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 예제1 : 기본 Seq2Seq 구조"
      ],
      "metadata": {
        "id": "N5J3KSgJ2CRp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.lstm = nn.LSTM(embed_size, hidden_size, batch_first=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        output, (hidden, cell) = self.lstm(embedded)\n",
        "        return hidden, cell  # 마지막 상태만 반환\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.lstm = nn.LSTM(embed_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, x, hidden, cell):\n",
        "        embedded = self.embedding(x)\n",
        "        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n",
        "        prediction = self.fc(output)\n",
        "        return prediction, hidden, cell\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def forward(self, source, target):\n",
        "        # 인코딩: 입력 시퀀스를 context vector로 압축\n",
        "        hidden, cell = self.encoder(source)\n",
        "\n",
        "        # 디코딩: context vector로부터 출력 시퀀스 생성\n",
        "        outputs = []\n",
        "        input_token = target[:, 0:1]  # 시작 토큰\n",
        "\n",
        "        for i in range(1, target.size(1)):\n",
        "            output, hidden, cell = self.decoder(input_token, hidden, cell)\n",
        "            outputs.append(output)\n",
        "            input_token = target[:, i:i+1]  # 다음 입력\n",
        "\n",
        "        return torch.cat(outputs, dim=1)\n",
        "\n",
        "# 모델 생성\n",
        "vocab_size = 1000\n",
        "embed_size = 256\n",
        "hidden_size = 512\n",
        "\n",
        "encoder = Encoder(vocab_size, embed_size, hidden_size)\n",
        "decoder = Decoder(vocab_size, embed_size, hidden_size)\n",
        "model = Seq2Seq(encoder, decoder)\n",
        "\n",
        "print(\"Seq2Seq 모델 생성 완료!\")\n",
        "print(f\"Encoder: {encoder}\")\n",
        "print(f\"Decoder: {decoder}\")\n"
      ],
      "metadata": {
        "id": "nwb6s0XU2E2q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mNIgtqgEnQ8"
      },
      "source": [
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Attention 메커니즘**"
      ],
      "metadata": {
        "id": "6RRXAC1HeE0p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **어떤 부분에 집중할지 결정하는 메커니즘**(어텐션은 관련성이 높은 정보에 집중하는 메커니즘)\n",
        "- 마치 책을 읽을 때 중요한 문장에 형광펜을 치는 것과 같다\n",
        "\n"
      ],
      "metadata": {
        "id": "5EN82gyCeMKz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 구성요소\n",
        "\n",
        "|구성 요소|역할|비유|\n",
        "|---|---|---|\n",
        "|Query (Q)|질문, 찾고 있는 것|\"번역하려는 현재 단어\"|\n",
        "|Key (K)|참조할 정보들의 인덱스|\"사전의 찾기 목록\"|\n",
        "|Value (V)|실제 정보 내용|\"사전의 실제 뜻 설명\"|\n",
        "\n",
        "- Attention 종류\n",
        "\n",
        "|어텐션 타입|용도|Query|Key|Value|\n",
        "|---|---|---|---|---|\n",
        "|Self-Attention|문맥 이해|같은 문장|같은 문장|같은 문장|\n",
        "|Cross-Attention|번역, 요약|타겟 언어|소스 언어|소스 언어|\n",
        "|Visual Attention|이미지 캡션|생성할 단어|이미지 영역|이미지 특징|\n",
        "\n",
        "\n",
        "- 작동 원리\n",
        "    - Query로 어떤 정보를 찾을지 정의\n",
        "    - 모든 Key들과 유사도 계산 (점수 매기기)\n",
        "    - 점수가 높은 Value들에 더 집중\n",
        "    - 가중합으로 최종 결과 생성\n",
        "\n",
        "- 아키텍처 (참고 https://denev6.tistory.com/entry/Attention-Mechanism?category=1039051)"
      ],
      "metadata": {
        "id": "u3AybMBDttxF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Attention](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdna%2FcX4CSD%2FbtsGcN8fNML%2FAAAAAAAAAAAAAAAAAAAAAPaDbvrD7FwLowmxlMTPy2qzgCdlrNUXriNwMrY3_fn9%2Fimg.png%3Fcredential%3DyqXZFxpELC7KVnFOS48ylbz2pIh7yKj8%26expires%3D1759244399%26allow_ip%3D%26allow_referer%3D%26signature%3DeYx75sEAKFiOWcrO7ISyOMDj3VM%253D \"Attention\")\n",
        "\n"
      ],
      "metadata": {
        "id": "FNTEhsL2s8up"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Self-Attention**:\n",
        "    - \"한 시퀀스 내의 모든 위치가 서로 직접적으로 상호작용하여 각 위치가 다른 모든 위치들로부터 얼마나 정보를 가져올지 결정하는 메커니즘\"\n",
        "    - 기본 아이디어\n",
        "        - Query, Key, Value가 모두 같은 입력에서 나옴\n",
        "        - 문장의 각 단어가 같은 문장의 다른 모든 단어들과 관계를 계산\n",
        "    - 작동 과정\n",
        "        > 입력 문장: \"The cat sat on the mat\"\n",
        "\n",
        "        >각 단어가 질문: \"나와 관련있는 단어는?\"\n",
        "        >- \"cat\" → \"The\"(0.1), \"cat\"(0.8), \"sat\"(0.6), \"on\"(0.1), \"the\"(0.1), \"mat\"(0.3)\n",
        "        >- \"sat\" → \"The\"(0.1), \"cat\"(0.7), \"sat\"(0.9), \"on\"(0.4), \"the\"(0.1), \"mat\"(0.2)\n"
      ],
      "metadata": {
        "id": "1BZsiD2MDNWK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Scaled Dot-Product Attention\n",
        "    - Query와 Key의 유사도를 계산하고 √d_k로 스케일링한 후, 소프트맥스를 적용해 Value에 가중치를 부여하는 어텐션 메커니즘\n",
        "    - Attention(Q,K,V) = softmax(QK^T/√d_k)V\n",
        "        - QK^T: Query와 Key 간 유사도 계산\n",
        "        - √d_k: 차원 크기로 나눠서 기울기 안정화\n",
        "        - softmax: 확률 분포로 변환\n",
        "        - V: 실제 정보에 가중치 적용\n",
        "    - 비유 : 비유: \"시험 볼 때 중요한 부분에 형광펜 치는 것\" - 관련성 높은 정보에 더 집중!"
      ],
      "metadata": {
        "id": "YHuPulCM-gWD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **예제1 : 가장 기본적인 Attention**"
      ],
      "metadata": {
        "id": "tYVCIKaLqk5_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "def basic_attention(query, key, value):\n",
        "    \"\"\"가장 기본적인 어텐션 메커니즘\"\"\"\n",
        "    # 1. 점수 계산 (Query와 Key의 유사도)\n",
        "    scores = torch.matmul(query, key.transpose(-2, -1))\n",
        "    print(f\"점수 계산 결과: {scores}\")\n",
        "\n",
        "    # 2. 소프트맥스로 가중치 변환\n",
        "    attention_weights = F.softmax(scores, dim=-1)\n",
        "    print(f\"어텐션 가중치: {attention_weights}\")\n",
        "\n",
        "    # 3. Value에 가중치 적용\n",
        "    output = torch.matmul(attention_weights, value)\n",
        "\n",
        "    return output, attention_weights\n",
        "\n",
        "# 예제: 3개 단어, 4차원 임베딩\n",
        "query = torch.tensor([[1.0, 0.5, 0.2, 0.1]])  # 찾고자 하는 것\n",
        "key = torch.tensor([\n",
        "    [1.0, 0.3, 0.1, 0.0],  # 첫 번째 단어\n",
        "    [0.2, 1.0, 0.5, 0.3],  # 두 번째 단어\n",
        "    [0.1, 0.4, 1.0, 0.8]   # 세 번째 단어\n",
        "])\n",
        "value = torch.tensor([\n",
        "    [2.0, 1.0, 0.5, 0.2],  # 첫 번째 단어의 실제 정보\n",
        "    [1.5, 2.0, 1.0, 0.8],  # 두 번째 단어의 실제 정보\n",
        "    [0.8, 1.2, 2.0, 1.5]   # 세 번째 단어의 실제 정보\n",
        "])\n",
        "\n",
        "print(\"=== 기본 Attention 예제 ===\")\n",
        "output, weights = basic_attention(query, key, value)\n",
        "print(f\"최종 출력: {output}\")\n",
        "print(f\"어느 단어에 집중했나: {weights.squeeze().numpy()}\")"
      ],
      "metadata": {
        "id": "dHH_pLP2qjyw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **예제2 : Scaled Dot-Product Attention** (실제 사용되는 방식)"
      ],
      "metadata": {
        "id": "Nf1jtCg2rQyr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "def scaled_dot_product_attention(Q, K, V, mask=None):\n",
        "    \"\"\"실제 Transformer에서 사용하는 어텐션\"\"\"\n",
        "    d_k = Q.size(-1)  # Key 차원\n",
        "\n",
        "    # 1. Q와 K의 점곱 후 스케일링\n",
        "    scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)\n",
        "\n",
        "    # 2. 마스킹 적용 (필요한 경우)\n",
        "    if mask is not None:\n",
        "        scores = scores.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "    # 3. 소프트맥스로 확률 변환\n",
        "    attention_weights = F.softmax(scores, dim=-1)\n",
        "\n",
        "    # 4. Value에 가중치 적용\n",
        "    output = torch.matmul(attention_weights, V)\n",
        "\n",
        "    return output, attention_weights\n",
        "\n",
        "# 예제: 문장 \"I love AI\" (3개 단어)\n",
        "print(\"\\n=== Scaled Dot-Product Attention 예제 ===\")\n",
        "\n",
        "# 각 단어의 임베딩 (간단한 예시)\n",
        "sentence_embeddings = torch.tensor([\n",
        "    [1.0, 0.2, 0.3, 0.1],  # \"I\"\n",
        "    [0.3, 1.0, 0.8, 0.2],  # \"love\"\n",
        "    [0.1, 0.5, 1.0, 0.9]   # \"AI\"\n",
        "], dtype=torch.float32)\n",
        "\n",
        "# Q, K, V가 모두 같은 경우 (Self-Attention)\n",
        "Q = K = V = sentence_embeddings\n",
        "\n",
        "output, attention_weights = scaled_dot_product_attention(Q, K, V)\n",
        "\n",
        "print(\"입력 문장 임베딩:\")\n",
        "print(sentence_embeddings)\n",
        "print(\"\\n어텐션 가중치 (단어별 집중도):\")\n",
        "print(attention_weights.numpy())\n",
        "print(\"\\n어텐션 적용 후 출력:\")\n",
        "print(output.numpy())\n",
        "\n",
        "# 각 단어가 어디에 집중했는지 해석\n",
        "words = [\"I\", \"love\", \"AI\"]\n",
        "print(\"\\n=== 해석 ===\")\n",
        "for i, word in enumerate(words):\n",
        "    weights = attention_weights[i].numpy()\n",
        "    print(f\"'{word}'가 집중한 단어들:\")\n",
        "    for j, w in enumerate(words):\n",
        "        print(f\"  {w}: {weights[j]:.3f}\")\n",
        "    print()"
      ],
      "metadata": {
        "id": "a4q2Jl7trUAV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **예제3 : 실제 번역 예제로 이해하기**"
      ],
      "metadata": {
        "id": "nu9V7WLLrUc0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def attention_translation_example():\n",
        "    \"\"\"번역에서 어텐션이 어떻게 작동하는지 보여주는 예제\"\"\"\n",
        "\n",
        "    # 영어 문장: \"I love you\"\n",
        "    english_words = [\"I\", \"love\", \"you\"]\n",
        "    # 한국어 번역: \"나는 너를 사랑해\"\n",
        "    korean_words = [\"나는\", \"너를\", \"사랑해\"]\n",
        "\n",
        "    # 간단한 임베딩 (실제로는 더 복잡)\n",
        "    english_embeddings = torch.tensor([\n",
        "        [1.0, 0.1, 0.2],  # I\n",
        "        [0.2, 1.0, 0.8],  # love\n",
        "        [0.1, 0.2, 1.0]   # you\n",
        "    ], dtype=torch.float32)\n",
        "\n",
        "    korean_embeddings = torch.tensor([\n",
        "        [0.9, 0.1, 0.1],  # 나는\n",
        "        [0.1, 0.1, 0.9],  # 너를\n",
        "        [0.1, 0.9, 0.2]   # 사랑해\n",
        "    ], dtype=torch.float32)\n",
        "\n",
        "    print(\"=== 번역 어텐션 예제 ===\")\n",
        "    print(\"영어:\", english_words)\n",
        "    print(\"한국어:\", korean_words)\n",
        "\n",
        "    # 각 한국어 단어가 영어 단어들에 얼마나 집중하는지\n",
        "    for i, korean_word in enumerate(korean_words):\n",
        "        query = korean_embeddings[i:i+1]  # 현재 한국어 단어\n",
        "        key = english_embeddings          # 모든 영어 단어들\n",
        "        value = english_embeddings\n",
        "\n",
        "        output, weights = scaled_dot_product_attention(query, key, value)\n",
        "\n",
        "        print(f\"\\n'{korean_word}' 번역시 영어 단어 집중도:\")\n",
        "        for j, eng_word in enumerate(english_words):\n",
        "            print(f\"  {eng_word}: {weights[0][j].item():.3f}\")\n",
        "\n",
        "attention_translation_example()"
      ],
      "metadata": {
        "id": "nkUNNPH-rUrd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **예제4 : 시각적 어텐션** (이미지에서 텍스트 생성)"
      ],
      "metadata": {
        "id": "lWzMtrxjr7S2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def visual_attention_example():\n",
        "    \"\"\"이미지 캡션 생성에서의 어텐션 예제\"\"\"\n",
        "\n",
        "    # 이미지의 다른 영역들 (5개 영역)\n",
        "    image_regions = [\"하늘\", \"나무\", \"사람\", \"강아지\", \"잔디\"]\n",
        "\n",
        "    # 각 영역의 특징 벡터 (간단히 3차원)\n",
        "    region_features = torch.tensor([\n",
        "        [0.8, 0.2, 0.1],  # 하늘 (파란색 위주)\n",
        "        [0.2, 0.8, 0.3],  # 나무 (초록색 위주)\n",
        "        [0.6, 0.4, 0.5],  # 사람 (복합적)\n",
        "        [0.7, 0.3, 0.2],  # 강아지 (갈색 위주)\n",
        "        [0.1, 0.9, 0.2]   # 잔디 (초록색 위주)\n",
        "    ], dtype=torch.float32)\n",
        "\n",
        "    # 생성하려는 단어들\n",
        "    words_to_generate = [\"사람이\", \"강아지와\", \"함께\"]\n",
        "    word_queries = torch.tensor([\n",
        "        [0.6, 0.4, 0.5],  # \"사람이\" - 사람 영역에 집중해야 함\n",
        "        [0.7, 0.3, 0.2],  # \"강아지와\" - 강아지 영역에 집중\n",
        "        [0.4, 0.4, 0.4]   # \"함께\" - 전체적으로 봐야 함\n",
        "    ], dtype=torch.float32)\n",
        "\n",
        "    print(\"=== 시각적 어텐션 예제 ===\")\n",
        "    print(\"이미지 영역들:\", image_regions)\n",
        "\n",
        "    for i, word in enumerate(words_to_generate):\n",
        "        query = word_queries[i:i+1]\n",
        "        key = value = region_features\n",
        "\n",
        "        output, weights = scaled_dot_product_attention(query, key, value)\n",
        "\n",
        "        print(f\"\\n'{word}' 생성시 이미지 영역별 집중도:\")\n",
        "        for j, region in enumerate(image_regions):\n",
        "            attention_score = weights[0][j].item()\n",
        "            bar = \"█\" * int(attention_score * 20)  # 막대 그래프로 표시\n",
        "            print(f\"  {region:4s}: {attention_score:.3f} {bar}\")\n",
        "\n",
        "visual_attention_example()"
      ],
      "metadata": {
        "id": "B99OetApr9uu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **예제5 : Self-Attention으로 문맥 이해하기**"
      ],
      "metadata": {
        "id": "Z6tNtp9PsRJv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def self_attention_context():\n",
        "    \"\"\"Self-Attention으로 문맥을 어떻게 이해하는지 보여주는 예제\"\"\"\n",
        "\n",
        "    # 문장: \"The bank can guarantee deposits will eventually cover future tuition costs\"\n",
        "    # \"bank\"가 금융기관인지 강둑인지 문맥으로 판단해야 함\n",
        "\n",
        "    sentence = [\"The\", \"bank\", \"can\", \"guarantee\", \"deposits\"]\n",
        "\n",
        "    # 단어별 간단한 임베딩 (실제로는 훨씬 복잡)\n",
        "    embeddings = torch.tensor([\n",
        "        [0.1, 0.2, 0.3, 0.1],  # The\n",
        "        [0.5, 0.3, 0.2, 0.4],  # bank (애매한 의미)\n",
        "        [0.2, 0.4, 0.8, 0.1],  # can\n",
        "        [0.3, 0.6, 0.4, 0.7],  # guarantee (보장하다 - 금융 관련)\n",
        "        [0.8, 0.2, 0.3, 0.9]   # deposits (예금 - 금융 관련)\n",
        "    ], dtype=torch.float32)\n",
        "\n",
        "    print(\"=== Self-Attention 문맥 이해 예제 ===\")\n",
        "    print(\"문장:\", \" \".join(sentence))\n",
        "\n",
        "    # Self-attention 계산\n",
        "    output, attention_weights = scaled_dot_product_attention(embeddings, embeddings, embeddings)\n",
        "\n",
        "    # \"bank\" 단어(인덱스 1)가 다른 단어들에 얼마나 집중하는지 확인\n",
        "    bank_attention = attention_weights[1]\n",
        "\n",
        "    print(f\"\\n'bank'가 각 단어에 집중하는 정도:\")\n",
        "    for i, word in enumerate(sentence):\n",
        "        attention_score = bank_attention[i].item()\n",
        "        print(f\"  {word:9s}: {attention_score:.3f}\")\n",
        "\n",
        "    print(f\"\\n해석: 'bank'가 'guarantee'({bank_attention[3]:.3f})와 'deposits'({bank_attention[4]:.3f})에\")\n",
        "    print(\"높은 집중도를 보이므로, 금융기관의 의미로 이해됨!\")\n",
        "\n",
        "self_attention_context()"
      ],
      "metadata": {
        "id": "2vAnOCXhsRiV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Transformer 메커니즘**"
      ],
      "metadata": {
        "id": "bmAOXjcPtd9C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 2017년 \"Attention Is All You Need\" 논문 발표 (Google)\n",
        "- https://arxiv.org/abs/1706.03762\n",
        "- 배경: RNN의 순차 처리 한계 극복\n",
        "- 혁신: 병렬 처리 + Self-Attention으로 성능 대폭 향상\n",
        "-\n",
        "- 핵심 구조\n",
        "|구성 요소|기능|핵심 특징|\n",
        "|---|---|---|\n",
        "|Self-Attention|문장 내 단어들 간의 관계 파악|모든 위치를 동시에 참조|\n",
        "|Multi-Head Attention|여러 관점에서 동시에 집중|8개 헤드로 다양한 패턴 학습|\n",
        "|Positional Encoding|단어 순서 정보 제공|삼각함수로 위치 인코딩|\n",
        "|Feed Forward Network|비선형 변환|2층 완전연결층|\n",
        "|Layer Normalization|학습 안정화|각 층마다 정규화|"
      ],
      "metadata": {
        "id": "KL-57hMvt5GV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 비유\n",
        "    - RNN: 책을 한 줄씩 읽기 (느리지만 순서 중요)\n",
        "    - Transformer: 책 전체를 한번에 스캔 후 중요 부분에 집중 (빠르고 정확)\n",
        "        - \"모든 단어가 모든 단어와 직접 대화할 수 있게 하자!\"\n",
        "- 아키텍처 (참고 https://wikidocs.net/31379)"
      ],
      "metadata": {
        "id": "2GK5EpOnv4mO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<!-- ![Transformer](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdna%2FbaUw7E%2FbtsGwRgo7Gh%2FAAAAAAAAAAAAAAAAAAAAADsw8eQNDsJIV5siCEHkZ47STihuV8H4bDHsi0Pn9TWk%2Fimg.png%3Fcredential%3DyqXZFxpELC7KVnFOS48ylbz2pIh7yKj8%26expires%3D1759244399%26allow_ip%3D%26allow_referer%3D%26signature%3D2JkSSaqR3ITrU1zIsxbLgcxJomQ%253D \"Transformer\") -->\n",
        "\n",
        "\n",
        "![Transformer](https://wikidocs.net/images/page/236193/%ED%8A%B8%EB%9E%9C%EC%8A%A4%ED%8F%AC%EB%A8%B8.png \"Transformer\")"
      ],
      "metadata": {
        "id": "uVwA9W381S4t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **예제 1: 간단한 Attention 계산**"
      ],
      "metadata": {
        "id": "uYPn-39xukWq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "def simple_attention(query, key, value):\n",
        "    \"\"\"간단한 어텐션 메커니즘\"\"\"\n",
        "    # 1. Query와 Key의 유사도 계산\n",
        "    scores = torch.matmul(query, key.transpose(-2, -1))\n",
        "\n",
        "    # 2. 소프트맥스로 가중치 계산\n",
        "    attention_weights = F.softmax(scores, dim=-1)\n",
        "\n",
        "    # 3. Value에 가중치 적용\n",
        "    output = torch.matmul(attention_weights, value)\n",
        "\n",
        "    return output, attention_weights\n",
        "\n",
        "# 예제 데이터 (문장 길이=4, 임베딩 차원=6)\n",
        "seq_len, d_model = 4, 6\n",
        "query = torch.randn(1, seq_len, d_model)\n",
        "key = torch.randn(1, seq_len, d_model)\n",
        "value = torch.randn(1, seq_len, d_model)\n",
        "\n",
        "# 어텐션 계산\n",
        "output, weights = simple_attention(query, key, value)\n",
        "\n",
        "print(\"입력 shape:\", query.shape)\n",
        "print(\"출력 shape:\", output.shape)\n",
        "print(\"어텐션 가중치 shape:\", weights.shape)\n",
        "print(\"\\n어텐션 가중치 (어디에 집중했는지):\")\n",
        "print(weights[0].detach().numpy())"
      ],
      "metadata": {
        "id": "kYj-Ex4ZusGl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **예제 2: Multi-Head Attention**"
      ],
      "metadata": {
        "id": "tmBAvUQhustY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_model // num_heads\n",
        "\n",
        "        # 선형 변환층들\n",
        "        self.W_q = nn.Linear(d_model, d_model)\n",
        "        self.W_k = nn.Linear(d_model, d_model)\n",
        "        self.W_v = nn.Linear(d_model, d_model)\n",
        "        self.W_o = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_len, d_model = x.size()\n",
        "\n",
        "        # 1. Q, K, V 생성\n",
        "        Q = self.W_q(x)\n",
        "        K = self.W_k(x)\n",
        "        V = self.W_v(x)\n",
        "\n",
        "        # 2. 멀티헤드로 분할\n",
        "        Q = Q.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        K = K.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        V = V.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        # 3. 어텐션 계산\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
        "        attention_weights = F.softmax(scores, dim=-1)\n",
        "        attention_output = torch.matmul(attention_weights, V)\n",
        "\n",
        "        # 4. 헤드들 합치기\n",
        "        attention_output = attention_output.transpose(1, 2).contiguous().view(\n",
        "            batch_size, seq_len, d_model)\n",
        "\n",
        "        # 5. 최종 선형 변환\n",
        "        output = self.W_o(attention_output)\n",
        "\n",
        "        return output\n",
        "\n",
        "# 사용 예시\n",
        "d_model, num_heads = 512, 8\n",
        "mha = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "# 입력: (배치=2, 시퀀스=10, 특성=512)\n",
        "x = torch.randn(2, 10, d_model)\n",
        "output = mha(x)\n",
        "\n",
        "print(f\"입력 shape: {x.shape}\")\n",
        "print(f\"출력 shape: {output.shape}\")\n",
        "print(\"Multi-Head Attention 완료!\")"
      ],
      "metadata": {
        "id": "aFus3Q5nus6R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **예제 3: 간단한 Transformer 블록**"
      ],
      "metadata": {
        "id": "nfTUKYQ7utF9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# (가정) MultiHeadAttention은 아래와 같은 시그니처/동작을 가진다고 가정합니다.\n",
        "# - 입력: x (B, T, d_model)\n",
        "# - 출력: (B, T, d_model)  ← 각 헤드에서 산출된 어텐션 결과를 concat + 선형 변환 후 반환\n",
        "# - (확장) 실제 구현은 마스크, key_padding_mask 등을 받을 수 있음. 여기서는 단순화.\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super().__init__()\n",
        "        assert d_model % num_heads == 0, \"d_model은 num_heads로 나누어 떨어져야 합니다.\"\n",
        "        self.attn = nn.MultiheadAttention(embed_dim=d_model, num_heads=num_heads, batch_first=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 자기어텐션(Self-Attention): Q=K=V=x\n",
        "        # MultiheadAttention은 (B, T, C) 입력을 지원(batch_first=True)하므로 그대로 사용 가능\n",
        "        out, _ = self.attn(x, x, x, need_weights=False)\n",
        "        return out\n",
        "\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "        \"\"\"\n",
        "        하나의 Transformer 인코더 블록(원 논문의 Post-LN 구성과 동일한 형태):\n",
        "        - 서브층 1: 멀티헤드 자기어텐션(Multi-Head Self-Attention)\n",
        "        - 서브층 2: 포지션-와이즈 피드포워드 네트워크(FFN)\n",
        "        - 각 서브층 뒤에 잔차연결(Residual) + LayerNorm\n",
        "        - 드롭아웃으로 과적합 방지\n",
        "\n",
        "        Args:\n",
        "            d_model (int): 임베딩 차원 (모든 서브층의 입출력 채널 수)\n",
        "            num_heads (int): 멀티헤드 개수 (d_model % num_heads == 0 권장/필수)\n",
        "            d_ff (int): FFN 내부 확장 차원 (일반적으로 4 * d_model)\n",
        "            dropout (float): 드롭아웃 비율 (학습 시만 적용, eval()에서는 비활성)\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # 1) Multi-Head Attention 서브층\n",
        "        #    입력과 동일 차원의 출력을 내는 자기어텐션(Self-Attention) 모듈\n",
        "        self.attention = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "        # 2) Position-wise Feed-Forward Network (FFN)\n",
        "        #    각 시점(토큰 위치)별로 동일한 두 개의 선형변환을 적용:\n",
        "        #      d_model → d_ff (확장) → 활성화(ReLU) → d_ff → d_model (축소)\n",
        "        #    ReLU 대신 GeLU를 쓰는 변형도 많음(BERT/Transformers).\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(d_model, d_ff),  # 확장\n",
        "            nn.ReLU(),                 # 비선형성 (학습표현력 향상)\n",
        "            nn.Linear(d_ff, d_model)   # 원래 차원으로 축소\n",
        "        )\n",
        "\n",
        "        # 3) Layer Normalization (Post-LN)\n",
        "        #    각 서브층의 출력 + 잔차를 더한 뒤 정규화.\n",
        "        #    원 논문(2017)은 Post-LN, 최근엔 Pre-LN(서브층 앞에 LN)도 자주 사용됨.\n",
        "        self.norm1 = nn.LayerNorm(d_model)  # 어텐션 서브층 뒤\n",
        "        self.norm2 = nn.LayerNorm(d_model)  # FFN 서브층 뒤\n",
        "\n",
        "        # 4) Dropout\n",
        "        #    서브층 출력에 드롭아웃을 적용하여 과적합을 줄임.\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: 입력 시퀀스 임베딩 텐서 (B, T, d_model)\n",
        "        Returns:\n",
        "            (B, T, d_model): 입력과 동일한 shape. (Residual로 “정보 보존 + 변환”)\n",
        "        \"\"\"\n",
        "        # --- 서브층 1: Self-Attention + Residual + LayerNorm ---\n",
        "        # attention_output: (B, T, d_model)\n",
        "        attention_output = self.attention(x)\n",
        "\n",
        "        # 드롭아웃 후, 입력 x에 Residual 연결로 더함 (정보 경로 보존 & 기울기 흐름 안정화)\n",
        "        # 그 다음 LayerNorm으로 분포를 정규화하여 학습을 안정화 (Post-LN 패턴)\n",
        "        x = self.norm1(x + self.dropout(attention_output))\n",
        "\n",
        "        # --- 서브층 2: Feed-Forward + Residual + LayerNorm ---\n",
        "        # ffn_output: (B, T, d_model)  ← position-wise로 독립적 적용 (T마다 같은 파라미터)\n",
        "        ffn_output = self.ffn(x)\n",
        "\n",
        "        # 다시 드롭아웃 → Residual → LayerNorm (Post-LN)\n",
        "        x = self.norm2(x + self.dropout(ffn_output))\n",
        "\n",
        "        # 출력 shape는 입력과 동일 (B, T, d_model)\n",
        "        return x\n",
        "\n",
        "\n",
        "# =========================\n",
        "# Transformer 블록 테스트\n",
        "# =========================\n",
        "\n",
        "# 하이퍼파라미터 설정 예:\n",
        "# - d_model=512: 토큰 임베딩/채널 수\n",
        "# - num_heads=8: 512/8=64 → 각 헤드의 차원 d_k=64 (정수로 나눠떨어짐)\n",
        "# - d_ff=2048: FFN 내부 확장(=4 * d_model) → 원 논문 기본 권장 설정\n",
        "transformer_block = TransformerBlock(d_model=512, num_heads=8, d_ff=2048)\n",
        "\n",
        "# 더미 입력: 배치 크기 B=2, 시퀀스 길이 T=10, 채널 C=d_model=512\n",
        "x = torch.randn(2, 10, 512)  # (B, T, C)\n",
        "\n",
        "# 순전파: 출력도 (B, T, C) 형태를 유지 (Residual 덕분에 차원 보존)\n",
        "output = transformer_block(x)\n",
        "\n",
        "print(f\"Transformer 블록 입력:  {x.shape}\")     # torch.Size([2, 10, 512])\n",
        "print(f\"Transformer 블록 출력: {output.shape}\") # torch.Size([2, 10, 512])\n",
        "print(\"Transformer 블록 처리 완료!\")\n"
      ],
      "metadata": {
        "id": "Ef6XeDwYutOD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "F__Q1Jh7Gaq7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **BERT**\n"
      ],
      "metadata": {
        "id": "RXLkoXSVGdOW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **BERT(Bidirectional Encoder Representations from Transformers)**\n",
        "- 2018년 구글에서 발표한 혁신적인 자연어처리 모델\n",
        "- 기존의 일방향 언어모델과 달리 양방향으로 문맥을 이해할 수 있어 획기적인 성능 향상을 보임\n",
        "한국어 BERT 모델들은 이러한 BERT 아키텍처를 한국어에 맞게 적용한 것으로:\n"
      ],
      "metadata": {
        "id": "PksYoKg3Gu9X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 한국어 BERT 모델\n",
        "- 2019년: SKT에서 KoBERT 공개\n",
        "- 2020년: Beomi님이 KcBERT 공개\n",
        "- 2020년: KETI에서 KR-BERT 공개"
      ],
      "metadata": {
        "id": "9s9HGlq8Gw0o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 주의사항\n",
        "    - **메모리 관리**: 코랩에서는 GPU 메모리 제한이 있으므로 배치크기 조절 필요\n",
        "    - **모델 선택**: KoBERT, KcBERT, KR-BERT 중 태스크에 맞는 모델 선택\n",
        "    - **데이터 전처리**: 한국어 특성을 고려한 전처리 (띄어쓰기, 특수문자 등)\n",
        "    - **평가 지표**: 정확도 외에 F1-score, Precision, Recall 등 다양한 지표 활용"
      ],
      "metadata": {
        "id": "3j3T10z4H8t6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **예제1: 기본 설치 및 설정**"
      ],
      "metadata": {
        "id": "QXn9w4rGHDbn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 필요한 라이브러리 설치\n",
        "!pip install transformers torch datasets sentencepiece\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification\n",
        "from transformers import TrainingArguments, Trainer\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# GPU 사용 확인\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"사용 디바이스: {device}\")"
      ],
      "metadata": {
        "id": "7EE82MMmHH2I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **예제2: KoBERT 기본 사용법**"
      ],
      "metadata": {
        "id": "YUtoeVayHH9o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# KoBERT 모델 및 토크나이저 로드\n",
        "model_name = \"skt/kobert-base-v1\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModel.from_pretrained(model_name)\n",
        "\n",
        "# 텍스트 인코딩 예제\n",
        "texts = [\n",
        "    \"오늘 날씨가 정말 좋네요!\",\n",
        "    \"이 영화는 너무 재미없어요.\",\n",
        "    \"파이썬 프로그래밍을 배우고 싶습니다.\"\n",
        "]\n",
        "\n",
        "# 토큰화 및 인코딩\n",
        "encoded = tokenizer(texts, padding=True, truncation=True, return_tensors='pt')\n",
        "print(\"토큰화 결과:\")\n",
        "for i, text in enumerate(texts):\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    print(f\"'{text}' -> {tokens}\")\n",
        "\n",
        "# 모델 추론\n",
        "with torch.no_grad():\n",
        "    outputs = model(**encoded)\n",
        "    last_hidden_states = outputs.last_hidden_state\n",
        "\n",
        "print(f\"\\n임베딩 차원: {last_hidden_states.shape}\")"
      ],
      "metadata": {
        "id": "kQwlgaAFHIEF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **예제3: 감정분석 실습 (파인튜닝)**"
      ],
      "metadata": {
        "id": "3gg5MnWjHIJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 샘플 데이터셋 생성 (실제로는 더 큰 데이터셋 사용)\n",
        "data = {\n",
        "    'text': [\n",
        "        \"이 제품 정말 좋아요! 강추합니다.\",\n",
        "        \"배송이 너무 늦어서 짜증나네요.\",\n",
        "        \"가격 대비 괜찮은 것 같아요.\",\n",
        "        \"품질이 생각보다 좋지 않네요.\",\n",
        "        \"서비스가 친절하고 만족스러워요.\",\n",
        "        \"다시는 이용하고 싶지 않습니다.\",\n",
        "        \"보통 수준인 것 같아요.\",\n",
        "        \"정말 실망스러운 경험이었습니다.\"\n",
        "    ],\n",
        "    'label': [1, 0, 1, 0, 1, 0, 1, 0]  # 1: 긍정, 0: 부정\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "\n",
        "# 데이터셋 클래스 정의\n",
        "class SentimentDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts.iloc[idx])\n",
        "        label = self.labels.iloc[idx]\n",
        "\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=self.max_length,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# 데이터셋 생성\n",
        "train_dataset = SentimentDataset(\n",
        "    train_df['text'], train_df['label'], tokenizer\n",
        ")\n",
        "val_dataset = SentimentDataset(\n",
        "    val_df['text'], val_df['label'], tokenizer\n",
        ")\n",
        "\n",
        "# 분류 모델 로드\n",
        "classification_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_name, num_labels=2\n",
        ")\n",
        "\n",
        "# 훈련 설정\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        ")\n",
        "\n",
        "# 평가 함수\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    return {'accuracy': accuracy_score(labels, predictions)}\n",
        "\n",
        "# 트레이너 설정 및 훈련\n",
        "trainer = Trainer(\n",
        "    model=classification_model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "# 모델 훈련 (주석 처리 - 실제 사용시 활성화)\n",
        "# trainer.train()\n",
        "\n",
        "# 예측 함수\n",
        "def predict_sentiment(text):\n",
        "    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n",
        "    with torch.no_grad():\n",
        "        outputs = classification_model(**inputs)\n",
        "        predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "        predicted_class = torch.argmax(predictions, dim=-1).item()\n",
        "        confidence = predictions[0][predicted_class].item()\n",
        "\n",
        "    sentiment = \"긍정\" if predicted_class == 1 else \"부정\"\n",
        "    return sentiment, confidence\n",
        "\n",
        "# 테스트\n",
        "test_texts = [\n",
        "    \"이 강의는 정말 유익하고 재미있어요!\",\n",
        "    \"설명이 너무 어려워서 이해하기 힘들어요.\",\n",
        "    \"적당한 수준인 것 같습니다.\"\n",
        "]\n",
        "\n",
        "for text in test_texts:\n",
        "    sentiment, confidence = predict_sentiment(text)\n",
        "    print(f\"'{text}' -> {sentiment} (확신도: {confidence:.2f})\")"
      ],
      "metadata": {
        "id": "irqRcnc-HIQo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **예제4: 한국어 단어 유사도 측정**"
      ],
      "metadata": {
        "id": "4J4_MmyFHbS0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def get_word_embedding(word):\n",
        "    \"\"\"단어의 임베딩 벡터를 구하는 함수\"\"\"\n",
        "    inputs = tokenizer(word, return_tensors='pt')\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        # [CLS] 토큰의 임베딩 사용\n",
        "        embedding = outputs.last_hidden_state[0, 0, :].numpy()\n",
        "    return embedding\n",
        "\n",
        "# 단어들의 유사도 측정\n",
        "words = [\"사랑\", \"좋아\", \"행복\", \"슬픔\", \"화남\", \"기쁨\"]\n",
        "embeddings = []\n",
        "\n",
        "for word in words:\n",
        "    embedding = get_word_embedding(word)\n",
        "    embeddings.append(embedding)\n",
        "\n",
        "# 유사도 행렬 계산\n",
        "similarity_matrix = cosine_similarity(embeddings)\n",
        "\n",
        "print(\"단어 유사도 매트릭스:\")\n",
        "print(\"\\t\", \"\\t\".join(words))\n",
        "for i, word in enumerate(words):\n",
        "    similarities = [f\"{sim:.3f}\" for sim in similarity_matrix[i]]\n",
        "    print(f\"{word}\\t\" + \"\\t\".join(similarities))"
      ],
      "metadata": {
        "id": "GGsOXOMpHmXb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **예제5: 문장 완성 게임**"
      ],
      "metadata": {
        "id": "KN6l87K2HqId"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# 마스크 언어 모델 파이프라인 생성\n",
        "fill_mask = pipeline(\"fill-mask\", model=model_name, tokenizer=tokenizer)\n",
        "\n",
        "def sentence_completion_game(sentence_with_mask):\n",
        "    \"\"\"마스크된 단어를 예측하는 게임\"\"\"\n",
        "    results = fill_mask(sentence_with_mask)\n",
        "\n",
        "    print(f\"원문: {sentence_with_mask}\")\n",
        "    print(\"예측 결과:\")\n",
        "    for i, result in enumerate(results[:5], 1):\n",
        "        filled_sentence = result['sequence']\n",
        "        score = result['score']\n",
        "        print(f\"{i}. {filled_sentence} (확률: {score:.3f})\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "# 게임 예제들\n",
        "game_sentences = [\n",
        "    \"오늘 [MASK]가 정말 좋네요.\",\n",
        "    \"파이썬은 [MASK] 프로그래밍 언어입니다.\",\n",
        "    \"AI는 미래의 [MASK]를 바꿀 것입니다.\"\n",
        "]\n",
        "\n",
        "for sentence in game_sentences:\n",
        "    sentence_completion_game(sentence)"
      ],
      "metadata": {
        "id": "Lp1WF74PHqRi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **예제6: 텍스트 분류 놀이**"
      ],
      "metadata": {
        "id": "-jWxlIrnHqZO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def classify_text_topic(text):\n",
        "    \"\"\"텍스트의 주제를 분류하는 간단한 예제\"\"\"\n",
        "    # 키워드 기반 간단 분류 (실제로는 더 정교한 모델 필요)\n",
        "    tech_keywords = [\"프로그래밍\", \"코딩\", \"AI\", \"컴퓨터\", \"소프트웨어\"]\n",
        "    food_keywords = [\"음식\", \"맛있\", \"요리\", \"식당\", \"메뉴\"]\n",
        "    movie_keywords = [\"영화\", \"배우\", \"감독\", \"스토리\", \"상영\"]\n",
        "\n",
        "    tech_score = sum(1 for keyword in tech_keywords if keyword in text)\n",
        "    food_score = sum(1 for keyword in food_keywords if keyword in text)\n",
        "    movie_score = sum(1 for keyword in movie_keywords if keyword in text)\n",
        "\n",
        "    scores = {\"기술\": tech_score, \"음식\": food_score, \"영화\": movie_score}\n",
        "    predicted_topic = max(scores, key=scores.get)\n",
        "\n",
        "    return predicted_topic, scores\n",
        "\n",
        "# 테스트 문장들\n",
        "test_sentences = [\n",
        "    \"파이썬으로 AI 프로그래밍을 배우고 있어요.\",\n",
        "    \"어제 먹은 파스타가 정말 맛있었어요.\",\n",
        "    \"이번 주말에 개봉한 영화를 보러 갈 예정입니다.\"\n",
        "]\n",
        "\n",
        "print(\"텍스트 주제 분류 결과:\")\n",
        "for text in test_sentences:\n",
        "    topic, scores = classify_text_topic(text)\n",
        "    print(f\"'{text}' -> 주제: {topic}, 점수: {scores}\")"
      ],
      "metadata": {
        "id": "9V72EK28Hqhx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "HTH8dVxiIXxl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **GPT**"
      ],
      "metadata": {
        "id": "FataFDpgIZMS"
      }
    }
  ]
}