{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanghyun-ai/ktcloud_genai/blob/main/110_LLM_Tokenization_%E1%84%8B%E1%85%AA%E1%86%AB%E1%84%89%E1%85%A5%E1%86%BC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Tokenization**"
      ],
      "metadata": {
        "id": "_intVJo1FXQF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "- 💡 **NOTE**\n",
        "    - 이 노트북의 코드를 실행하려면 GPU를 사용하는 것이 좋습니다. 구글 코랩에서는 **런타임 > 런타임 유형 변경 > 하드웨어 가속기 > T4 GPU**를 선택하세요.\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "81_Ybs4LI7IX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Tokenization 이란?**"
      ],
      "metadata": {
        "id": "X8ya0DICf_mC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 텍스트를 컴퓨터가 처리할 수 있는 의미있는 단위(토큰)로 나누는 전처리 과정\n",
        "- 기본 아이디어\n",
        "    - 텍스트 → 토큰 리스트 변환\n",
        "    - AI 모델이 이해할 수 있는 최소 의미 단위(=토큰)로 분할\n",
        "    - 언어마다 다른 방식 적용\n",
        "- 토큰화 방법\n",
        "\n",
        "|방법|설명|예시|장점|단점|\n",
        "|---|---|---|---|---|\n",
        "|단어 단위|공백으로 분할\"|I love AI\" → [\"I\", \"love\", \"AI\"]|직관적어휘 크기 폭발|\n",
        "|문자 단위|글자별 분할|\"Hello\" → [\"H\", \"e\", \"l\", \"l\", \"o\"]|어휘 크기 작음|의미 정보 손실|\n",
        "|서브워드|단어를 더 작은 단위로|\"unknown\" → [\"un\", \"know\", \"n\"]|균형잡힌 접근|복잡한 알고리즘|\n",
        "|형태소 분석|의미 단위로 분할|\"먹었다\" → [\"먹\", \"었\", \"다\"]|언어학적 정확성|언어별 특화 필요|"
      ],
      "metadata": {
        "id": "F2_OuRR6LLCG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 예제 1: 기본 토큰화 방법들"
      ],
      "metadata": {
        "id": "_m2T0NYhIG8H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "def basic_tokenization_examples():\n",
        "    text = \"Hello, World! I'm learning NLP. It's amazing!\"\n",
        "\n",
        "    print(\"=== 기본 토큰화 방법들 ===\")\n",
        "    print(f\"원본 텍스트: {text}\")\n",
        "\n",
        "    # 1. 공백 기반 분할\n",
        "    whitespace_tokens = text.split()\n",
        "    print(f\"\\n1. 공백 분할: {whitespace_tokens}\")\n",
        "\n",
        "    # 2. 정규식으로 단어 추출\n",
        "    word_tokens = re.findall(r'\\w+', text.lower())\n",
        "    print(f\"2. 단어만 추출: {word_tokens}\")\n",
        "\n",
        "    # 3. 문자 단위 분할\n",
        "    char_tokens = list(text.lower())\n",
        "    print(f\"3. 문자 분할: {char_tokens}\")\n",
        "\n",
        "    # 4. 구두점 포함 분할\n",
        "    punct_tokens = re.findall(r'\\w+|[^\\w\\s]', text)\n",
        "    print(f\"4. 구두점 포함: {punct_tokens}\")\n",
        "\n",
        "basic_tokenization_examples()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eH2TnXbxIHGx",
        "outputId": "5fca33f3-997a-488e-de87-9be7240ca180"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== 기본 토큰화 방법들 ===\n",
            "원본 텍스트: Hello, World! I'm learning NLP. It's amazing!\n",
            "\n",
            "1. 공백 분할: ['Hello,', 'World!', \"I'm\", 'learning', 'NLP.', \"It's\", 'amazing!']\n",
            "2. 단어만 추출: ['hello', 'world', 'i', 'm', 'learning', 'nlp', 'it', 's', 'amazing']\n",
            "3. 문자 분할: ['h', 'e', 'l', 'l', 'o', ',', ' ', 'w', 'o', 'r', 'l', 'd', '!', ' ', 'i', \"'\", 'm', ' ', 'l', 'e', 'a', 'r', 'n', 'i', 'n', 'g', ' ', 'n', 'l', 'p', '.', ' ', 'i', 't', \"'\", 's', ' ', 'a', 'm', 'a', 'z', 'i', 'n', 'g', '!']\n",
            "4. 구두점 포함: ['Hello', ',', 'World', '!', 'I', \"'\", 'm', 'learning', 'NLP', '.', 'It', \"'\", 's', 'amazing', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 예제 2: 한국어 토큰화"
      ],
      "metadata": {
        "id": "ZEq8FkV8IOEy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def korean_tokenization():\n",
        "    korean_text = \"안녕하세요! 저는 한국어를 공부하고 있습니다.\"\n",
        "\n",
        "    print(\"\\n=== 한국어 토큰화 ===\")\n",
        "    print(f\"원본 텍스트: {korean_text}\")\n",
        "\n",
        "    # 1. 공백 기반 (한국어는 부정확)\n",
        "    space_tokens = korean_text.split()\n",
        "    print(f\"1. 공백 분할: {space_tokens}\")\n",
        "\n",
        "    # 2. 문자 단위\n",
        "    char_tokens = list(korean_text)\n",
        "    print(f\"2. 문자 분할: {char_tokens}\")\n",
        "\n",
        "    # 3. 간단한 한국어 토큰화 (실제로는 형태소 분석기 사용)\n",
        "    # 여기서는 어절 단위로 분리\n",
        "    simple_tokens = re.findall(r'[가-힣]+|[a-zA-Z]+|[!?.]', korean_text)\n",
        "    print(f\"3. 간단한 분할: {simple_tokens}\")\n",
        "\n",
        "korean_tokenization()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GF_FDBu-IP9I",
        "outputId": "9d6b1ba7-5813-49c0-ba25-fa92e77c52f2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== 한국어 토큰화 ===\n",
            "원본 텍스트: 안녕하세요! 저는 한국어를 공부하고 있습니다.\n",
            "1. 공백 분할: ['안녕하세요!', '저는', '한국어를', '공부하고', '있습니다.']\n",
            "2. 문자 분할: ['안', '녕', '하', '세', '요', '!', ' ', '저', '는', ' ', '한', '국', '어', '를', ' ', '공', '부', '하', '고', ' ', '있', '습', '니', '다', '.']\n",
            "3. 간단한 분할: ['안녕하세요', '!', '저는', '한국어를', '공부하고', '있습니다', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 예제 3: 서브워드 토큰화 시뮬레이션"
      ],
      "metadata": {
        "id": "8aWpo7ejISlq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def subword_tokenization_demo():\n",
        "    words = [\"unknown\", \"unhappy\", \"replay\", \"preprocessing\"]\n",
        "\n",
        "    print(\"\\n=== 서브워드 토큰화 데모 ===\")\n",
        "\n",
        "    # 간단한 BPE 시뮬레이션\n",
        "    vocab = set()\n",
        "\n",
        "    # 모든 문자를 기본 어휘에 추가\n",
        "    for word in words:\n",
        "        vocab.update(list(word))\n",
        "\n",
        "    # 자주 나오는 문자 조합 찾기\n",
        "    bigrams = Counter()\n",
        "    for word in words:\n",
        "        for i in range(len(word) - 1):\n",
        "            bigrams[word[i:i+2]] += 1\n",
        "\n",
        "    # 가장 빈번한 바이그램을 어휘에 추가\n",
        "    common_bigrams = bigrams.most_common(5)\n",
        "    for bigram, count in common_bigrams:\n",
        "        vocab.add(bigram)\n",
        "        print(f\"추가된 서브워드: '{bigram}' (빈도: {count})\")\n",
        "\n",
        "    print(f\"\\n최종 어휘: {sorted(vocab)}\")\n",
        "\n",
        "    # 단어를 서브워드로 분해 (단순화된 버전)\n",
        "    def tokenize_word(word):\n",
        "        tokens = []\n",
        "        i = 0\n",
        "        while i < len(word):\n",
        "            # 가장 긴 매칭 찾기\n",
        "            found = False\n",
        "            for length in range(min(3, len(word) - i), 0, -1):\n",
        "                subword = word[i:i+length]\n",
        "                if subword in vocab:\n",
        "                    tokens.append(subword)\n",
        "                    i += length\n",
        "                    found = True\n",
        "                    break\n",
        "            if not found:\n",
        "                tokens.append(word[i])\n",
        "                i += 1\n",
        "        return tokens\n",
        "\n",
        "    print(f\"\\n=== 서브워드 토큰화 결과 ===\")\n",
        "    for word in words:\n",
        "        tokens = tokenize_word(word)\n",
        "        print(f\"{word:15} → {tokens}\")\n",
        "\n",
        "subword_tokenization_demo()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7MF_j6FjITvt",
        "outputId": "ae31bac5-c26c-426a-eac2-058eca0a5ac6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== 서브워드 토큰화 데모 ===\n",
            "추가된 서브워드: 'un' (빈도: 2)\n",
            "추가된 서브워드: 're' (빈도: 2)\n",
            "추가된 서브워드: 'ep' (빈도: 2)\n",
            "추가된 서브워드: 'pr' (빈도: 2)\n",
            "추가된 서브워드: 'nk' (빈도: 1)\n",
            "\n",
            "최종 어휘: ['a', 'c', 'e', 'ep', 'g', 'h', 'i', 'k', 'l', 'n', 'nk', 'o', 'p', 'pr', 'r', 're', 's', 'u', 'un', 'w', 'y']\n",
            "\n",
            "=== 서브워드 토큰화 결과 ===\n",
            "unknown         → ['un', 'k', 'n', 'o', 'w', 'n']\n",
            "unhappy         → ['un', 'h', 'a', 'p', 'p', 'y']\n",
            "replay          → ['re', 'p', 'l', 'a', 'y']\n",
            "preprocessing   → ['pr', 'ep', 'r', 'o', 'c', 'e', 's', 's', 'i', 'n', 'g']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 예제 4: 실제 AI 모델에서 사용하는 토큰화"
      ],
      "metadata": {
        "id": "WuAIVwlRIcEG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ai_model_tokenization():\n",
        "    sentences = [\n",
        "        \"I love artificial intelligence!\",\n",
        "        \"Machine learning is fascinating.\",\n",
        "        \"Natural language processing rocks!\"\n",
        "    ]\n",
        "\n",
        "    print(\"\\n=== AI 모델용 토큰화 ===\")\n",
        "\n",
        "    # 1. 어휘 사전 구축\n",
        "    word_counter = Counter()\n",
        "    for sentence in sentences:\n",
        "        words = re.findall(r'\\w+', sentence.lower())\n",
        "        word_counter.update(words)\n",
        "\n",
        "    # 2. 특수 토큰 추가\n",
        "    vocab = {\n",
        "        '<pad>': 0,    # 패딩\n",
        "        '<unk>': 1,    # 미지 단어\n",
        "        '<start>': 2,  # 시작\n",
        "        '<end>': 3     # 끝\n",
        "    }\n",
        "\n",
        "    # 3. 빈번한 단어들 추가\n",
        "    for word, count in word_counter.most_common():\n",
        "        if count >= 1:  # 최소 빈도\n",
        "            vocab[word] = len(vocab)\n",
        "\n",
        "    print(f\"어휘 사전 크기: {len(vocab)}\")\n",
        "    print(f\"어휘: {list(vocab.keys())}\")\n",
        "\n",
        "    # 4. 문장을 토큰 ID로 변환\n",
        "    def sentence_to_ids(sentence, vocab, max_len=10):\n",
        "        words = re.findall(r'\\w+', sentence.lower())\n",
        "        ids = [vocab['<start>']]\n",
        "\n",
        "        for word in words:\n",
        "            if len(ids) >= max_len - 1:  # <end> 공간 확보\n",
        "                break\n",
        "            ids.append(vocab.get(word, vocab['<unk>']))\n",
        "\n",
        "        ids.append(vocab['<end>'])\n",
        "\n",
        "        # 패딩\n",
        "        while len(ids) < max_len:\n",
        "            ids.append(vocab['<pad>'])\n",
        "\n",
        "        return ids[:max_len]\n",
        "\n",
        "    print(f\"\\n=== 토큰 ID 변환 ===\")\n",
        "    for sentence in sentences:\n",
        "        token_ids = sentence_to_ids(sentence, vocab)\n",
        "        print(f\"'{sentence}'\")\n",
        "        print(f\"토큰 ID: {token_ids}\")\n",
        "\n",
        "        # 역변환으로 확인\n",
        "        id_to_vocab = {v: k for k, v in vocab.items()}\n",
        "        recovered_tokens = [id_to_vocab[id] for id in token_ids]\n",
        "        print(f\"토큰: {recovered_tokens}\")\n",
        "        print()\n",
        "\n",
        "ai_model_tokenization()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-hafV4SlIdFQ",
        "outputId": "ea7f2f9b-ff7f-424e-a3fc-9c97dffe12e7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== AI 모델용 토큰화 ===\n",
            "어휘 사전 크기: 16\n",
            "어휘: ['<pad>', '<unk>', '<start>', '<end>', 'i', 'love', 'artificial', 'intelligence', 'machine', 'learning', 'is', 'fascinating', 'natural', 'language', 'processing', 'rocks']\n",
            "\n",
            "=== 토큰 ID 변환 ===\n",
            "'I love artificial intelligence!'\n",
            "토큰 ID: [2, 4, 5, 6, 7, 3, 0, 0, 0, 0]\n",
            "토큰: ['<start>', 'i', 'love', 'artificial', 'intelligence', '<end>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
            "\n",
            "'Machine learning is fascinating.'\n",
            "토큰 ID: [2, 8, 9, 10, 11, 3, 0, 0, 0, 0]\n",
            "토큰: ['<start>', 'machine', 'learning', 'is', 'fascinating', '<end>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
            "\n",
            "'Natural language processing rocks!'\n",
            "토큰 ID: [2, 12, 13, 14, 15, 3, 0, 0, 0, 0]\n",
            "토큰: ['<start>', 'natural', 'language', 'processing', 'rocks', '<end>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "qOWBFhLLI3ZD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 현대 AI에서 사용하는 토크나이저\n"
      ],
      "metadata": {
        "id": "zMbMTp09I71r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "|토크나이저|사용 모델|특징\n",
        "|---|---|---|\n",
        "|BPE|GPT 시리즈|바이트 쌍 인코딩|\n",
        "|SentencePiece|T5, mT5|언어 무관한 서브워드|\n",
        "|WordPiece|BERT|구글 개발, 한국어 지원|"
      ],
      "metadata": {
        "id": "-sTcbqOgJCFo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 핵심 포인트\n",
        "    - 전처리의 첫 단계: 모든 NLP 태스크의 시작점\n",
        "    - 언어별 최적화: 한국어는 형태소 분석 필수\n",
        "    - 균형점 찾기: 의미 vs 어휘 크기 vs 계산 효율성\n",
        "    - 특수 토큰: <pad>, <unk>, <start>, <end> 등으로 모델 제어\n",
        "\n",
        "- 비유:\n",
        "    - \"문장을 레고 블록으로 분해하는 것\" - 컴퓨터가 조립할 수 있는 크기로!\n",
        "    - 토큰화는 모든 NLP 모델의 입구이며, 성능에 큰 영향을 미치는 중요한 전처리 과정이다!"
      ],
      "metadata": {
        "id": "rWLg6zh1JTqC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "iOlNvoVKJj7L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformers 토크나이저 사용 예제"
      ],
      "metadata": {
        "id": "VSjsIcSfJkoh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **예제 1: 토크나이제이션 과정 단계별 확인**"
      ],
      "metadata": {
        "id": "sbZjTUHSgJiJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 경고 메시지만 숨기기\n",
        "import warnings\n",
        "import os\n",
        "\n",
        "# 1. 경고 메시지 비활성화\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "# 2. Hugging Face 진행률 표시 끄기\n",
        "os.environ[\"HF_HUB_DISABLE_PROGRESS_BARS\"] = \"1\""
      ],
      "metadata": {
        "id": "HK9BCOKOg6SK"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t_ssqmvCw_-k"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer\n",
        "import torch\n",
        "\n",
        "# 토크나이저 로드\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2') # 무료 공개 모델\n",
        "\n",
        "# 원본 텍스트\n",
        "input_text = \"프로그래밍은 재미있다. 프로그래밍은\"\n",
        "print(f\"✅ 원본 텍스트: '{input_text}'\")\n",
        "print()\n",
        "\n",
        "# 1단계: 토큰으로 분할 (문자열 형태)\n",
        "tokens = tokenizer.tokenize(input_text)\n",
        "print(f\"1️⃣ 토큰 분할 결과: {tokens}\")\n",
        "print()\n",
        "\n",
        "# 2단계: 각 토큰의 ID 확인\n",
        "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "print(f\"2️⃣ 토큰 ID들: {token_ids}\")\n",
        "print()\n",
        "\n",
        "# 3단계: encode 함수로 한번에 처리\n",
        "input_ids_list = tokenizer.encode(input_text)\n",
        "print(f\"3️⃣ encode 결과 (리스트): {input_ids_list}\")\n",
        "print()\n",
        "\n",
        "# 4단계: 텐서 형태로 변환\n",
        "input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
        "print(f\"4️⃣ 텐서 형태로 변환\")\n",
        "print(f\"⭢ input_ids (텐서): {input_ids}\")\n",
        "print(f\"⭢ input_ids 형태: {input_ids.shape}\")\n",
        "print(f\"⭢ input_ids 타입: {type(input_ids)}\")\n",
        "print()\n",
        "\n",
        "# 역변환: ID를 다시 텍스트로\n",
        "decoded_text = tokenizer.decode(input_ids[0])\n",
        "print(f\"✅ 역변환 결과: '{decoded_text}'\")\n"
      ],
      "metadata": {
        "id": "Gk6peed0gYNy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2889dad-b288-4fce-b2ef-587c44440aed"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ 원본 텍스트: '프로그래밍은 재미있다. 프로그래밍은'\n",
            "\n",
            "1️⃣ 토큰 분할 결과: ['í', 'Ķ', 'Ħ', 'ë', '¡', 'ľ', 'ê', '·', '¸', 'ë', 'ŀ', 'ĺ', 'ë', '°', 'į', 'ìĿ', 'Ģ', 'Ġì', 'ŀ', '¬', 'ë', '¯', '¸', 'ì', 'ŀ', 'Ī', 'ëĭ', '¤', '.', 'Ġ', 'í', 'Ķ', 'Ħ', 'ë', '¡', 'ľ', 'ê', '·', '¸', 'ë', 'ŀ', 'ĺ', 'ë', '°', 'į', 'ìĿ', 'Ģ']\n",
            "\n",
            "2️⃣ 토큰 ID들: [169, 242, 226, 167, 94, 250, 166, 115, 116, 167, 252, 246, 167, 108, 235, 35975, 222, 23821, 252, 105, 167, 107, 116, 168, 252, 230, 46695, 97, 13, 220, 169, 242, 226, 167, 94, 250, 166, 115, 116, 167, 252, 246, 167, 108, 235, 35975, 222]\n",
            "\n",
            "3️⃣ encode 결과 (리스트): [169, 242, 226, 167, 94, 250, 166, 115, 116, 167, 252, 246, 167, 108, 235, 35975, 222, 23821, 252, 105, 167, 107, 116, 168, 252, 230, 46695, 97, 13, 220, 169, 242, 226, 167, 94, 250, 166, 115, 116, 167, 252, 246, 167, 108, 235, 35975, 222]\n",
            "\n",
            "4️⃣ 텐서 형태로 변환\n",
            "⭢ input_ids (텐서): tensor([[  169,   242,   226,   167,    94,   250,   166,   115,   116,   167,\n",
            "           252,   246,   167,   108,   235, 35975,   222, 23821,   252,   105,\n",
            "           167,   107,   116,   168,   252,   230, 46695,    97,    13,   220,\n",
            "           169,   242,   226,   167,    94,   250,   166,   115,   116,   167,\n",
            "           252,   246,   167,   108,   235, 35975,   222]])\n",
            "⭢ input_ids 형태: torch.Size([1, 47])\n",
            "⭢ input_ids 타입: <class 'torch.Tensor'>\n",
            "\n",
            "✅ 역변환 결과: '프로그래밍은 재미있다. 프로그래밍은'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **예제 2: 다양한 텍스트의 토크나이제이션 비교**"
      ],
      "metadata": {
        "id": "NPnBbz4XgSHJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "\n",
        "# 다양한 텍스트 예제\n",
        "texts = [\n",
        "    \"안녕하세요\",\n",
        "    \"Hello world\",\n",
        "    \"프로그래밍\",\n",
        "    \"AI는 미래다\",\n",
        "    \"123456\",\n",
        "    \"hello@email.com\"\n",
        "]\n",
        "\n",
        "print(\"=== 다양한 텍스트의 토크나이제이션 결과 ===\")\n",
        "for text in texts:\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    input_ids = tokenizer.encode(text, return_tensors='pt')\n",
        "\n",
        "    print(f\"텍스트: '{text}'\")\n",
        "    print(f\"토큰: {tokens}\")\n",
        "    print(f\"input_ids: {input_ids.tolist()}\")\n",
        "    print(f\"토큰 개수: {len(tokens)}\")\n",
        "    print(\"-\" * 50)"
      ],
      "metadata": {
        "id": "1MBOshvUkB_s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6b7109e-24a8-40e7-a137-fff13187bafb"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== 다양한 텍스트의 토크나이제이션 결과 ===\n",
            "텍스트: '안녕하세요'\n",
            "토큰: ['ì', 'ķ', 'Ī', 'ë', 'ħ', 'ķ', 'íķ', 'ĺ', 'ì', 'Ħ', '¸', 'ì', 'ļ', 'Ķ']\n",
            "input_ids: [[168, 243, 230, 167, 227, 243, 47991, 246, 168, 226, 116, 168, 248, 242]]\n",
            "토큰 개수: 14\n",
            "--------------------------------------------------\n",
            "텍스트: 'Hello world'\n",
            "토큰: ['Hello', 'Ġworld']\n",
            "input_ids: [[15496, 995]]\n",
            "토큰 개수: 2\n",
            "--------------------------------------------------\n",
            "텍스트: '프로그래밍'\n",
            "토큰: ['í', 'Ķ', 'Ħ', 'ë', '¡', 'ľ', 'ê', '·', '¸', 'ë', 'ŀ', 'ĺ', 'ë', '°', 'į']\n",
            "input_ids: [[169, 242, 226, 167, 94, 250, 166, 115, 116, 167, 252, 246, 167, 108, 235]]\n",
            "토큰 개수: 15\n",
            "--------------------------------------------------\n",
            "텍스트: 'AI는 미래다'\n",
            "토큰: ['AI', 'ë', 'Ĭ', 'Ķ', 'Ġë', '¯', '¸', 'ë', 'ŀ', 'ĺ', 'ëĭ', '¤']\n",
            "input_ids: [[20185, 167, 232, 242, 31619, 107, 116, 167, 252, 246, 46695, 97]]\n",
            "토큰 개수: 12\n",
            "--------------------------------------------------\n",
            "텍스트: '123456'\n",
            "토큰: ['123', '456']\n",
            "input_ids: [[10163, 29228]]\n",
            "토큰 개수: 2\n",
            "--------------------------------------------------\n",
            "텍스트: 'hello@email.com'\n",
            "토큰: ['hello', '@', 'email', '.', 'com']\n",
            "input_ids: [[31373, 31, 12888, 13, 785]]\n",
            "토큰 개수: 5\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **예제 3: 토큰 ID의 사용 과정**"
      ],
      "metadata": {
        "id": "eU3ZzrJImaQn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "import torch\n",
        "\n",
        "# 모델과 토크나이저 로드\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "\n",
        "# 원본 텍스트\n",
        "input_text = \"프로그래밍은 재미있다. 프로그래밍은\"\n",
        "print(f\"입력 텍스트: '{input_text}'\")\n",
        "\n",
        "# 토크나이제이션\n",
        "input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
        "print(f\"input_ids: {input_ids}\")\n",
        "print(f\"각 ID가 나타내는 토큰:\")\n",
        "\n",
        "# 각 ID가 무슨 토큰인지 확인\n",
        "for i, token_id in enumerate(input_ids[0]):\n",
        "    token = tokenizer.decode([token_id])\n",
        "    print(f\"  위치 {i}: ID {token_id.item()} → '{token}'\")\n",
        "\n",
        "print()\n",
        "\n",
        "# 모델에 입력하여 다음 토큰 확률 계산\n",
        "with torch.no_grad():\n",
        "    outputs = model(input_ids)\n",
        "    # logits: [배치_크기, 시퀀스_길이, 어휘_크기]\n",
        "    logits = outputs.logits\n",
        "\n",
        "print(f\"모델 출력 형태: {logits.shape}\")\n",
        "print(f\"마지막 토큰 위치의 확률 분포 크기: {logits[0, -1, :].shape}\")\n",
        "\n",
        "# 다음 토큰으로 가능성이 높은 상위 5개 확인\n",
        "last_token_logits = logits[0, -1, :]\n",
        "probabilities = torch.softmax(last_token_logits, dim=-1)\n",
        "top_5_prob, top_5_indices = torch.topk(probabilities, 5)\n",
        "\n",
        "print(\"\\n다음 토큰 예측 상위 5개:\")\n",
        "for i, (prob, idx) in enumerate(zip(top_5_prob, top_5_indices)):\n",
        "    token = tokenizer.decode([idx])\n",
        "    print(f\"{i+1}. '{token}' (확률: {prob:.4f})\")"
      ],
      "metadata": {
        "id": "S-nY1StkmiJZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6744e18c-b38b-4848-f3f3-b308a472267f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "입력 텍스트: '프로그래밍은 재미있다. 프로그래밍은'\n",
            "input_ids: tensor([[  169,   242,   226,   167,    94,   250,   166,   115,   116,   167,\n",
            "           252,   246,   167,   108,   235, 35975,   222, 23821,   252,   105,\n",
            "           167,   107,   116,   168,   252,   230, 46695,    97,    13,   220,\n",
            "           169,   242,   226,   167,    94,   250,   166,   115,   116,   167,\n",
            "           252,   246,   167,   108,   235, 35975,   222]])\n",
            "각 ID가 나타내는 토큰:\n",
            "  위치 0: ID 169 → '�'\n",
            "  위치 1: ID 242 → '�'\n",
            "  위치 2: ID 226 → '�'\n",
            "  위치 3: ID 167 → '�'\n",
            "  위치 4: ID 94 → '�'\n",
            "  위치 5: ID 250 → '�'\n",
            "  위치 6: ID 166 → '�'\n",
            "  위치 7: ID 115 → '�'\n",
            "  위치 8: ID 116 → '�'\n",
            "  위치 9: ID 167 → '�'\n",
            "  위치 10: ID 252 → '�'\n",
            "  위치 11: ID 246 → '�'\n",
            "  위치 12: ID 167 → '�'\n",
            "  위치 13: ID 108 → '�'\n",
            "  위치 14: ID 235 → '�'\n",
            "  위치 15: ID 35975 → '�'\n",
            "  위치 16: ID 222 → '�'\n",
            "  위치 17: ID 23821 → ' �'\n",
            "  위치 18: ID 252 → '�'\n",
            "  위치 19: ID 105 → '�'\n",
            "  위치 20: ID 167 → '�'\n",
            "  위치 21: ID 107 → '�'\n",
            "  위치 22: ID 116 → '�'\n",
            "  위치 23: ID 168 → '�'\n",
            "  위치 24: ID 252 → '�'\n",
            "  위치 25: ID 230 → '�'\n",
            "  위치 26: ID 46695 → '�'\n",
            "  위치 27: ID 97 → '�'\n",
            "  위치 28: ID 13 → '.'\n",
            "  위치 29: ID 220 → ' '\n",
            "  위치 30: ID 169 → '�'\n",
            "  위치 31: ID 242 → '�'\n",
            "  위치 32: ID 226 → '�'\n",
            "  위치 33: ID 167 → '�'\n",
            "  위치 34: ID 94 → '�'\n",
            "  위치 35: ID 250 → '�'\n",
            "  위치 36: ID 166 → '�'\n",
            "  위치 37: ID 115 → '�'\n",
            "  위치 38: ID 116 → '�'\n",
            "  위치 39: ID 167 → '�'\n",
            "  위치 40: ID 252 → '�'\n",
            "  위치 41: ID 246 → '�'\n",
            "  위치 42: ID 167 → '�'\n",
            "  위치 43: ID 108 → '�'\n",
            "  위치 44: ID 235 → '�'\n",
            "  위치 45: ID 35975 → '�'\n",
            "  위치 46: ID 222 → '�'\n",
            "\n",
            "모델 출력 형태: torch.Size([1, 47, 50257])\n",
            "마지막 토큰 위치의 확률 분포 크기: torch.Size([50257])\n",
            "\n",
            "다음 토큰 예측 상위 5개:\n",
            "1. ' �' (확률: 0.8128)\n",
            "2. ' �' (확률: 0.1152)\n",
            "3. ' ' (확률: 0.0436)\n",
            "4. '�' (확률: 0.0058)\n",
            "5. '�' (확률: 0.0029)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **[주의!] 이상한 토큰들의 정체**\n",
        "    - 출력 결과에서 GPT2는 한글을 바이트 단위로 분해 때문에 토큰의 이상하게 보여질 수 있다⭢의미 손실 발생할 수 있다.\n",
        "- 의미 단위로 분해해야 정확함\n",
        "    - ✅ 원본 텍스트: '프로그래밍은 재미있다. 프로그래밍은'\n",
        "    - 1️⃣ 토큰 분할 결과: ['í', 'Ķ', 'Ħ', 'ë', '¡', 'ľ', 'ê', '·', '¸', 'ë', 'ŀ', 'ĺ', 'ë', '°', 'į', 'ìĿ', 'Ģ', 'Ġì', 'ŀ', '¬', 'ë', '¯', '¸', 'ì', 'ŀ', 'Ī', 'ëĭ', '¤', '.', 'Ġ', 'í', 'Ķ', 'Ħ', 'ë', '¡', 'ľ', 'ê', '·', '¸', 'ë', 'ŀ', 'ĺ', 'ë', '°', 'į', 'ìĿ', 'Ģ']\n",
        "- ['í', 'Ķ', 'Ħ', 'ë', '¡', 'ľ' ...] 이 토큰들의 정체\n",
        "\n",
        "|정체|설명|문제점|\n",
        "|--- |--- |--- |\n",
        "|UTF-8 바이트의 잘못된 해석 |한글 바이트를 Latin-1로 디코딩한 결과 |의미 완전 손실 |\n",
        "|BPE 알고리즘의 한계 |영어 위주 학습으로 한글 패턴 미학습 |비효율적 토큰화 |\n",
        "|어휘집 부족 |GPT-2 어휘집에 한글 토큰 거의 없음 |알 수 없는 토큰으로 처리 |"
      ],
      "metadata": {
        "id": "ub9T5C1Sjj14"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1단계: 문제 상황 정확한 분석**"
      ],
      "metadata": {
        "id": "qqqDtMxRkPB3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "\n",
        "# 한글 텍스트\n",
        "korean_text = \"프로그래밍은 재미있다\"\n",
        "print(f\"원본 텍스트: '{korean_text}'\")\n",
        "print()\n",
        "\n",
        "# 토큰 분할 결과\n",
        "tokens = tokenizer.tokenize(korean_text)\n",
        "print(f\"토큰 개수: {len(tokens)}개\")\n",
        "print(f\"토큰들: {tokens[:10]}... (처음 10개만 표시)\")\n",
        "print()\n",
        "\n",
        "# 이상한 문자들의 정체 확인\n",
        "print(\"=== 이상한 토큰들의 정체 ===\")\n",
        "for i, token in enumerate(tokens[:5]):\n",
        "    # 토큰을 바이트로 변환해보기\n",
        "    try:\n",
        "        token_id = tokenizer.convert_tokens_to_ids([token])[0]\n",
        "        print(f\"토큰 {i+1}: '{token}' → ID: {token_id}\")\n",
        "    except:\n",
        "        print(f\"토큰 {i+1}: '{token}' → 변환 불가\")\n",
        "\n",
        "print()\n",
        "\n",
        "# UTF-8 바이트 분석\n",
        "print(\"=== UTF-8 바이트 레벨 분석 ===\")\n",
        "korean_bytes = korean_text.encode('utf-8')\n",
        "print(f\"한글 텍스트의 UTF-8 바이트: {korean_bytes}\")\n",
        "print(f\"바이트 개수: {len(korean_bytes)}개\")\n",
        "\n",
        "# 각 바이트를 개별 문자로 디코딩 시도\n",
        "print(\"바이트별 분석:\")\n",
        "for i, byte_val in enumerate(korean_bytes[:10]):\n",
        "    print(f\"바이트 {i+1}: {byte_val} (0x{byte_val:02x})\")"
      ],
      "metadata": {
        "id": "DjbN4OT2jktt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26755902-f9a4-46d1-f7b9-9536971d4d6d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "원본 텍스트: '프로그래밍은 재미있다'\n",
            "\n",
            "토큰 개수: 28개\n",
            "토큰들: ['í', 'Ķ', 'Ħ', 'ë', '¡', 'ľ', 'ê', '·', '¸', 'ë']... (처음 10개만 표시)\n",
            "\n",
            "=== 이상한 토큰들의 정체 ===\n",
            "토큰 1: 'í' → ID: 169\n",
            "토큰 2: 'Ķ' → ID: 242\n",
            "토큰 3: 'Ħ' → ID: 226\n",
            "토큰 4: 'ë' → ID: 167\n",
            "토큰 5: '¡' → ID: 94\n",
            "\n",
            "=== UTF-8 바이트 레벨 분석 ===\n",
            "한글 텍스트의 UTF-8 바이트: b'\\xed\\x94\\x84\\xeb\\xa1\\x9c\\xea\\xb7\\xb8\\xeb\\x9e\\x98\\xeb\\xb0\\x8d\\xec\\x9d\\x80 \\xec\\x9e\\xac\\xeb\\xaf\\xb8\\xec\\x9e\\x88\\xeb\\x8b\\xa4'\n",
            "바이트 개수: 31개\n",
            "바이트별 분석:\n",
            "바이트 1: 237 (0xed)\n",
            "바이트 2: 148 (0x94)\n",
            "바이트 3: 132 (0x84)\n",
            "바이트 4: 235 (0xeb)\n",
            "바이트 5: 161 (0xa1)\n",
            "바이트 6: 156 (0x9c)\n",
            "바이트 7: 234 (0xea)\n",
            "바이트 8: 183 (0xb7)\n",
            "바이트 9: 184 (0xb8)\n",
            "바이트 10: 235 (0xeb)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2단계: 영어와 한글 토크나이제이션 비교**"
      ],
      "metadata": {
        "id": "MGMcqoTrkRSN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "\n",
        "# 비교 텍스트들\n",
        "texts = {\n",
        "    \"영어\": \"Programming is fun\",\n",
        "    \"한글\": \"프로그래밍은 재미있다\",\n",
        "    \"숫자\": \"12345\",\n",
        "    \"특수문자\": \"Hello! @#$%\"\n",
        "}\n",
        "\n",
        "print(\"=== 언어별 토크나이제이션 비교 ===\")\n",
        "for lang, text in texts.items():\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    token_count = len(tokens)\n",
        "    char_count = len(text)\n",
        "\n",
        "    print(f\"\\n{lang}: '{text}'\")\n",
        "    print(f\"  문자 수: {char_count}\")\n",
        "    print(f\"  토큰 수: {token_count}\")\n",
        "    print(f\"  효율성: {token_count/char_count:.2f} (토큰/문자)\")\n",
        "    print(f\"  토큰 예시: {tokens[:5]}...\")"
      ],
      "metadata": {
        "id": "jzaUiWdekRcD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ffeeb94-698f-4c4f-f286-02a630a8b766"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== 언어별 토크나이제이션 비교 ===\n",
            "\n",
            "영어: 'Programming is fun'\n",
            "  문자 수: 18\n",
            "  토큰 수: 4\n",
            "  효율성: 0.22 (토큰/문자)\n",
            "  토큰 예시: ['Program', 'ming', 'Ġis', 'Ġfun']...\n",
            "\n",
            "한글: '프로그래밍은 재미있다'\n",
            "  문자 수: 11\n",
            "  토큰 수: 28\n",
            "  효율성: 2.55 (토큰/문자)\n",
            "  토큰 예시: ['í', 'Ķ', 'Ħ', 'ë', '¡']...\n",
            "\n",
            "숫자: '12345'\n",
            "  문자 수: 5\n",
            "  토큰 수: 2\n",
            "  효율성: 0.40 (토큰/문자)\n",
            "  토큰 예시: ['123', '45']...\n",
            "\n",
            "특수문자: 'Hello! @#$%'\n",
            "  문자 수: 11\n",
            "  토큰 수: 5\n",
            "  효율성: 0.45 (토큰/문자)\n",
            "  토큰 예시: ['Hello', '!', 'Ġ@', '#$', '%']...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3단계: 올바른 다국어 토크나이저 사용**"
      ],
      "metadata": {
        "id": "h6wnNFbwkRxS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# 다국어 지원 토크나이저들 비교\n",
        "tokenizer_models = {\n",
        "    \"GPT-2 (영어 전용)\": \"gpt2\",\n",
        "    \"mBERT (다국어)\": \"bert-base-multilingual-cased\",\n",
        "    \"XLM-RoBERTa (다국어)\": \"xlm-roberta-base\",\n",
        "    \"KoBERT (한국어)\" : \"skt/kobert-base-v1\",\n",
        "    \"KoGPT2 Base v2 (한국어)\" : \"skt/kogpt2-base-v2\",   # GPT-2 기반\n",
        "    \"Ko-GPT-Trinity 1.2B (v0.5) (한국어)\" : \"skt/ko-gpt-trinity-1.2B-v0.5\"  # GPT-3 스타일\n",
        "}\n",
        "\n",
        "korean_text = \"프로그래밍은 재미있다\"\n",
        "\n",
        "print(\"=== 다양한 토크나이저 비교 ===\")\n",
        "for model_name, model_id in tokenizer_models.items():\n",
        "    try:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "        tokens = tokenizer.tokenize(korean_text)\n",
        "        token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "        print(f\"\\n{model_name}:\")\n",
        "        print(f\"  토큰 수: {len(tokens)}\")\n",
        "        print(f\"  토큰들: {tokens}\")\n",
        "        print(f\"  토큰ID: {token_ids}\")\n",
        "\n",
        "        # 역변환 확인\n",
        "        reconstructed = tokenizer.convert_tokens_to_string(tokens)\n",
        "        print(f\"  역변환: '{reconstructed}'\")\n",
        "        print(f\"  원본과 동일: {'✅' if reconstructed.strip() == korean_text else '❌'}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n{model_name}: 로드 실패 - {e}\")"
      ],
      "metadata": {
        "id": "tra_GeXrkR5A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32503a87-2009-449d-8663-4f9c321309d8"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== 다양한 토크나이저 비교 ===\n",
            "\n",
            "GPT-2 (영어 전용):\n",
            "  토큰 수: 28\n",
            "  토큰들: ['í', 'Ķ', 'Ħ', 'ë', '¡', 'ľ', 'ê', '·', '¸', 'ë', 'ŀ', 'ĺ', 'ë', '°', 'į', 'ìĿ', 'Ģ', 'Ġì', 'ŀ', '¬', 'ë', '¯', '¸', 'ì', 'ŀ', 'Ī', 'ëĭ', '¤']\n",
            "  토큰ID: [169, 242, 226, 167, 94, 250, 166, 115, 116, 167, 252, 246, 167, 108, 235, 35975, 222, 23821, 252, 105, 167, 107, 116, 168, 252, 230, 46695, 97]\n",
            "  역변환: '프로그래밍은 재미있다'\n",
            "  원본과 동일: ✅\n",
            "\n",
            "mBERT (다국어):\n",
            "  토큰 수: 8\n",
            "  토큰들: ['프로', '##그', '##래', '##밍', '##은', '재', '##미', '##있다']\n",
            "  토큰ID: [102574, 78136, 37388, 118960, 10892, 9659, 22458, 76820]\n",
            "  역변환: '프로그래밍은 재미있다'\n",
            "  원본과 동일: ✅\n",
            "\n",
            "XLM-RoBERTa (다국어):\n",
            "  토큰 수: 6\n",
            "  토큰들: ['▁프로', '그래', '밍', '은', '▁재미있', '다']\n",
            "  토큰ID: [54099, 61882, 144667, 697, 157403, 1875]\n",
            "  역변환: '프로그래밍은 재미있다'\n",
            "  원본과 동일: ✅\n",
            "\n",
            "KoBERT (한국어):\n",
            "  토큰 수: 14\n",
            "  토큰들: ['▁', '프로', 'ᄀ', 'ᅳ래ᄆ', 'ᅵ', 'ᆼ', 'ᄋ', 'ᅳᆫ', '▁', '재ᄆ', 'ᅵ', 'ᄋ', 'ᅵ', 'ᆻ다']\n",
            "  토큰ID: [517, 0, 490, 0, 494, 0, 491, 0, 517, 0, 494, 491, 494, 0]\n",
            "  역변환: '프로그래밍은 재미있다'\n",
            "  원본과 동일: ❌\n",
            "\n",
            "KoGPT2 Base v2 (한국어):\n",
            "  토큰 수: 7\n",
            "  토큰들: ['▁프로', '그래', '밍', '은', '▁재미', '있', '다']\n",
            "  토큰ID: [9726, 19561, 7593, 8135, 18767, 8155, 7182]\n",
            "  역변환: '프로그래밍은 재미있다'\n",
            "  원본과 동일: ✅\n",
            "\n",
            "Ko-GPT-Trinity 1.2B (v0.5) (한국어):\n",
            "  토큰 수: 5\n",
            "  토큰들: ['▁프로그래', '밍', '은', '▁재미', '있다']\n",
            "  토큰ID: [49017, 22901, 25768, 32579, 45092]\n",
            "  역변환: '프로그래밍은 재미있다'\n",
            "  원본과 동일: ✅\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 한글 처리를 위한 올바른 선택\n",
        "\n",
        "|용도 |권장 모델 |이유\n",
        "|--- |--- |--- |\n",
        "| 한글 텍스트 생성| GPT-3.5/4, KoGPT| 한글 데이터로 훈련됨|\n",
        "| 한글 이해/분류| KoBERT, KoELECTRA| 한국어 특화 모델|\n",
        "| 다국어 처리| mBERT, XLM-RoBERTa| 다국어 동시 지원|\n",
        "| 실습/학습용| 영어 예제 사용| GPT-2 본래 성능 확인|"
      ],
      "metadata": {
        "id": "P0_mx4WXldNe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 학습용 개선된 예제"
      ],
      "metadata": {
        "id": "Aj62TQvZl7G5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 올바른 접근: 영어로 실습하기\n",
        "from transformers import GPT2Tokenizer\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "\n",
        "# GPT-2가 잘 처리하는 영어 텍스트\n",
        "english_text = \"Programming is fun. Programming is\"\n",
        "print(f\"영어 텍스트: '{english_text}'\")\n",
        "\n",
        "tokens = tokenizer.tokenize(english_text)\n",
        "print(f\"토큰들: {tokens}\")\n",
        "print(f\"토큰 수: {len(tokens)}\")\n",
        "\n",
        "# 각 토큰의 의미 확인\n",
        "print(\"\\n토큰별 의미:\")\n",
        "for i, token in enumerate(tokens):\n",
        "    print(f\"{i+1}. '{token}' → 의미있는 단위\")"
      ],
      "metadata": {
        "id": "0TeAXjxwnUQq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17f0eb48-08c1-41df-9917-fc128bf0240b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "영어 텍스트: 'Programming is fun. Programming is'\n",
            "토큰들: ['Program', 'ming', 'Ġis', 'Ġfun', '.', 'ĠProgramming', 'Ġis']\n",
            "토큰 수: 7\n",
            "\n",
            "토큰별 의미:\n",
            "1. 'Program' → 의미있는 단위\n",
            "2. 'ming' → 의미있는 단위\n",
            "3. 'Ġis' → 의미있는 단위\n",
            "4. 'Ġfun' → 의미있는 단위\n",
            "5. '.' → 의미있는 단위\n",
            "6. 'ĠProgramming' → 의미있는 단위\n",
            "7. 'Ġis' → 의미있는 단위\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "IeP3BFVJf-00"
      }
    }
  ]
}
