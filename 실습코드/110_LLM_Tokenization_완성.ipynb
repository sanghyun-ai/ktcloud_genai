{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanghyun-ai/ktcloud_genai/blob/main/110_LLM_Tokenization_%E1%84%8B%E1%85%AA%E1%86%AB%E1%84%89%E1%85%A5%E1%86%BC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Tokenization**"
      ],
      "metadata": {
        "id": "_intVJo1FXQF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "- ğŸ’¡ **NOTE**\n",
        "    - ì´ ë…¸íŠ¸ë¶ì˜ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ë ¤ë©´ GPUë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤. êµ¬ê¸€ ì½”ë©ì—ì„œëŠ” **ëŸ°íƒ€ì„ > ëŸ°íƒ€ì„ ìœ í˜• ë³€ê²½ > í•˜ë“œì›¨ì–´ ê°€ì†ê¸° > T4 GPU**ë¥¼ ì„ íƒí•˜ì„¸ìš”.\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "81_Ybs4LI7IX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Tokenization ì´ë€?**"
      ],
      "metadata": {
        "id": "X8ya0DICf_mC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- í…ìŠ¤íŠ¸ë¥¼ ì»´í“¨í„°ê°€ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” ì˜ë¯¸ìˆëŠ” ë‹¨ìœ„(í† í°)ë¡œ ë‚˜ëˆ„ëŠ” ì „ì²˜ë¦¬ ê³¼ì •\n",
        "- ê¸°ë³¸ ì•„ì´ë””ì–´\n",
        "    - í…ìŠ¤íŠ¸ â†’ í† í° ë¦¬ìŠ¤íŠ¸ ë³€í™˜\n",
        "    - AI ëª¨ë¸ì´ ì´í•´í•  ìˆ˜ ìˆëŠ” ìµœì†Œ ì˜ë¯¸ ë‹¨ìœ„(=í† í°)ë¡œ ë¶„í• \n",
        "    - ì–¸ì–´ë§ˆë‹¤ ë‹¤ë¥¸ ë°©ì‹ ì ìš©\n",
        "- í† í°í™” ë°©ë²•\n",
        "\n",
        "|ë°©ë²•|ì„¤ëª…|ì˜ˆì‹œ|ì¥ì |ë‹¨ì |\n",
        "|---|---|---|---|---|\n",
        "|ë‹¨ì–´ ë‹¨ìœ„|ê³µë°±ìœ¼ë¡œ ë¶„í• \"|I love AI\" â†’ [\"I\", \"love\", \"AI\"]|ì§ê´€ì ì–´íœ˜ í¬ê¸° í­ë°œ|\n",
        "|ë¬¸ì ë‹¨ìœ„|ê¸€ìë³„ ë¶„í• |\"Hello\" â†’ [\"H\", \"e\", \"l\", \"l\", \"o\"]|ì–´íœ˜ í¬ê¸° ì‘ìŒ|ì˜ë¯¸ ì •ë³´ ì†ì‹¤|\n",
        "|ì„œë¸Œì›Œë“œ|ë‹¨ì–´ë¥¼ ë” ì‘ì€ ë‹¨ìœ„ë¡œ|\"unknown\" â†’ [\"un\", \"know\", \"n\"]|ê· í˜•ì¡íŒ ì ‘ê·¼|ë³µì¡í•œ ì•Œê³ ë¦¬ì¦˜|\n",
        "|í˜•íƒœì†Œ ë¶„ì„|ì˜ë¯¸ ë‹¨ìœ„ë¡œ ë¶„í• |\"ë¨¹ì—ˆë‹¤\" â†’ [\"ë¨¹\", \"ì—ˆ\", \"ë‹¤\"]|ì–¸ì–´í•™ì  ì •í™•ì„±|ì–¸ì–´ë³„ íŠ¹í™” í•„ìš”|"
      ],
      "metadata": {
        "id": "F2_OuRR6LLCG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ì˜ˆì œ 1: ê¸°ë³¸ í† í°í™” ë°©ë²•ë“¤"
      ],
      "metadata": {
        "id": "_m2T0NYhIG8H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "def basic_tokenization_examples():\n",
        "    text = \"Hello, World! I'm learning NLP. It's amazing!\"\n",
        "\n",
        "    print(\"=== ê¸°ë³¸ í† í°í™” ë°©ë²•ë“¤ ===\")\n",
        "    print(f\"ì›ë³¸ í…ìŠ¤íŠ¸: {text}\")\n",
        "\n",
        "    # 1. ê³µë°± ê¸°ë°˜ ë¶„í• \n",
        "    whitespace_tokens = text.split()\n",
        "    print(f\"\\n1. ê³µë°± ë¶„í• : {whitespace_tokens}\")\n",
        "\n",
        "    # 2. ì •ê·œì‹ìœ¼ë¡œ ë‹¨ì–´ ì¶”ì¶œ\n",
        "    word_tokens = re.findall(r'\\w+', text.lower())\n",
        "    print(f\"2. ë‹¨ì–´ë§Œ ì¶”ì¶œ: {word_tokens}\")\n",
        "\n",
        "    # 3. ë¬¸ì ë‹¨ìœ„ ë¶„í• \n",
        "    char_tokens = list(text.lower())\n",
        "    print(f\"3. ë¬¸ì ë¶„í• : {char_tokens}\")\n",
        "\n",
        "    # 4. êµ¬ë‘ì  í¬í•¨ ë¶„í• \n",
        "    punct_tokens = re.findall(r'\\w+|[^\\w\\s]', text)\n",
        "    print(f\"4. êµ¬ë‘ì  í¬í•¨: {punct_tokens}\")\n",
        "\n",
        "basic_tokenization_examples()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eH2TnXbxIHGx",
        "outputId": "5fca33f3-997a-488e-de87-9be7240ca180"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== ê¸°ë³¸ í† í°í™” ë°©ë²•ë“¤ ===\n",
            "ì›ë³¸ í…ìŠ¤íŠ¸: Hello, World! I'm learning NLP. It's amazing!\n",
            "\n",
            "1. ê³µë°± ë¶„í• : ['Hello,', 'World!', \"I'm\", 'learning', 'NLP.', \"It's\", 'amazing!']\n",
            "2. ë‹¨ì–´ë§Œ ì¶”ì¶œ: ['hello', 'world', 'i', 'm', 'learning', 'nlp', 'it', 's', 'amazing']\n",
            "3. ë¬¸ì ë¶„í• : ['h', 'e', 'l', 'l', 'o', ',', ' ', 'w', 'o', 'r', 'l', 'd', '!', ' ', 'i', \"'\", 'm', ' ', 'l', 'e', 'a', 'r', 'n', 'i', 'n', 'g', ' ', 'n', 'l', 'p', '.', ' ', 'i', 't', \"'\", 's', ' ', 'a', 'm', 'a', 'z', 'i', 'n', 'g', '!']\n",
            "4. êµ¬ë‘ì  í¬í•¨: ['Hello', ',', 'World', '!', 'I', \"'\", 'm', 'learning', 'NLP', '.', 'It', \"'\", 's', 'amazing', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ì˜ˆì œ 2: í•œêµ­ì–´ í† í°í™”"
      ],
      "metadata": {
        "id": "ZEq8FkV8IOEy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def korean_tokenization():\n",
        "    korean_text = \"ì•ˆë…•í•˜ì„¸ìš”! ì €ëŠ” í•œêµ­ì–´ë¥¼ ê³µë¶€í•˜ê³  ìˆìŠµë‹ˆë‹¤.\"\n",
        "\n",
        "    print(\"\\n=== í•œêµ­ì–´ í† í°í™” ===\")\n",
        "    print(f\"ì›ë³¸ í…ìŠ¤íŠ¸: {korean_text}\")\n",
        "\n",
        "    # 1. ê³µë°± ê¸°ë°˜ (í•œêµ­ì–´ëŠ” ë¶€ì •í™•)\n",
        "    space_tokens = korean_text.split()\n",
        "    print(f\"1. ê³µë°± ë¶„í• : {space_tokens}\")\n",
        "\n",
        "    # 2. ë¬¸ì ë‹¨ìœ„\n",
        "    char_tokens = list(korean_text)\n",
        "    print(f\"2. ë¬¸ì ë¶„í• : {char_tokens}\")\n",
        "\n",
        "    # 3. ê°„ë‹¨í•œ í•œêµ­ì–´ í† í°í™” (ì‹¤ì œë¡œëŠ” í˜•íƒœì†Œ ë¶„ì„ê¸° ì‚¬ìš©)\n",
        "    # ì—¬ê¸°ì„œëŠ” ì–´ì ˆ ë‹¨ìœ„ë¡œ ë¶„ë¦¬\n",
        "    simple_tokens = re.findall(r'[ê°€-í£]+|[a-zA-Z]+|[!?.]', korean_text)\n",
        "    print(f\"3. ê°„ë‹¨í•œ ë¶„í• : {simple_tokens}\")\n",
        "\n",
        "korean_tokenization()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GF_FDBu-IP9I",
        "outputId": "9d6b1ba7-5813-49c0-ba25-fa92e77c52f2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== í•œêµ­ì–´ í† í°í™” ===\n",
            "ì›ë³¸ í…ìŠ¤íŠ¸: ì•ˆë…•í•˜ì„¸ìš”! ì €ëŠ” í•œêµ­ì–´ë¥¼ ê³µë¶€í•˜ê³  ìˆìŠµë‹ˆë‹¤.\n",
            "1. ê³µë°± ë¶„í• : ['ì•ˆë…•í•˜ì„¸ìš”!', 'ì €ëŠ”', 'í•œêµ­ì–´ë¥¼', 'ê³µë¶€í•˜ê³ ', 'ìˆìŠµë‹ˆë‹¤.']\n",
            "2. ë¬¸ì ë¶„í• : ['ì•ˆ', 'ë…•', 'í•˜', 'ì„¸', 'ìš”', '!', ' ', 'ì €', 'ëŠ”', ' ', 'í•œ', 'êµ­', 'ì–´', 'ë¥¼', ' ', 'ê³µ', 'ë¶€', 'í•˜', 'ê³ ', ' ', 'ìˆ', 'ìŠµ', 'ë‹ˆ', 'ë‹¤', '.']\n",
            "3. ê°„ë‹¨í•œ ë¶„í• : ['ì•ˆë…•í•˜ì„¸ìš”', '!', 'ì €ëŠ”', 'í•œêµ­ì–´ë¥¼', 'ê³µë¶€í•˜ê³ ', 'ìˆìŠµë‹ˆë‹¤', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ì˜ˆì œ 3: ì„œë¸Œì›Œë“œ í† í°í™” ì‹œë®¬ë ˆì´ì…˜"
      ],
      "metadata": {
        "id": "8aWpo7ejISlq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def subword_tokenization_demo():\n",
        "    words = [\"unknown\", \"unhappy\", \"replay\", \"preprocessing\"]\n",
        "\n",
        "    print(\"\\n=== ì„œë¸Œì›Œë“œ í† í°í™” ë°ëª¨ ===\")\n",
        "\n",
        "    # ê°„ë‹¨í•œ BPE ì‹œë®¬ë ˆì´ì…˜\n",
        "    vocab = set()\n",
        "\n",
        "    # ëª¨ë“  ë¬¸ìë¥¼ ê¸°ë³¸ ì–´íœ˜ì— ì¶”ê°€\n",
        "    for word in words:\n",
        "        vocab.update(list(word))\n",
        "\n",
        "    # ìì£¼ ë‚˜ì˜¤ëŠ” ë¬¸ì ì¡°í•© ì°¾ê¸°\n",
        "    bigrams = Counter()\n",
        "    for word in words:\n",
        "        for i in range(len(word) - 1):\n",
        "            bigrams[word[i:i+2]] += 1\n",
        "\n",
        "    # ê°€ì¥ ë¹ˆë²ˆí•œ ë°”ì´ê·¸ë¨ì„ ì–´íœ˜ì— ì¶”ê°€\n",
        "    common_bigrams = bigrams.most_common(5)\n",
        "    for bigram, count in common_bigrams:\n",
        "        vocab.add(bigram)\n",
        "        print(f\"ì¶”ê°€ëœ ì„œë¸Œì›Œë“œ: '{bigram}' (ë¹ˆë„: {count})\")\n",
        "\n",
        "    print(f\"\\nìµœì¢… ì–´íœ˜: {sorted(vocab)}\")\n",
        "\n",
        "    # ë‹¨ì–´ë¥¼ ì„œë¸Œì›Œë“œë¡œ ë¶„í•´ (ë‹¨ìˆœí™”ëœ ë²„ì „)\n",
        "    def tokenize_word(word):\n",
        "        tokens = []\n",
        "        i = 0\n",
        "        while i < len(word):\n",
        "            # ê°€ì¥ ê¸´ ë§¤ì¹­ ì°¾ê¸°\n",
        "            found = False\n",
        "            for length in range(min(3, len(word) - i), 0, -1):\n",
        "                subword = word[i:i+length]\n",
        "                if subword in vocab:\n",
        "                    tokens.append(subword)\n",
        "                    i += length\n",
        "                    found = True\n",
        "                    break\n",
        "            if not found:\n",
        "                tokens.append(word[i])\n",
        "                i += 1\n",
        "        return tokens\n",
        "\n",
        "    print(f\"\\n=== ì„œë¸Œì›Œë“œ í† í°í™” ê²°ê³¼ ===\")\n",
        "    for word in words:\n",
        "        tokens = tokenize_word(word)\n",
        "        print(f\"{word:15} â†’ {tokens}\")\n",
        "\n",
        "subword_tokenization_demo()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7MF_j6FjITvt",
        "outputId": "ae31bac5-c26c-426a-eac2-058eca0a5ac6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== ì„œë¸Œì›Œë“œ í† í°í™” ë°ëª¨ ===\n",
            "ì¶”ê°€ëœ ì„œë¸Œì›Œë“œ: 'un' (ë¹ˆë„: 2)\n",
            "ì¶”ê°€ëœ ì„œë¸Œì›Œë“œ: 're' (ë¹ˆë„: 2)\n",
            "ì¶”ê°€ëœ ì„œë¸Œì›Œë“œ: 'ep' (ë¹ˆë„: 2)\n",
            "ì¶”ê°€ëœ ì„œë¸Œì›Œë“œ: 'pr' (ë¹ˆë„: 2)\n",
            "ì¶”ê°€ëœ ì„œë¸Œì›Œë“œ: 'nk' (ë¹ˆë„: 1)\n",
            "\n",
            "ìµœì¢… ì–´íœ˜: ['a', 'c', 'e', 'ep', 'g', 'h', 'i', 'k', 'l', 'n', 'nk', 'o', 'p', 'pr', 'r', 're', 's', 'u', 'un', 'w', 'y']\n",
            "\n",
            "=== ì„œë¸Œì›Œë“œ í† í°í™” ê²°ê³¼ ===\n",
            "unknown         â†’ ['un', 'k', 'n', 'o', 'w', 'n']\n",
            "unhappy         â†’ ['un', 'h', 'a', 'p', 'p', 'y']\n",
            "replay          â†’ ['re', 'p', 'l', 'a', 'y']\n",
            "preprocessing   â†’ ['pr', 'ep', 'r', 'o', 'c', 'e', 's', 's', 'i', 'n', 'g']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ì˜ˆì œ 4: ì‹¤ì œ AI ëª¨ë¸ì—ì„œ ì‚¬ìš©í•˜ëŠ” í† í°í™”"
      ],
      "metadata": {
        "id": "WuAIVwlRIcEG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ai_model_tokenization():\n",
        "    sentences = [\n",
        "        \"I love artificial intelligence!\",\n",
        "        \"Machine learning is fascinating.\",\n",
        "        \"Natural language processing rocks!\"\n",
        "    ]\n",
        "\n",
        "    print(\"\\n=== AI ëª¨ë¸ìš© í† í°í™” ===\")\n",
        "\n",
        "    # 1. ì–´íœ˜ ì‚¬ì „ êµ¬ì¶•\n",
        "    word_counter = Counter()\n",
        "    for sentence in sentences:\n",
        "        words = re.findall(r'\\w+', sentence.lower())\n",
        "        word_counter.update(words)\n",
        "\n",
        "    # 2. íŠ¹ìˆ˜ í† í° ì¶”ê°€\n",
        "    vocab = {\n",
        "        '<pad>': 0,    # íŒ¨ë”©\n",
        "        '<unk>': 1,    # ë¯¸ì§€ ë‹¨ì–´\n",
        "        '<start>': 2,  # ì‹œì‘\n",
        "        '<end>': 3     # ë\n",
        "    }\n",
        "\n",
        "    # 3. ë¹ˆë²ˆí•œ ë‹¨ì–´ë“¤ ì¶”ê°€\n",
        "    for word, count in word_counter.most_common():\n",
        "        if count >= 1:  # ìµœì†Œ ë¹ˆë„\n",
        "            vocab[word] = len(vocab)\n",
        "\n",
        "    print(f\"ì–´íœ˜ ì‚¬ì „ í¬ê¸°: {len(vocab)}\")\n",
        "    print(f\"ì–´íœ˜: {list(vocab.keys())}\")\n",
        "\n",
        "    # 4. ë¬¸ì¥ì„ í† í° IDë¡œ ë³€í™˜\n",
        "    def sentence_to_ids(sentence, vocab, max_len=10):\n",
        "        words = re.findall(r'\\w+', sentence.lower())\n",
        "        ids = [vocab['<start>']]\n",
        "\n",
        "        for word in words:\n",
        "            if len(ids) >= max_len - 1:  # <end> ê³µê°„ í™•ë³´\n",
        "                break\n",
        "            ids.append(vocab.get(word, vocab['<unk>']))\n",
        "\n",
        "        ids.append(vocab['<end>'])\n",
        "\n",
        "        # íŒ¨ë”©\n",
        "        while len(ids) < max_len:\n",
        "            ids.append(vocab['<pad>'])\n",
        "\n",
        "        return ids[:max_len]\n",
        "\n",
        "    print(f\"\\n=== í† í° ID ë³€í™˜ ===\")\n",
        "    for sentence in sentences:\n",
        "        token_ids = sentence_to_ids(sentence, vocab)\n",
        "        print(f\"'{sentence}'\")\n",
        "        print(f\"í† í° ID: {token_ids}\")\n",
        "\n",
        "        # ì—­ë³€í™˜ìœ¼ë¡œ í™•ì¸\n",
        "        id_to_vocab = {v: k for k, v in vocab.items()}\n",
        "        recovered_tokens = [id_to_vocab[id] for id in token_ids]\n",
        "        print(f\"í† í°: {recovered_tokens}\")\n",
        "        print()\n",
        "\n",
        "ai_model_tokenization()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-hafV4SlIdFQ",
        "outputId": "ea7f2f9b-ff7f-424e-a3fc-9c97dffe12e7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== AI ëª¨ë¸ìš© í† í°í™” ===\n",
            "ì–´íœ˜ ì‚¬ì „ í¬ê¸°: 16\n",
            "ì–´íœ˜: ['<pad>', '<unk>', '<start>', '<end>', 'i', 'love', 'artificial', 'intelligence', 'machine', 'learning', 'is', 'fascinating', 'natural', 'language', 'processing', 'rocks']\n",
            "\n",
            "=== í† í° ID ë³€í™˜ ===\n",
            "'I love artificial intelligence!'\n",
            "í† í° ID: [2, 4, 5, 6, 7, 3, 0, 0, 0, 0]\n",
            "í† í°: ['<start>', 'i', 'love', 'artificial', 'intelligence', '<end>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
            "\n",
            "'Machine learning is fascinating.'\n",
            "í† í° ID: [2, 8, 9, 10, 11, 3, 0, 0, 0, 0]\n",
            "í† í°: ['<start>', 'machine', 'learning', 'is', 'fascinating', '<end>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
            "\n",
            "'Natural language processing rocks!'\n",
            "í† í° ID: [2, 12, 13, 14, 15, 3, 0, 0, 0, 0]\n",
            "í† í°: ['<start>', 'natural', 'language', 'processing', 'rocks', '<end>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "qOWBFhLLI3ZD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# í˜„ëŒ€ AIì—ì„œ ì‚¬ìš©í•˜ëŠ” í† í¬ë‚˜ì´ì €\n"
      ],
      "metadata": {
        "id": "zMbMTp09I71r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "|í† í¬ë‚˜ì´ì €|ì‚¬ìš© ëª¨ë¸|íŠ¹ì§•\n",
        "|---|---|---|\n",
        "|BPE|GPT ì‹œë¦¬ì¦ˆ|ë°”ì´íŠ¸ ìŒ ì¸ì½”ë”©|\n",
        "|SentencePiece|T5, mT5|ì–¸ì–´ ë¬´ê´€í•œ ì„œë¸Œì›Œë“œ|\n",
        "|WordPiece|BERT|êµ¬ê¸€ ê°œë°œ, í•œêµ­ì–´ ì§€ì›|"
      ],
      "metadata": {
        "id": "-sTcbqOgJCFo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- í•µì‹¬ í¬ì¸íŠ¸\n",
        "    - ì „ì²˜ë¦¬ì˜ ì²« ë‹¨ê³„: ëª¨ë“  NLP íƒœìŠ¤í¬ì˜ ì‹œì‘ì \n",
        "    - ì–¸ì–´ë³„ ìµœì í™”: í•œêµ­ì–´ëŠ” í˜•íƒœì†Œ ë¶„ì„ í•„ìˆ˜\n",
        "    - ê· í˜•ì  ì°¾ê¸°: ì˜ë¯¸ vs ì–´íœ˜ í¬ê¸° vs ê³„ì‚° íš¨ìœ¨ì„±\n",
        "    - íŠ¹ìˆ˜ í† í°: <pad>, <unk>, <start>, <end> ë“±ìœ¼ë¡œ ëª¨ë¸ ì œì–´\n",
        "\n",
        "- ë¹„ìœ :\n",
        "    - \"ë¬¸ì¥ì„ ë ˆê³  ë¸”ë¡ìœ¼ë¡œ ë¶„í•´í•˜ëŠ” ê²ƒ\" - ì»´í“¨í„°ê°€ ì¡°ë¦½í•  ìˆ˜ ìˆëŠ” í¬ê¸°ë¡œ!\n",
        "    - í† í°í™”ëŠ” ëª¨ë“  NLP ëª¨ë¸ì˜ ì…êµ¬ì´ë©°, ì„±ëŠ¥ì— í° ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” ì¤‘ìš”í•œ ì „ì²˜ë¦¬ ê³¼ì •ì´ë‹¤!"
      ],
      "metadata": {
        "id": "rWLg6zh1JTqC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "iOlNvoVKJj7L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformers í† í¬ë‚˜ì´ì € ì‚¬ìš© ì˜ˆì œ"
      ],
      "metadata": {
        "id": "VSjsIcSfJkoh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **ì˜ˆì œ 1: í† í¬ë‚˜ì´ì œì´ì…˜ ê³¼ì • ë‹¨ê³„ë³„ í™•ì¸**"
      ],
      "metadata": {
        "id": "sbZjTUHSgJiJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ê²½ê³  ë©”ì‹œì§€ë§Œ ìˆ¨ê¸°ê¸°\n",
        "import warnings\n",
        "import os\n",
        "\n",
        "# 1. ê²½ê³  ë©”ì‹œì§€ ë¹„í™œì„±í™”\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "# 2. Hugging Face ì§„í–‰ë¥  í‘œì‹œ ë„ê¸°\n",
        "os.environ[\"HF_HUB_DISABLE_PROGRESS_BARS\"] = \"1\""
      ],
      "metadata": {
        "id": "HK9BCOKOg6SK"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t_ssqmvCw_-k"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer\n",
        "import torch\n",
        "\n",
        "# í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2') # ë¬´ë£Œ ê³µê°œ ëª¨ë¸\n",
        "\n",
        "# ì›ë³¸ í…ìŠ¤íŠ¸\n",
        "input_text = \"í”„ë¡œê·¸ë˜ë°ì€ ì¬ë¯¸ìˆë‹¤. í”„ë¡œê·¸ë˜ë°ì€\"\n",
        "print(f\"âœ… ì›ë³¸ í…ìŠ¤íŠ¸: '{input_text}'\")\n",
        "print()\n",
        "\n",
        "# 1ë‹¨ê³„: í† í°ìœ¼ë¡œ ë¶„í•  (ë¬¸ìì—´ í˜•íƒœ)\n",
        "tokens = tokenizer.tokenize(input_text)\n",
        "print(f\"1ï¸âƒ£ í† í° ë¶„í•  ê²°ê³¼: {tokens}\")\n",
        "print()\n",
        "\n",
        "# 2ë‹¨ê³„: ê° í† í°ì˜ ID í™•ì¸\n",
        "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "print(f\"2ï¸âƒ£ í† í° IDë“¤: {token_ids}\")\n",
        "print()\n",
        "\n",
        "# 3ë‹¨ê³„: encode í•¨ìˆ˜ë¡œ í•œë²ˆì— ì²˜ë¦¬\n",
        "input_ids_list = tokenizer.encode(input_text)\n",
        "print(f\"3ï¸âƒ£ encode ê²°ê³¼ (ë¦¬ìŠ¤íŠ¸): {input_ids_list}\")\n",
        "print()\n",
        "\n",
        "# 4ë‹¨ê³„: í…ì„œ í˜•íƒœë¡œ ë³€í™˜\n",
        "input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
        "print(f\"4ï¸âƒ£ í…ì„œ í˜•íƒœë¡œ ë³€í™˜\")\n",
        "print(f\"â­¢ input_ids (í…ì„œ): {input_ids}\")\n",
        "print(f\"â­¢ input_ids í˜•íƒœ: {input_ids.shape}\")\n",
        "print(f\"â­¢ input_ids íƒ€ì…: {type(input_ids)}\")\n",
        "print()\n",
        "\n",
        "# ì—­ë³€í™˜: IDë¥¼ ë‹¤ì‹œ í…ìŠ¤íŠ¸ë¡œ\n",
        "decoded_text = tokenizer.decode(input_ids[0])\n",
        "print(f\"âœ… ì—­ë³€í™˜ ê²°ê³¼: '{decoded_text}'\")\n"
      ],
      "metadata": {
        "id": "Gk6peed0gYNy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2889dad-b288-4fce-b2ef-587c44440aed"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… ì›ë³¸ í…ìŠ¤íŠ¸: 'í”„ë¡œê·¸ë˜ë°ì€ ì¬ë¯¸ìˆë‹¤. í”„ë¡œê·¸ë˜ë°ì€'\n",
            "\n",
            "1ï¸âƒ£ í† í° ë¶„í•  ê²°ê³¼: ['Ã­', 'Ä¶', 'Ä¦', 'Ã«', 'Â¡', 'Ä¾', 'Ãª', 'Â·', 'Â¸', 'Ã«', 'Å€', 'Äº', 'Ã«', 'Â°', 'Ä¯', 'Ã¬Ä¿', 'Ä¢', 'Ä Ã¬', 'Å€', 'Â¬', 'Ã«', 'Â¯', 'Â¸', 'Ã¬', 'Å€', 'Äª', 'Ã«Ä­', 'Â¤', '.', 'Ä ', 'Ã­', 'Ä¶', 'Ä¦', 'Ã«', 'Â¡', 'Ä¾', 'Ãª', 'Â·', 'Â¸', 'Ã«', 'Å€', 'Äº', 'Ã«', 'Â°', 'Ä¯', 'Ã¬Ä¿', 'Ä¢']\n",
            "\n",
            "2ï¸âƒ£ í† í° IDë“¤: [169, 242, 226, 167, 94, 250, 166, 115, 116, 167, 252, 246, 167, 108, 235, 35975, 222, 23821, 252, 105, 167, 107, 116, 168, 252, 230, 46695, 97, 13, 220, 169, 242, 226, 167, 94, 250, 166, 115, 116, 167, 252, 246, 167, 108, 235, 35975, 222]\n",
            "\n",
            "3ï¸âƒ£ encode ê²°ê³¼ (ë¦¬ìŠ¤íŠ¸): [169, 242, 226, 167, 94, 250, 166, 115, 116, 167, 252, 246, 167, 108, 235, 35975, 222, 23821, 252, 105, 167, 107, 116, 168, 252, 230, 46695, 97, 13, 220, 169, 242, 226, 167, 94, 250, 166, 115, 116, 167, 252, 246, 167, 108, 235, 35975, 222]\n",
            "\n",
            "4ï¸âƒ£ í…ì„œ í˜•íƒœë¡œ ë³€í™˜\n",
            "â­¢ input_ids (í…ì„œ): tensor([[  169,   242,   226,   167,    94,   250,   166,   115,   116,   167,\n",
            "           252,   246,   167,   108,   235, 35975,   222, 23821,   252,   105,\n",
            "           167,   107,   116,   168,   252,   230, 46695,    97,    13,   220,\n",
            "           169,   242,   226,   167,    94,   250,   166,   115,   116,   167,\n",
            "           252,   246,   167,   108,   235, 35975,   222]])\n",
            "â­¢ input_ids í˜•íƒœ: torch.Size([1, 47])\n",
            "â­¢ input_ids íƒ€ì…: <class 'torch.Tensor'>\n",
            "\n",
            "âœ… ì—­ë³€í™˜ ê²°ê³¼: 'í”„ë¡œê·¸ë˜ë°ì€ ì¬ë¯¸ìˆë‹¤. í”„ë¡œê·¸ë˜ë°ì€'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **ì˜ˆì œ 2: ë‹¤ì–‘í•œ í…ìŠ¤íŠ¸ì˜ í† í¬ë‚˜ì´ì œì´ì…˜ ë¹„êµ**"
      ],
      "metadata": {
        "id": "NPnBbz4XgSHJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "\n",
        "# ë‹¤ì–‘í•œ í…ìŠ¤íŠ¸ ì˜ˆì œ\n",
        "texts = [\n",
        "    \"ì•ˆë…•í•˜ì„¸ìš”\",\n",
        "    \"Hello world\",\n",
        "    \"í”„ë¡œê·¸ë˜ë°\",\n",
        "    \"AIëŠ” ë¯¸ë˜ë‹¤\",\n",
        "    \"123456\",\n",
        "    \"hello@email.com\"\n",
        "]\n",
        "\n",
        "print(\"=== ë‹¤ì–‘í•œ í…ìŠ¤íŠ¸ì˜ í† í¬ë‚˜ì´ì œì´ì…˜ ê²°ê³¼ ===\")\n",
        "for text in texts:\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    input_ids = tokenizer.encode(text, return_tensors='pt')\n",
        "\n",
        "    print(f\"í…ìŠ¤íŠ¸: '{text}'\")\n",
        "    print(f\"í† í°: {tokens}\")\n",
        "    print(f\"input_ids: {input_ids.tolist()}\")\n",
        "    print(f\"í† í° ê°œìˆ˜: {len(tokens)}\")\n",
        "    print(\"-\" * 50)"
      ],
      "metadata": {
        "id": "1MBOshvUkB_s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6b7109e-24a8-40e7-a137-fff13187bafb"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== ë‹¤ì–‘í•œ í…ìŠ¤íŠ¸ì˜ í† í¬ë‚˜ì´ì œì´ì…˜ ê²°ê³¼ ===\n",
            "í…ìŠ¤íŠ¸: 'ì•ˆë…•í•˜ì„¸ìš”'\n",
            "í† í°: ['Ã¬', 'Ä·', 'Äª', 'Ã«', 'Ä§', 'Ä·', 'Ã­Ä·', 'Äº', 'Ã¬', 'Ä¦', 'Â¸', 'Ã¬', 'Ä¼', 'Ä¶']\n",
            "input_ids: [[168, 243, 230, 167, 227, 243, 47991, 246, 168, 226, 116, 168, 248, 242]]\n",
            "í† í° ê°œìˆ˜: 14\n",
            "--------------------------------------------------\n",
            "í…ìŠ¤íŠ¸: 'Hello world'\n",
            "í† í°: ['Hello', 'Ä world']\n",
            "input_ids: [[15496, 995]]\n",
            "í† í° ê°œìˆ˜: 2\n",
            "--------------------------------------------------\n",
            "í…ìŠ¤íŠ¸: 'í”„ë¡œê·¸ë˜ë°'\n",
            "í† í°: ['Ã­', 'Ä¶', 'Ä¦', 'Ã«', 'Â¡', 'Ä¾', 'Ãª', 'Â·', 'Â¸', 'Ã«', 'Å€', 'Äº', 'Ã«', 'Â°', 'Ä¯']\n",
            "input_ids: [[169, 242, 226, 167, 94, 250, 166, 115, 116, 167, 252, 246, 167, 108, 235]]\n",
            "í† í° ê°œìˆ˜: 15\n",
            "--------------------------------------------------\n",
            "í…ìŠ¤íŠ¸: 'AIëŠ” ë¯¸ë˜ë‹¤'\n",
            "í† í°: ['AI', 'Ã«', 'Ä¬', 'Ä¶', 'Ä Ã«', 'Â¯', 'Â¸', 'Ã«', 'Å€', 'Äº', 'Ã«Ä­', 'Â¤']\n",
            "input_ids: [[20185, 167, 232, 242, 31619, 107, 116, 167, 252, 246, 46695, 97]]\n",
            "í† í° ê°œìˆ˜: 12\n",
            "--------------------------------------------------\n",
            "í…ìŠ¤íŠ¸: '123456'\n",
            "í† í°: ['123', '456']\n",
            "input_ids: [[10163, 29228]]\n",
            "í† í° ê°œìˆ˜: 2\n",
            "--------------------------------------------------\n",
            "í…ìŠ¤íŠ¸: 'hello@email.com'\n",
            "í† í°: ['hello', '@', 'email', '.', 'com']\n",
            "input_ids: [[31373, 31, 12888, 13, 785]]\n",
            "í† í° ê°œìˆ˜: 5\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **ì˜ˆì œ 3: í† í° IDì˜ ì‚¬ìš© ê³¼ì •**"
      ],
      "metadata": {
        "id": "eU3ZzrJImaQn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "import torch\n",
        "\n",
        "# ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "\n",
        "# ì›ë³¸ í…ìŠ¤íŠ¸\n",
        "input_text = \"í”„ë¡œê·¸ë˜ë°ì€ ì¬ë¯¸ìˆë‹¤. í”„ë¡œê·¸ë˜ë°ì€\"\n",
        "print(f\"ì…ë ¥ í…ìŠ¤íŠ¸: '{input_text}'\")\n",
        "\n",
        "# í† í¬ë‚˜ì´ì œì´ì…˜\n",
        "input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
        "print(f\"input_ids: {input_ids}\")\n",
        "print(f\"ê° IDê°€ ë‚˜íƒ€ë‚´ëŠ” í† í°:\")\n",
        "\n",
        "# ê° IDê°€ ë¬´ìŠ¨ í† í°ì¸ì§€ í™•ì¸\n",
        "for i, token_id in enumerate(input_ids[0]):\n",
        "    token = tokenizer.decode([token_id])\n",
        "    print(f\"  ìœ„ì¹˜ {i}: ID {token_id.item()} â†’ '{token}'\")\n",
        "\n",
        "print()\n",
        "\n",
        "# ëª¨ë¸ì— ì…ë ¥í•˜ì—¬ ë‹¤ìŒ í† í° í™•ë¥  ê³„ì‚°\n",
        "with torch.no_grad():\n",
        "    outputs = model(input_ids)\n",
        "    # logits: [ë°°ì¹˜_í¬ê¸°, ì‹œí€€ìŠ¤_ê¸¸ì´, ì–´íœ˜_í¬ê¸°]\n",
        "    logits = outputs.logits\n",
        "\n",
        "print(f\"ëª¨ë¸ ì¶œë ¥ í˜•íƒœ: {logits.shape}\")\n",
        "print(f\"ë§ˆì§€ë§‰ í† í° ìœ„ì¹˜ì˜ í™•ë¥  ë¶„í¬ í¬ê¸°: {logits[0, -1, :].shape}\")\n",
        "\n",
        "# ë‹¤ìŒ í† í°ìœ¼ë¡œ ê°€ëŠ¥ì„±ì´ ë†’ì€ ìƒìœ„ 5ê°œ í™•ì¸\n",
        "last_token_logits = logits[0, -1, :]\n",
        "probabilities = torch.softmax(last_token_logits, dim=-1)\n",
        "top_5_prob, top_5_indices = torch.topk(probabilities, 5)\n",
        "\n",
        "print(\"\\në‹¤ìŒ í† í° ì˜ˆì¸¡ ìƒìœ„ 5ê°œ:\")\n",
        "for i, (prob, idx) in enumerate(zip(top_5_prob, top_5_indices)):\n",
        "    token = tokenizer.decode([idx])\n",
        "    print(f\"{i+1}. '{token}' (í™•ë¥ : {prob:.4f})\")"
      ],
      "metadata": {
        "id": "S-nY1StkmiJZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6744e18c-b38b-4848-f3f3-b308a472267f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ì…ë ¥ í…ìŠ¤íŠ¸: 'í”„ë¡œê·¸ë˜ë°ì€ ì¬ë¯¸ìˆë‹¤. í”„ë¡œê·¸ë˜ë°ì€'\n",
            "input_ids: tensor([[  169,   242,   226,   167,    94,   250,   166,   115,   116,   167,\n",
            "           252,   246,   167,   108,   235, 35975,   222, 23821,   252,   105,\n",
            "           167,   107,   116,   168,   252,   230, 46695,    97,    13,   220,\n",
            "           169,   242,   226,   167,    94,   250,   166,   115,   116,   167,\n",
            "           252,   246,   167,   108,   235, 35975,   222]])\n",
            "ê° IDê°€ ë‚˜íƒ€ë‚´ëŠ” í† í°:\n",
            "  ìœ„ì¹˜ 0: ID 169 â†’ 'ï¿½'\n",
            "  ìœ„ì¹˜ 1: ID 242 â†’ 'ï¿½'\n",
            "  ìœ„ì¹˜ 2: ID 226 â†’ 'ï¿½'\n",
            "  ìœ„ì¹˜ 3: ID 167 â†’ 'ï¿½'\n",
            "  ìœ„ì¹˜ 4: ID 94 â†’ 'ï¿½'\n",
            "  ìœ„ì¹˜ 5: ID 250 â†’ 'ï¿½'\n",
            "  ìœ„ì¹˜ 6: ID 166 â†’ 'ï¿½'\n",
            "  ìœ„ì¹˜ 7: ID 115 â†’ 'ï¿½'\n",
            "  ìœ„ì¹˜ 8: ID 116 â†’ 'ï¿½'\n",
            "  ìœ„ì¹˜ 9: ID 167 â†’ 'ï¿½'\n",
            "  ìœ„ì¹˜ 10: ID 252 â†’ 'ï¿½'\n",
            "  ìœ„ì¹˜ 11: ID 246 â†’ 'ï¿½'\n",
            "  ìœ„ì¹˜ 12: ID 167 â†’ 'ï¿½'\n",
            "  ìœ„ì¹˜ 13: ID 108 â†’ 'ï¿½'\n",
            "  ìœ„ì¹˜ 14: ID 235 â†’ 'ï¿½'\n",
            "  ìœ„ì¹˜ 15: ID 35975 â†’ 'ï¿½'\n",
            "  ìœ„ì¹˜ 16: ID 222 â†’ 'ï¿½'\n",
            "  ìœ„ì¹˜ 17: ID 23821 â†’ ' ï¿½'\n",
            "  ìœ„ì¹˜ 18: ID 252 â†’ 'ï¿½'\n",
            "  ìœ„ì¹˜ 19: ID 105 â†’ 'ï¿½'\n",
            "  ìœ„ì¹˜ 20: ID 167 â†’ 'ï¿½'\n",
            "  ìœ„ì¹˜ 21: ID 107 â†’ 'ï¿½'\n",
            "  ìœ„ì¹˜ 22: ID 116 â†’ 'ï¿½'\n",
            "  ìœ„ì¹˜ 23: ID 168 â†’ 'ï¿½'\n",
            "  ìœ„ì¹˜ 24: ID 252 â†’ 'ï¿½'\n",
            "  ìœ„ì¹˜ 25: ID 230 â†’ 'ï¿½'\n",
            "  ìœ„ì¹˜ 26: ID 46695 â†’ 'ï¿½'\n",
            "  ìœ„ì¹˜ 27: ID 97 â†’ 'ï¿½'\n",
            "  ìœ„ì¹˜ 28: ID 13 â†’ '.'\n",
            "  ìœ„ì¹˜ 29: ID 220 â†’ ' '\n",
            "  ìœ„ì¹˜ 30: ID 169 â†’ 'ï¿½'\n",
            "  ìœ„ì¹˜ 31: ID 242 â†’ 'ï¿½'\n",
            "  ìœ„ì¹˜ 32: ID 226 â†’ 'ï¿½'\n",
            "  ìœ„ì¹˜ 33: ID 167 â†’ 'ï¿½'\n",
            "  ìœ„ì¹˜ 34: ID 94 â†’ 'ï¿½'\n",
            "  ìœ„ì¹˜ 35: ID 250 â†’ 'ï¿½'\n",
            "  ìœ„ì¹˜ 36: ID 166 â†’ 'ï¿½'\n",
            "  ìœ„ì¹˜ 37: ID 115 â†’ 'ï¿½'\n",
            "  ìœ„ì¹˜ 38: ID 116 â†’ 'ï¿½'\n",
            "  ìœ„ì¹˜ 39: ID 167 â†’ 'ï¿½'\n",
            "  ìœ„ì¹˜ 40: ID 252 â†’ 'ï¿½'\n",
            "  ìœ„ì¹˜ 41: ID 246 â†’ 'ï¿½'\n",
            "  ìœ„ì¹˜ 42: ID 167 â†’ 'ï¿½'\n",
            "  ìœ„ì¹˜ 43: ID 108 â†’ 'ï¿½'\n",
            "  ìœ„ì¹˜ 44: ID 235 â†’ 'ï¿½'\n",
            "  ìœ„ì¹˜ 45: ID 35975 â†’ 'ï¿½'\n",
            "  ìœ„ì¹˜ 46: ID 222 â†’ 'ï¿½'\n",
            "\n",
            "ëª¨ë¸ ì¶œë ¥ í˜•íƒœ: torch.Size([1, 47, 50257])\n",
            "ë§ˆì§€ë§‰ í† í° ìœ„ì¹˜ì˜ í™•ë¥  ë¶„í¬ í¬ê¸°: torch.Size([50257])\n",
            "\n",
            "ë‹¤ìŒ í† í° ì˜ˆì¸¡ ìƒìœ„ 5ê°œ:\n",
            "1. ' ï¿½' (í™•ë¥ : 0.8128)\n",
            "2. ' ï¿½' (í™•ë¥ : 0.1152)\n",
            "3. ' ' (í™•ë¥ : 0.0436)\n",
            "4. 'ï¿½' (í™•ë¥ : 0.0058)\n",
            "5. 'ï¿½' (í™•ë¥ : 0.0029)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **[ì£¼ì˜!] ì´ìƒí•œ í† í°ë“¤ì˜ ì •ì²´**\n",
        "    - ì¶œë ¥ ê²°ê³¼ì—ì„œ GPT2ëŠ” í•œê¸€ì„ ë°”ì´íŠ¸ ë‹¨ìœ„ë¡œ ë¶„í•´ ë•Œë¬¸ì— í† í°ì˜ ì´ìƒí•˜ê²Œ ë³´ì—¬ì§ˆ ìˆ˜ ìˆë‹¤â­¢ì˜ë¯¸ ì†ì‹¤ ë°œìƒí•  ìˆ˜ ìˆë‹¤.\n",
        "- ì˜ë¯¸ ë‹¨ìœ„ë¡œ ë¶„í•´í•´ì•¼ ì •í™•í•¨\n",
        "    - âœ… ì›ë³¸ í…ìŠ¤íŠ¸: 'í”„ë¡œê·¸ë˜ë°ì€ ì¬ë¯¸ìˆë‹¤. í”„ë¡œê·¸ë˜ë°ì€'\n",
        "    - 1ï¸âƒ£ í† í° ë¶„í•  ê²°ê³¼: ['Ã­', 'Ä¶', 'Ä¦', 'Ã«', 'Â¡', 'Ä¾', 'Ãª', 'Â·', 'Â¸', 'Ã«', 'Å€', 'Äº', 'Ã«', 'Â°', 'Ä¯', 'Ã¬Ä¿', 'Ä¢', 'Ä Ã¬', 'Å€', 'Â¬', 'Ã«', 'Â¯', 'Â¸', 'Ã¬', 'Å€', 'Äª', 'Ã«Ä­', 'Â¤', '.', 'Ä ', 'Ã­', 'Ä¶', 'Ä¦', 'Ã«', 'Â¡', 'Ä¾', 'Ãª', 'Â·', 'Â¸', 'Ã«', 'Å€', 'Äº', 'Ã«', 'Â°', 'Ä¯', 'Ã¬Ä¿', 'Ä¢']\n",
        "- ['Ã­', 'Ä¶', 'Ä¦', 'Ã«', 'Â¡', 'Ä¾' ...] ì´ í† í°ë“¤ì˜ ì •ì²´\n",
        "\n",
        "|ì •ì²´|ì„¤ëª…|ë¬¸ì œì |\n",
        "|--- |--- |--- |\n",
        "|UTF-8 ë°”ì´íŠ¸ì˜ ì˜ëª»ëœ í•´ì„ |í•œê¸€ ë°”ì´íŠ¸ë¥¼ Latin-1ë¡œ ë””ì½”ë”©í•œ ê²°ê³¼ |ì˜ë¯¸ ì™„ì „ ì†ì‹¤ |\n",
        "|BPE ì•Œê³ ë¦¬ì¦˜ì˜ í•œê³„ |ì˜ì–´ ìœ„ì£¼ í•™ìŠµìœ¼ë¡œ í•œê¸€ íŒ¨í„´ ë¯¸í•™ìŠµ |ë¹„íš¨ìœ¨ì  í† í°í™” |\n",
        "|ì–´íœ˜ì§‘ ë¶€ì¡± |GPT-2 ì–´íœ˜ì§‘ì— í•œê¸€ í† í° ê±°ì˜ ì—†ìŒ |ì•Œ ìˆ˜ ì—†ëŠ” í† í°ìœ¼ë¡œ ì²˜ë¦¬ |"
      ],
      "metadata": {
        "id": "ub9T5C1Sjj14"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1ë‹¨ê³„: ë¬¸ì œ ìƒí™© ì •í™•í•œ ë¶„ì„**"
      ],
      "metadata": {
        "id": "qqqDtMxRkPB3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "\n",
        "# í•œê¸€ í…ìŠ¤íŠ¸\n",
        "korean_text = \"í”„ë¡œê·¸ë˜ë°ì€ ì¬ë¯¸ìˆë‹¤\"\n",
        "print(f\"ì›ë³¸ í…ìŠ¤íŠ¸: '{korean_text}'\")\n",
        "print()\n",
        "\n",
        "# í† í° ë¶„í•  ê²°ê³¼\n",
        "tokens = tokenizer.tokenize(korean_text)\n",
        "print(f\"í† í° ê°œìˆ˜: {len(tokens)}ê°œ\")\n",
        "print(f\"í† í°ë“¤: {tokens[:10]}... (ì²˜ìŒ 10ê°œë§Œ í‘œì‹œ)\")\n",
        "print()\n",
        "\n",
        "# ì´ìƒí•œ ë¬¸ìë“¤ì˜ ì •ì²´ í™•ì¸\n",
        "print(\"=== ì´ìƒí•œ í† í°ë“¤ì˜ ì •ì²´ ===\")\n",
        "for i, token in enumerate(tokens[:5]):\n",
        "    # í† í°ì„ ë°”ì´íŠ¸ë¡œ ë³€í™˜í•´ë³´ê¸°\n",
        "    try:\n",
        "        token_id = tokenizer.convert_tokens_to_ids([token])[0]\n",
        "        print(f\"í† í° {i+1}: '{token}' â†’ ID: {token_id}\")\n",
        "    except:\n",
        "        print(f\"í† í° {i+1}: '{token}' â†’ ë³€í™˜ ë¶ˆê°€\")\n",
        "\n",
        "print()\n",
        "\n",
        "# UTF-8 ë°”ì´íŠ¸ ë¶„ì„\n",
        "print(\"=== UTF-8 ë°”ì´íŠ¸ ë ˆë²¨ ë¶„ì„ ===\")\n",
        "korean_bytes = korean_text.encode('utf-8')\n",
        "print(f\"í•œê¸€ í…ìŠ¤íŠ¸ì˜ UTF-8 ë°”ì´íŠ¸: {korean_bytes}\")\n",
        "print(f\"ë°”ì´íŠ¸ ê°œìˆ˜: {len(korean_bytes)}ê°œ\")\n",
        "\n",
        "# ê° ë°”ì´íŠ¸ë¥¼ ê°œë³„ ë¬¸ìë¡œ ë””ì½”ë”© ì‹œë„\n",
        "print(\"ë°”ì´íŠ¸ë³„ ë¶„ì„:\")\n",
        "for i, byte_val in enumerate(korean_bytes[:10]):\n",
        "    print(f\"ë°”ì´íŠ¸ {i+1}: {byte_val} (0x{byte_val:02x})\")"
      ],
      "metadata": {
        "id": "DjbN4OT2jktt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26755902-f9a4-46d1-f7b9-9536971d4d6d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ì›ë³¸ í…ìŠ¤íŠ¸: 'í”„ë¡œê·¸ë˜ë°ì€ ì¬ë¯¸ìˆë‹¤'\n",
            "\n",
            "í† í° ê°œìˆ˜: 28ê°œ\n",
            "í† í°ë“¤: ['Ã­', 'Ä¶', 'Ä¦', 'Ã«', 'Â¡', 'Ä¾', 'Ãª', 'Â·', 'Â¸', 'Ã«']... (ì²˜ìŒ 10ê°œë§Œ í‘œì‹œ)\n",
            "\n",
            "=== ì´ìƒí•œ í† í°ë“¤ì˜ ì •ì²´ ===\n",
            "í† í° 1: 'Ã­' â†’ ID: 169\n",
            "í† í° 2: 'Ä¶' â†’ ID: 242\n",
            "í† í° 3: 'Ä¦' â†’ ID: 226\n",
            "í† í° 4: 'Ã«' â†’ ID: 167\n",
            "í† í° 5: 'Â¡' â†’ ID: 94\n",
            "\n",
            "=== UTF-8 ë°”ì´íŠ¸ ë ˆë²¨ ë¶„ì„ ===\n",
            "í•œê¸€ í…ìŠ¤íŠ¸ì˜ UTF-8 ë°”ì´íŠ¸: b'\\xed\\x94\\x84\\xeb\\xa1\\x9c\\xea\\xb7\\xb8\\xeb\\x9e\\x98\\xeb\\xb0\\x8d\\xec\\x9d\\x80 \\xec\\x9e\\xac\\xeb\\xaf\\xb8\\xec\\x9e\\x88\\xeb\\x8b\\xa4'\n",
            "ë°”ì´íŠ¸ ê°œìˆ˜: 31ê°œ\n",
            "ë°”ì´íŠ¸ë³„ ë¶„ì„:\n",
            "ë°”ì´íŠ¸ 1: 237 (0xed)\n",
            "ë°”ì´íŠ¸ 2: 148 (0x94)\n",
            "ë°”ì´íŠ¸ 3: 132 (0x84)\n",
            "ë°”ì´íŠ¸ 4: 235 (0xeb)\n",
            "ë°”ì´íŠ¸ 5: 161 (0xa1)\n",
            "ë°”ì´íŠ¸ 6: 156 (0x9c)\n",
            "ë°”ì´íŠ¸ 7: 234 (0xea)\n",
            "ë°”ì´íŠ¸ 8: 183 (0xb7)\n",
            "ë°”ì´íŠ¸ 9: 184 (0xb8)\n",
            "ë°”ì´íŠ¸ 10: 235 (0xeb)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2ë‹¨ê³„: ì˜ì–´ì™€ í•œê¸€ í† í¬ë‚˜ì´ì œì´ì…˜ ë¹„êµ**"
      ],
      "metadata": {
        "id": "MGMcqoTrkRSN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "\n",
        "# ë¹„êµ í…ìŠ¤íŠ¸ë“¤\n",
        "texts = {\n",
        "    \"ì˜ì–´\": \"Programming is fun\",\n",
        "    \"í•œê¸€\": \"í”„ë¡œê·¸ë˜ë°ì€ ì¬ë¯¸ìˆë‹¤\",\n",
        "    \"ìˆ«ì\": \"12345\",\n",
        "    \"íŠ¹ìˆ˜ë¬¸ì\": \"Hello! @#$%\"\n",
        "}\n",
        "\n",
        "print(\"=== ì–¸ì–´ë³„ í† í¬ë‚˜ì´ì œì´ì…˜ ë¹„êµ ===\")\n",
        "for lang, text in texts.items():\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    token_count = len(tokens)\n",
        "    char_count = len(text)\n",
        "\n",
        "    print(f\"\\n{lang}: '{text}'\")\n",
        "    print(f\"  ë¬¸ì ìˆ˜: {char_count}\")\n",
        "    print(f\"  í† í° ìˆ˜: {token_count}\")\n",
        "    print(f\"  íš¨ìœ¨ì„±: {token_count/char_count:.2f} (í† í°/ë¬¸ì)\")\n",
        "    print(f\"  í† í° ì˜ˆì‹œ: {tokens[:5]}...\")"
      ],
      "metadata": {
        "id": "jzaUiWdekRcD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ffeeb94-698f-4c4f-f286-02a630a8b766"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== ì–¸ì–´ë³„ í† í¬ë‚˜ì´ì œì´ì…˜ ë¹„êµ ===\n",
            "\n",
            "ì˜ì–´: 'Programming is fun'\n",
            "  ë¬¸ì ìˆ˜: 18\n",
            "  í† í° ìˆ˜: 4\n",
            "  íš¨ìœ¨ì„±: 0.22 (í† í°/ë¬¸ì)\n",
            "  í† í° ì˜ˆì‹œ: ['Program', 'ming', 'Ä is', 'Ä fun']...\n",
            "\n",
            "í•œê¸€: 'í”„ë¡œê·¸ë˜ë°ì€ ì¬ë¯¸ìˆë‹¤'\n",
            "  ë¬¸ì ìˆ˜: 11\n",
            "  í† í° ìˆ˜: 28\n",
            "  íš¨ìœ¨ì„±: 2.55 (í† í°/ë¬¸ì)\n",
            "  í† í° ì˜ˆì‹œ: ['Ã­', 'Ä¶', 'Ä¦', 'Ã«', 'Â¡']...\n",
            "\n",
            "ìˆ«ì: '12345'\n",
            "  ë¬¸ì ìˆ˜: 5\n",
            "  í† í° ìˆ˜: 2\n",
            "  íš¨ìœ¨ì„±: 0.40 (í† í°/ë¬¸ì)\n",
            "  í† í° ì˜ˆì‹œ: ['123', '45']...\n",
            "\n",
            "íŠ¹ìˆ˜ë¬¸ì: 'Hello! @#$%'\n",
            "  ë¬¸ì ìˆ˜: 11\n",
            "  í† í° ìˆ˜: 5\n",
            "  íš¨ìœ¨ì„±: 0.45 (í† í°/ë¬¸ì)\n",
            "  í† í° ì˜ˆì‹œ: ['Hello', '!', 'Ä @', '#$', '%']...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3ë‹¨ê³„: ì˜¬ë°”ë¥¸ ë‹¤êµ­ì–´ í† í¬ë‚˜ì´ì € ì‚¬ìš©**"
      ],
      "metadata": {
        "id": "h6wnNFbwkRxS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# ë‹¤êµ­ì–´ ì§€ì› í† í¬ë‚˜ì´ì €ë“¤ ë¹„êµ\n",
        "tokenizer_models = {\n",
        "    \"GPT-2 (ì˜ì–´ ì „ìš©)\": \"gpt2\",\n",
        "    \"mBERT (ë‹¤êµ­ì–´)\": \"bert-base-multilingual-cased\",\n",
        "    \"XLM-RoBERTa (ë‹¤êµ­ì–´)\": \"xlm-roberta-base\",\n",
        "    \"KoBERT (í•œêµ­ì–´)\" : \"skt/kobert-base-v1\",\n",
        "    \"KoGPT2 Base v2 (í•œêµ­ì–´)\" : \"skt/kogpt2-base-v2\",   # GPT-2 ê¸°ë°˜\n",
        "    \"Ko-GPT-Trinity 1.2B (v0.5) (í•œêµ­ì–´)\" : \"skt/ko-gpt-trinity-1.2B-v0.5\"  # GPT-3 ìŠ¤íƒ€ì¼\n",
        "}\n",
        "\n",
        "korean_text = \"í”„ë¡œê·¸ë˜ë°ì€ ì¬ë¯¸ìˆë‹¤\"\n",
        "\n",
        "print(\"=== ë‹¤ì–‘í•œ í† í¬ë‚˜ì´ì € ë¹„êµ ===\")\n",
        "for model_name, model_id in tokenizer_models.items():\n",
        "    try:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "        tokens = tokenizer.tokenize(korean_text)\n",
        "        token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "        print(f\"\\n{model_name}:\")\n",
        "        print(f\"  í† í° ìˆ˜: {len(tokens)}\")\n",
        "        print(f\"  í† í°ë“¤: {tokens}\")\n",
        "        print(f\"  í† í°ID: {token_ids}\")\n",
        "\n",
        "        # ì—­ë³€í™˜ í™•ì¸\n",
        "        reconstructed = tokenizer.convert_tokens_to_string(tokens)\n",
        "        print(f\"  ì—­ë³€í™˜: '{reconstructed}'\")\n",
        "        print(f\"  ì›ë³¸ê³¼ ë™ì¼: {'âœ…' if reconstructed.strip() == korean_text else 'âŒ'}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n{model_name}: ë¡œë“œ ì‹¤íŒ¨ - {e}\")"
      ],
      "metadata": {
        "id": "tra_GeXrkR5A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32503a87-2009-449d-8663-4f9c321309d8"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== ë‹¤ì–‘í•œ í† í¬ë‚˜ì´ì € ë¹„êµ ===\n",
            "\n",
            "GPT-2 (ì˜ì–´ ì „ìš©):\n",
            "  í† í° ìˆ˜: 28\n",
            "  í† í°ë“¤: ['Ã­', 'Ä¶', 'Ä¦', 'Ã«', 'Â¡', 'Ä¾', 'Ãª', 'Â·', 'Â¸', 'Ã«', 'Å€', 'Äº', 'Ã«', 'Â°', 'Ä¯', 'Ã¬Ä¿', 'Ä¢', 'Ä Ã¬', 'Å€', 'Â¬', 'Ã«', 'Â¯', 'Â¸', 'Ã¬', 'Å€', 'Äª', 'Ã«Ä­', 'Â¤']\n",
            "  í† í°ID: [169, 242, 226, 167, 94, 250, 166, 115, 116, 167, 252, 246, 167, 108, 235, 35975, 222, 23821, 252, 105, 167, 107, 116, 168, 252, 230, 46695, 97]\n",
            "  ì—­ë³€í™˜: 'í”„ë¡œê·¸ë˜ë°ì€ ì¬ë¯¸ìˆë‹¤'\n",
            "  ì›ë³¸ê³¼ ë™ì¼: âœ…\n",
            "\n",
            "mBERT (ë‹¤êµ­ì–´):\n",
            "  í† í° ìˆ˜: 8\n",
            "  í† í°ë“¤: ['í”„ë¡œ', '##ê·¸', '##ë˜', '##ë°', '##ì€', 'ì¬', '##ë¯¸', '##ìˆë‹¤']\n",
            "  í† í°ID: [102574, 78136, 37388, 118960, 10892, 9659, 22458, 76820]\n",
            "  ì—­ë³€í™˜: 'í”„ë¡œê·¸ë˜ë°ì€ ì¬ë¯¸ìˆë‹¤'\n",
            "  ì›ë³¸ê³¼ ë™ì¼: âœ…\n",
            "\n",
            "XLM-RoBERTa (ë‹¤êµ­ì–´):\n",
            "  í† í° ìˆ˜: 6\n",
            "  í† í°ë“¤: ['â–í”„ë¡œ', 'ê·¸ë˜', 'ë°', 'ì€', 'â–ì¬ë¯¸ìˆ', 'ë‹¤']\n",
            "  í† í°ID: [54099, 61882, 144667, 697, 157403, 1875]\n",
            "  ì—­ë³€í™˜: 'í”„ë¡œê·¸ë˜ë°ì€ ì¬ë¯¸ìˆë‹¤'\n",
            "  ì›ë³¸ê³¼ ë™ì¼: âœ…\n",
            "\n",
            "KoBERT (í•œêµ­ì–´):\n",
            "  í† í° ìˆ˜: 14\n",
            "  í† í°ë“¤: ['â–', 'á„‘á…³á„…á…©', 'á„€', 'á…³á„…á…¢á„†', 'á…µ', 'á†¼', 'á„‹', 'á…³á†«', 'â–', 'á„Œá…¢á„†', 'á…µ', 'á„‹', 'á…µ', 'á†»á„ƒá…¡']\n",
            "  í† í°ID: [517, 0, 490, 0, 494, 0, 491, 0, 517, 0, 494, 491, 494, 0]\n",
            "  ì—­ë³€í™˜: 'á„‘á…³á„…á…©á„€á…³á„…á…¢á„†á…µá†¼á„‹á…³á†« á„Œá…¢á„†á…µá„‹á…µá†»á„ƒá…¡'\n",
            "  ì›ë³¸ê³¼ ë™ì¼: âŒ\n",
            "\n",
            "KoGPT2 Base v2 (í•œêµ­ì–´):\n",
            "  í† í° ìˆ˜: 7\n",
            "  í† í°ë“¤: ['â–í”„ë¡œ', 'ê·¸ë˜', 'ë°', 'ì€', 'â–ì¬ë¯¸', 'ìˆ', 'ë‹¤']\n",
            "  í† í°ID: [9726, 19561, 7593, 8135, 18767, 8155, 7182]\n",
            "  ì—­ë³€í™˜: 'í”„ë¡œê·¸ë˜ë°ì€ ì¬ë¯¸ìˆë‹¤'\n",
            "  ì›ë³¸ê³¼ ë™ì¼: âœ…\n",
            "\n",
            "Ko-GPT-Trinity 1.2B (v0.5) (í•œêµ­ì–´):\n",
            "  í† í° ìˆ˜: 5\n",
            "  í† í°ë“¤: ['â–í”„ë¡œê·¸ë˜', 'ë°', 'ì€', 'â–ì¬ë¯¸', 'ìˆë‹¤']\n",
            "  í† í°ID: [49017, 22901, 25768, 32579, 45092]\n",
            "  ì—­ë³€í™˜: 'í”„ë¡œê·¸ë˜ë°ì€ ì¬ë¯¸ìˆë‹¤'\n",
            "  ì›ë³¸ê³¼ ë™ì¼: âœ…\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- í•œê¸€ ì²˜ë¦¬ë¥¼ ìœ„í•œ ì˜¬ë°”ë¥¸ ì„ íƒ\n",
        "\n",
        "|ìš©ë„ |ê¶Œì¥ ëª¨ë¸ |ì´ìœ \n",
        "|--- |--- |--- |\n",
        "| í•œê¸€ í…ìŠ¤íŠ¸ ìƒì„±| GPT-3.5/4, KoGPT| í•œê¸€ ë°ì´í„°ë¡œ í›ˆë ¨ë¨|\n",
        "| í•œê¸€ ì´í•´/ë¶„ë¥˜| KoBERT, KoELECTRA| í•œêµ­ì–´ íŠ¹í™” ëª¨ë¸|\n",
        "| ë‹¤êµ­ì–´ ì²˜ë¦¬| mBERT, XLM-RoBERTa| ë‹¤êµ­ì–´ ë™ì‹œ ì§€ì›|\n",
        "| ì‹¤ìŠµ/í•™ìŠµìš©| ì˜ì–´ ì˜ˆì œ ì‚¬ìš©| GPT-2 ë³¸ë˜ ì„±ëŠ¥ í™•ì¸|"
      ],
      "metadata": {
        "id": "P0_mx4WXldNe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- í•™ìŠµìš© ê°œì„ ëœ ì˜ˆì œ"
      ],
      "metadata": {
        "id": "Aj62TQvZl7G5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ì˜¬ë°”ë¥¸ ì ‘ê·¼: ì˜ì–´ë¡œ ì‹¤ìŠµí•˜ê¸°\n",
        "from transformers import GPT2Tokenizer\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "\n",
        "# GPT-2ê°€ ì˜ ì²˜ë¦¬í•˜ëŠ” ì˜ì–´ í…ìŠ¤íŠ¸\n",
        "english_text = \"Programming is fun. Programming is\"\n",
        "print(f\"ì˜ì–´ í…ìŠ¤íŠ¸: '{english_text}'\")\n",
        "\n",
        "tokens = tokenizer.tokenize(english_text)\n",
        "print(f\"í† í°ë“¤: {tokens}\")\n",
        "print(f\"í† í° ìˆ˜: {len(tokens)}\")\n",
        "\n",
        "# ê° í† í°ì˜ ì˜ë¯¸ í™•ì¸\n",
        "print(\"\\ní† í°ë³„ ì˜ë¯¸:\")\n",
        "for i, token in enumerate(tokens):\n",
        "    print(f\"{i+1}. '{token}' â†’ ì˜ë¯¸ìˆëŠ” ë‹¨ìœ„\")"
      ],
      "metadata": {
        "id": "0TeAXjxwnUQq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17f0eb48-08c1-41df-9917-fc128bf0240b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ì˜ì–´ í…ìŠ¤íŠ¸: 'Programming is fun. Programming is'\n",
            "í† í°ë“¤: ['Program', 'ming', 'Ä is', 'Ä fun', '.', 'Ä Programming', 'Ä is']\n",
            "í† í° ìˆ˜: 7\n",
            "\n",
            "í† í°ë³„ ì˜ë¯¸:\n",
            "1. 'Program' â†’ ì˜ë¯¸ìˆëŠ” ë‹¨ìœ„\n",
            "2. 'ming' â†’ ì˜ë¯¸ìˆëŠ” ë‹¨ìœ„\n",
            "3. 'Ä is' â†’ ì˜ë¯¸ìˆëŠ” ë‹¨ìœ„\n",
            "4. 'Ä fun' â†’ ì˜ë¯¸ìˆëŠ” ë‹¨ìœ„\n",
            "5. '.' â†’ ì˜ë¯¸ìˆëŠ” ë‹¨ìœ„\n",
            "6. 'Ä Programming' â†’ ì˜ë¯¸ìˆëŠ” ë‹¨ìœ„\n",
            "7. 'Ä is' â†’ ì˜ë¯¸ìˆëŠ” ë‹¨ìœ„\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "IeP3BFVJf-00"
      }
    }
  ]
}
